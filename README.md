# Natural Language Processing Assignment 2&3

Group member: Youning, Wan Jing, Zoey, YanSong

**Due date: Saturday, 21 March 2020, 12:05 AM**

## Coursework instruction: [Link](https://docs.google.com/document/d/1WTKNrYTr-7ckw62WAqy21-9udEMIpll4bWM5lmgpHZI/edit)

## Draft report: [Overleaf](https://www.overleaf.com/project/5e4eed8fc806ef0001bfac1a)

## WikiHow data pre_trained model (ML for 10,000 iterations): [LINK](https://drive.google.com/drive/folders/1Yg5z4ixRVj-AZK2F7qXULb6YzsS_OTjj?usp=sharing)

## WikiHow Vocab file: [LINK](https://drive.google.com/file/d/1t6Nh8GTylnkU6naUqbGx0oVxcBe-dJie/view?usp=sharing)

## WikiHow validation data: [LINK](https://drive.google.com/file/d/1Ew2amhF3pJsC_BWmYdC3VdJXnePVQk2q/view?usp=sharing)


## Relavant publications 
* [Deep Transfer Reinforcement Learning for Text Summarization](https://arxiv.org/pdf/1810.06667.pdf) ---Yaser et al.(2019), [code](https://github.com/yaserkl/TransferRL)
* [Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization](https://www.aclweb.org/anthology/D19-1623.pdf) ---Siyao et al.(2019)
* [A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION](https://arxiv.org/pdf/1705.04304.pdf) ---Paulus et al.(2017), [code](https://github.com/oceanypt/A-DEEP-REINFORCED-MODEL-FOR-ABSTRACTIVE-SUMMARIZATION), [other implementations](https://paperswithcode.com/paper/a-deep-reinforced-model-for-abstractive)
* [Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting](https://arxiv.org/pdf/1805.11080.pdf), [code](https://github.com/ChenRocks/fast_abs_rl)

## Not that relavant but very fundamental and useful publications
* [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) ---Sutskever et al., 2014.  Encoder-Decoder Network.

## Relavant datasets
* CNN-Daily dataset ---[helper codes](https://github.com/yaserkl/TransferRL/tree/master/src/helper)
* Newsroom dataset ---[helper codes](https://github.com/yaserkl/TransferRL/tree/master/src/helper)
* WikiHow dataset ---[paper](https://arxiv.org/pdf/1810.09305.pdf), [data](https://github.com/mahnazkoupaee/WikiHow-Dataset), [Processed txt data](https://drive.google.com/drive/folders/1_8s_A0OC5153gktx6dSbzLh02QJtI9LS?usp=sharing), [bin file](https://drive.google.com/drive/folders/1oaYyf3NPYYbrnJCRXt6OAb4ngAX8UsTZ?usp=sharing)
* BigPatent dataset ---[paper](https://arxiv.org/pdf/1906.03741.pdf), [data](https://evasharma.github.io/bigpatent/)

## Some useful webset

* [Figure-eight](https://www.figure-eight.com/data-for-everyone/)---dataset, not that good
* [niderhoff](https://github.com/niderhoff/nlp-datasets) ---NLP dataset
* [Browse State-of-the-Art](https://paperswithcode.com/sota) ---SOTA model and methods
* [text_summurization_abstractive_methods](https://github.com/theamrzaki/text_summurization_abstractive_methods) ---This repo is built to collect multiple implementations for abstractive approaches to address text summarization
* [Comprehensive Research Summary of *summarisation in NLP*](https://github.com/mathsyouth/awesome-text-summarization) ---This repo contains summarisation relevant *dataset*, *word embedding method*, *sequence embedding method*, etc, will be a good guide when we do background research and get hint about how to improve our methods. 
* [Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models](https://explosion.ai/blog/deep-learning-formula-nlp) ---Four steps strategy for deeplearning with text, examples attached.
* [Encoder-Decoder Sequence to Sequence Model](https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346) ---An explaination of Encoder-Decoder model in machine translation.


## Advices from Jiang, Minqi

* [Basic RL algorithm](https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fspinningup.openai.com%2Fen%2Flatest%2Fspinningup%2Frl_intro2.html&data=02%7C01%7C%7Ca9283f0035d84c5f253408d7b5809c2c%7C1faf88fea9984c5b93c9210a11d9a5c2%7C0%7C0%7C637177436287831455&sdata=rejITU1AhX1g9WGSruzZq%2FicFEu3nBINpy6Xy9nnIX8%3D&reserved=0)
* [Policy Gradient Algorithms](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)

## Useful Recource for Experiment

* [ROUGE evaluation](https://rxnlp.com/how-rouge-works-for-evaluation-of-summarization-tasks/#.Xk54bRP7RQI) ---sets of metric for evaluating Abstrative Summarisation result.
* [Code for attention-based summarisation](https://github.com/facebookarchive/NAMAS) ----[Neural Attention Model for Abstractive Summarization](https://arxiv.org/pdf/1509.00685.pdf), Github.
* [Text Summarization models](https://github.com/theamrzaki/text_summurization_abstractive_methods) ---With tutorials!
* [Text-Summarizer-Pytorch](https://github.com/rohithreddy024/Text-Summarizer-Pytorch) ---I tried this one but failed to write the data into binary file.
* [Ocean!](https://github.com/oceanypt/A-DEEP-REINFORCED-MODEL-FOR-ABSTRACTIVE-SUMMARIZATION) ---NMT means Neural Machine Translation...
* [einops](https://github.com/arogozhnikov/einops) ---useful package for tensor operation.

## Useful Resource for Writing Paper

* [Styling plots for publication with matplotlib](https://jonchar.net/notebooks/matplotlib-styling/) - how to use matplotlib much cooler




{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AS on CNNDM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jjH_CLaiP_tG",
        "noRvm-4NETE8",
        "eHWBFyRm526o"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkP6ibYGwc6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/gdrive/My\\ Drive/NLPproject/CNNDM/dm_stories_tokenized.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAcDzU9VGwOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgJzv6B6lTd0",
        "colab_type": "text"
      },
      "source": [
        "#Download rouge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsmpalAllXFV",
        "colab_type": "code",
        "outputId": "c95f6770-0e35-4864-fbef-af55a04aceae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "!git clone https://github.com/google-research/google-research"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'google-research'...\n",
            "remote: Enumerating objects: 558, done.\u001b[K\n",
            "remote: Counting objects: 100% (558/558), done.\u001b[K\n",
            "remote: Compressing objects: 100% (458/458), done.\u001b[K\n",
            "remote: Total 14681 (delta 393), reused 255 (delta 90), pack-reused 14123\u001b[K\n",
            "Receiving objects: 100% (14681/14681), 241.85 MiB | 31.63 MiB/s, done.\n",
            "Resolving deltas: 100% (6817/6817), done.\n",
            "Checking out files: 100% (5414/5414), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6pxFegOlapI",
        "colab_type": "code",
        "outputId": "b650dc0e-34d3-45ea-f3b7-debb4b8fe575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/63/ac/b93411318529980ab7f41e59ed64ec3ffed08ead32389e29eb78585dd55d/rouge-0.3.2-py3-none-any.whl\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjH_CLaiP_tG",
        "colab_type": "text"
      },
      "source": [
        "#some Nonesense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yz5aVPdGwRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8qgwImFIDND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = []\n",
        "with open(\"/content/drive/My Drive/NLPproject/CNNDM/finished_files/finished_files/vocab\",'r') as f:\n",
        "  for data in f:\n",
        "    a = data.split()     \n",
        "    vocab.append(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noRvm-4NETE8",
        "colab_type": "text"
      },
      "source": [
        "#Summarization on the CNN/Dailymail Dataset Using Transformers(cant make it working)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wonXgGzDEUzs",
        "colab_type": "text"
      },
      "source": [
        "https://freddejn.github.io/github/repo/2019/07/03/summarize-cnn-dailymail.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRRGkugM_gxZ",
        "colab_type": "code",
        "outputId": "a1eefb15-0dfe-44cf-acb7-1a1567024703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "!pip install -q -U tensor2tensor\n",
        "!pip install -q -U tensorflow\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!pip install -q -U tensor2tensor\n",
        "PROJECT_ID = 'GCP-PROJECT-ID'           # Where the bucket is set up\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "\n",
        "BUCKET = 'tensor2tensor-bucket-name'    # Name of bucket.\n",
        "PROBLEM = 'summarize_cnn_dailymail32k'  # Name of problem in t2t.\n",
        "DATA_DIR = f'gs://{BUCKET}/data'        # Path to data in bucket.\n",
        "TMP_DIR = 'tmp_dir'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.4MB 2.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 421.8MB 38kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 51.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 48.1MB/s \n",
            "\u001b[?25h\u001b[1;31mERROR:\u001b[0m (gcloud.config.set) The project property must be set to a valid project ID, not the project name [GCP-PROJECT-ID]\n",
            "To set your project, run:\n",
            "\n",
            "  $ gcloud config set project PROJECT_ID\n",
            "\n",
            "or to unset it, run:\n",
            "\n",
            "  $ gcloud config unset project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJRhP-Xo_g0J",
        "colab_type": "code",
        "outputId": "79dc471f-abd0-4fea-d8dd-a0847d31a729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "PROJECT_ID = 'nlp-project-270011'           # Where the bucket is set up\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "\n",
        "BUCKET = 'tensor2tensor-bucket-name'    # Name of bucket.\n",
        "PROBLEM = 'summarize_cnn_dailymail32k'  # Name of problem in t2t.\n",
        "DATA_DIR = f'gs://{BUCKET}/data'        # Path to data in bucket.\n",
        "TMP_DIR = 'tmp_dir'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJRSwLEk_gsc",
        "colab_type": "code",
        "outputId": "fbfb2d02-3624-450e-df73-a98cafb5cb68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "!t2t-datagen \\\n",
        "    --data_dir=$DATA_DIR \\\n",
        "    --tmp_dir=$TMP_DIR \\\n",
        "    --problem=$PROBLEM \\"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-03 11:03:53.086534: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2020-03-03 11:03:53.087048: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2020-03-03 11:03:53.087101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/t2t-datagen\", line 18, in <module>\n",
            "    from tensor2tensor.bin import t2t_datagen\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensor2tensor/bin/t2t_datagen.py\", line 58, in <module>\n",
            "    flags = tf.flags\n",
            "AttributeError: module 'tensorflow' has no attribute 'flags'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Q5l4AJEPsX",
        "colab_type": "code",
        "outputId": "4a6a1ebc-a105-46a8-8a4f-c9eeda1671e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "\n",
        "\"\"\"CNN/DailyMail Summarization dataset, non-anonymized version.\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import hashlib\n",
        "import os\n",
        "from absl import logging\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets.public_api as tfds\n",
        "\n",
        "_DESCRIPTION = \"\"\"\\\n",
        "CNN/DailyMail non-anonymized summarization dataset.\n",
        "There are two features:\n",
        "  - article: text of news article, used as the document to be summarized\n",
        "  - highlights: joined text of highlights with <s> and </s> around each\n",
        "    highlight, which is the target summary\n",
        "\"\"\"\n",
        "\n",
        "# The second citation introduces the source data, while the first\n",
        "# introduces the specific form (non-anonymized) we use here.\n",
        "_CITATION = \"\"\"\\\n",
        "@article{DBLP:journals/corr/SeeLM17,\n",
        "  author    = {Abigail See and\n",
        "               Peter J. Liu and\n",
        "               Christopher D. Manning},\n",
        "  title     = {Get To The Point: Summarization with Pointer-Generator Networks},\n",
        "  journal   = {CoRR},\n",
        "  volume    = {abs/1704.04368},\n",
        "  year      = {2017},\n",
        "  url       = {http://arxiv.org/abs/1704.04368},\n",
        "  archivePrefix = {arXiv},\n",
        "  eprint    = {1704.04368},\n",
        "  timestamp = {Mon, 13 Aug 2018 16:46:08 +0200},\n",
        "  biburl    = {https://dblp.org/rec/bib/journals/corr/SeeLM17},\n",
        "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
        "}\n",
        "@inproceedings{hermann2015teaching,\n",
        "  title={Teaching machines to read and comprehend},\n",
        "  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},\n",
        "  booktitle={Advances in neural information processing systems},\n",
        "  pages={1693--1701},\n",
        "  year={2015}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "_DL_URLS = {\n",
        "    # pylint: disable=line-too-long\n",
        "    'cnn_stories':\n",
        "        'https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ',\n",
        "    'dm_stories':\n",
        "        'https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfM1BxdkxVaTY2bWs',\n",
        "    'test_urls':\n",
        "        'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_test.txt',\n",
        "    'train_urls':\n",
        "        'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_train.txt',\n",
        "    'val_urls':\n",
        "        'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_val.txt',\n",
        "    # pylint: enable=line-too-long\n",
        "}\n",
        "\n",
        "_HIGHLIGHTS = 'highlights'\n",
        "_ARTICLE = 'article'\n",
        "_SUPPORTED_VERSIONS = [\n",
        "    # Same data as 0.0.2\n",
        "    tfds.core.Version('1.0.0',\n",
        "                      'New split API (https://tensorflow.org/datasets/splits)'),\n",
        "    # Having the model predict newline separators makes it easier to evaluate\n",
        "    # using summary-level ROUGE.\n",
        "    tfds.core.Version('2.0.0', 'Separate target sentences with newline.')\n",
        "]\n",
        "\n",
        "# Using cased version.\n",
        "_DEFAULT_VERSION = tfds.core.Version('3.0.0', 'Using cased version.')\n",
        "\n",
        "\n",
        "class CnnDailymailConfig(tfds.core.BuilderConfig):\n",
        "  \"\"\"BuilderConfig for CnnDailymail.\"\"\"\n",
        "\n",
        "  @tfds.core.disallow_positional_args\n",
        "  def __init__(self, text_encoder_config=None, **kwargs):\n",
        "    \"\"\"BuilderConfig for CnnDailymail.\n",
        "    Args:\n",
        "      text_encoder_config: `tfds.features.text.TextEncoderConfig`, configuration\n",
        "        for the `tfds.features.text.TextEncoder` used for the CnnDailymail\n",
        "        (text) features\n",
        "      **kwargs: keyword arguments forwarded to super.\n",
        "    \"\"\"\n",
        "    super(CnnDailymailConfig, self).__init__(\n",
        "        version=_DEFAULT_VERSION,\n",
        "        supported_versions=_SUPPORTED_VERSIONS,\n",
        "        **kwargs)\n",
        "    self.text_encoder_config = (\n",
        "        text_encoder_config or tfds.features.text.TextEncoderConfig())\n",
        "\n",
        "\n",
        "def _get_url_hashes(path):\n",
        "  \"\"\"Get hashes of urls in file.\"\"\"\n",
        "  urls = _read_text_file(path)\n",
        "\n",
        "  def url_hash(u):\n",
        "    h = hashlib.sha1()\n",
        "    try:\n",
        "      u = u.encode('utf-8')\n",
        "    except UnicodeDecodeError:\n",
        "      logging.error('Cannot hash url: %s', u)\n",
        "    h.update(u)\n",
        "    return h.hexdigest()\n",
        "\n",
        "  return {url_hash(u): True for u in urls}\n",
        "\n",
        "\n",
        "def _find_files(dl_paths, publisher, url_dict):\n",
        "  \"\"\"Find files corresponding to urls.\"\"\"\n",
        "  if publisher == 'cnn':\n",
        "    top_dir = os.path.join(dl_paths['cnn_stories'], 'cnn', 'stories')\n",
        "  elif publisher == 'dm':\n",
        "    top_dir = os.path.join(dl_paths['dm_stories'], 'dailymail', 'stories')\n",
        "  else:\n",
        "    logging.fatal('Unsupported publisher: %s', publisher)\n",
        "  files = tf.io.gfile.listdir(top_dir)\n",
        "\n",
        "  ret_files = []\n",
        "  for p in files:\n",
        "    basename = os.path.basename(p)\n",
        "    if basename[0:basename.find('.story')] in url_dict:\n",
        "      ret_files.append(os.path.join(top_dir, p))\n",
        "  return ret_files\n",
        "\n",
        "\n",
        "def _subset_filenames(dl_paths, split):\n",
        "  \"\"\"Get filenames for a particular split.\"\"\"\n",
        "  assert isinstance(dl_paths, dict), dl_paths\n",
        "  # Get filenames for a split.\n",
        "  if split == tfds.Split.TRAIN:\n",
        "    urls = _get_url_hashes(dl_paths['train_urls'])\n",
        "  elif split == tfds.Split.VALIDATION:\n",
        "    urls = _get_url_hashes(dl_paths['val_urls'])\n",
        "  elif split == tfds.Split.TEST:\n",
        "    urls = _get_url_hashes(dl_paths['test_urls'])\n",
        "  else:\n",
        "    logging.fatal('Unsupported split: %s', split)\n",
        "  cnn = _find_files(dl_paths, 'cnn', urls)\n",
        "  dm = _find_files(dl_paths, 'dm', urls)\n",
        "  return cnn + dm\n",
        "\n",
        "\n",
        "DM_SINGLE_CLOSE_QUOTE = u'\\u2019'  # unicode\n",
        "DM_DOUBLE_CLOSE_QUOTE = u'\\u201d'\n",
        "# acceptable ways to end a sentence\n",
        "END_TOKENS = [\n",
        "    '.', '!', '?', '...', \"'\", '`', '\"', DM_SINGLE_CLOSE_QUOTE,\n",
        "    DM_DOUBLE_CLOSE_QUOTE, ')'\n",
        "]\n",
        "\n",
        "\n",
        "def _read_text_file(text_file):\n",
        "  lines = []\n",
        "  with tf.io.gfile.GFile(text_file, 'r') as f:\n",
        "    for line in f:\n",
        "      lines.append(line.strip())\n",
        "  return lines\n",
        "\n",
        "\n",
        "def _get_art_abs(story_file, tfds_version):\n",
        "  \"\"\"Get abstract (highlights) and article from a story file path.\"\"\"\n",
        "  # Based on https://github.com/abisee/cnn-dailymail/blob/master/\n",
        "  #     make_datafiles.py\n",
        "\n",
        "  lines = _read_text_file(story_file)\n",
        "\n",
        "  # The github code lowercase the text and we removed it in 3.0.0.\n",
        "\n",
        "  # Put periods on the ends of lines that are missing them\n",
        "  # (this is a problem in the dataset because many image captions don't end in\n",
        "  # periods; consequently they end up in the body of the article as run-on\n",
        "  # sentences)\n",
        "  def fix_missing_period(line):\n",
        "    \"\"\"Adds a period to a line that is missing a period.\"\"\"\n",
        "    if '@highlight' in line:\n",
        "      return line\n",
        "    if not line:\n",
        "      return line\n",
        "    if line[-1] in END_TOKENS:\n",
        "      return line\n",
        "    return line + ' .'\n",
        "\n",
        "  lines = [fix_missing_period(line) for line in lines]\n",
        "\n",
        "  # Separate out article and abstract sentences\n",
        "  article_lines = []\n",
        "  highlights = []\n",
        "  next_is_highlight = False\n",
        "  for line in lines:\n",
        "    if not line:\n",
        "      continue  # empty line\n",
        "    elif line.startswith('@highlight'):\n",
        "      next_is_highlight = True\n",
        "    elif next_is_highlight:\n",
        "      highlights.append(line)\n",
        "    else:\n",
        "      article_lines.append(line)\n",
        "\n",
        "  # Make article into a single string\n",
        "  article = ' '.join(article_lines)\n",
        "\n",
        "  if tfds_version >= '2.0.0':\n",
        "    abstract = '\\n'.join(highlights)\n",
        "  else:\n",
        "    abstract = ' '.join(highlights)\n",
        "\n",
        "  return article, abstract\n",
        "\n",
        "\n",
        "class CnnDailymail(tfds.core.GeneratorBasedBuilder):\n",
        "  \"\"\"CNN/DailyMail non-anonymized summarization dataset.\"\"\"\n",
        "  BUILDER_CONFIGS = [\n",
        "      CnnDailymailConfig(\n",
        "          name='plain_text',\n",
        "          description='Plain text',\n",
        "      ),\n",
        "      CnnDailymailConfig(\n",
        "          name='bytes',\n",
        "          description=('Uses byte-level text encoding with '\n",
        "                       '`tfds.features.text.ByteTextEncoder`'),\n",
        "          text_encoder_config=tfds.features.text.TextEncoderConfig(\n",
        "              encoder=tfds.features.text.ByteTextEncoder()),\n",
        "      ),\n",
        "      CnnDailymailConfig(\n",
        "          name='subwords32k',\n",
        "          description=('Uses `tfds.features.text.SubwordTextEncoder` with '\n",
        "                       '32k vocab size'),\n",
        "          text_encoder_config=tfds.features.text.TextEncoderConfig(\n",
        "              encoder_cls=tfds.features.text.SubwordTextEncoder,\n",
        "              vocab_size=2**15),\n",
        "      ),\n",
        "  ]\n",
        "\n",
        "  def _info(self):\n",
        "    # Should return a tfds.core.DatasetInfo object\n",
        "    return tfds.core.DatasetInfo(\n",
        "        builder=self,\n",
        "        description=_DESCRIPTION,\n",
        "        features=tfds.features.FeaturesDict({\n",
        "            _ARTICLE:\n",
        "                tfds.features.Text(\n",
        "                    encoder_config=self.builder_config.text_encoder_config),\n",
        "            _HIGHLIGHTS:\n",
        "                tfds.features.Text(\n",
        "                    encoder_config=self.builder_config.text_encoder_config),\n",
        "        }),\n",
        "        supervised_keys=(_ARTICLE, _HIGHLIGHTS),\n",
        "        homepage='https://github.com/abisee/cnn-dailymail',\n",
        "        citation=_CITATION,\n",
        "    )\n",
        "\n",
        "  def _vocab_text_gen(self, paths):\n",
        "    for _, ex in self._generate_examples(paths):\n",
        "      yield ' '.join([ex[_ARTICLE], ex[_HIGHLIGHTS]])\n",
        "\n",
        "  def _split_generators(self, dl_manager):\n",
        "    dl_paths = dl_manager.download_and_extract(_DL_URLS)\n",
        "    train_files = _subset_filenames(dl_paths, tfds.Split.TRAIN)\n",
        "    # Generate shared vocabulary\n",
        "    # maybe_build_from_corpus uses SubwordTextEncoder if that's configured\n",
        "    self.info.features[_ARTICLE].maybe_build_from_corpus(\n",
        "        self._vocab_text_gen(train_files))\n",
        "    encoder = self.info.features[_ARTICLE].encoder\n",
        "    # Use maybe_set_encoder because the encoder may have been restored from\n",
        "    # package data.\n",
        "    self.info.features[_HIGHLIGHTS].maybe_set_encoder(encoder)\n",
        "\n",
        "    return [\n",
        "        tfds.core.SplitGenerator(\n",
        "            name=tfds.Split.TRAIN,\n",
        "            gen_kwargs={'files': train_files}),\n",
        "        tfds.core.SplitGenerator(\n",
        "            name=tfds.Split.VALIDATION,\n",
        "            gen_kwargs={\n",
        "                'files': _subset_filenames(dl_paths, tfds.Split.VALIDATION)\n",
        "            }),\n",
        "        tfds.core.SplitGenerator(\n",
        "            name=tfds.Split.TEST,\n",
        "            gen_kwargs={'files': _subset_filenames(dl_paths, tfds.Split.TEST)})\n",
        "    ]\n",
        "\n",
        "  def _generate_examples(self, files):\n",
        "    for p in files:\n",
        "      article, highlights = _get_art_abs(p, self.version)\n",
        "      if not article or not highlights:\n",
        "        continue\n",
        "      fname = os.path.basename(p)\n",
        "      yield fname, {_ARTICLE: article, _HIGHLIGHTS: highlights}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcZuHoz8EPvT",
        "colab_type": "code",
        "outputId": "b81d681d-2055-4b89-b63d-11fc9e41a8a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Copyright 2020 The TensorFlow Datasets Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Tests for tensorflow_datasets.text.cnn_dailymail.\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tempfile\n",
        "\n",
        "from tensorflow_datasets import testing\n",
        "import tensorflow_datasets.public_api as tfds\n",
        "from tensorflow_datasets.summarization import cnn_dailymail\n",
        "\n",
        "_STORY_FILE = b\"\"\"Some article.\n",
        "This is some article text.\n",
        "@highlight\n",
        "highlight text\n",
        "@highlight\n",
        "Highlight two\n",
        "@highlight\n",
        "highlight Three\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class CnnDailymailTest(testing.DatasetBuilderTestCase):\n",
        "  DATASET_CLASS = cnn_dailymail.CnnDailymail\n",
        "  SPLITS = {'train': 3, 'validation': 2, 'test': 2}\n",
        "  DL_EXTRACT_RESULT = {\n",
        "      'cnn_stories': '',\n",
        "      'dm_stories': '',\n",
        "      'test_urls': 'all_test.txt',\n",
        "      'train_urls': 'all_train.txt',\n",
        "      'val_urls': 'all_val.txt'\n",
        "  }\n",
        "\n",
        "  def test_get_art_abs(self):\n",
        "    with tempfile.NamedTemporaryFile(delete=True) as f:\n",
        "      f.write(_STORY_FILE)\n",
        "      f.flush()\n",
        "      article, abstract = cnn_dailymail._get_art_abs(f.name,\n",
        "                                                     tfds.core.Version('1.0.0'))\n",
        "      self.assertEqual('Some article. This is some article text.', article)\n",
        "      # This is a bit weird, but the original code at\n",
        "      # https://github.com/abisee/cnn-dailymail/ adds space before period\n",
        "      # for abstracts and we retain this behavior.\n",
        "      self.assertEqual('highlight text . Highlight two . highlight Three .',\n",
        "                       abstract)\n",
        "\n",
        "      article, abstract = cnn_dailymail._get_art_abs(f.name,\n",
        "                                                     tfds.core.Version('2.0.0'))\n",
        "      self.assertEqual('highlight text .\\nHighlight two .\\nhighlight Three .',\n",
        "                       abstract)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  testing.test_main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running tests under Python 3.6.9: /usr/bin/python3\n",
            "[ RUN      ] CnnDailymailTest.test_baseclass\n",
            "I0303 11:17:05.688071 139686367799168 dataset_builder.py:736] No config specified, defaulting to first: cnn_dailymail/plain_text\n",
            "I0303 11:17:05.690219 139686367799168 dataset_builder.py:202] Load pre-computed datasetinfo (eg: splits) from bucket.\n",
            "[  FAILED  ] CnnDailymailTest.test_baseclass\n",
            "[ RUN      ] CnnDailymailTest.test_download_and_prepare_as_dataset\n",
            "I0303 11:17:05.696299 139686367799168 dataset_builder.py:736] No config specified, defaulting to first: cnn_dailymail/plain_text\n",
            "I0303 11:17:05.698181 139686367799168 dataset_builder.py:202] Load pre-computed datasetinfo (eg: splits) from bucket.\n",
            "[  FAILED  ] CnnDailymailTest.test_download_and_prepare_as_dataset\n",
            "[ RUN      ] CnnDailymailTest.test_get_art_abs\n",
            "I0303 11:17:05.703094 139686367799168 dataset_builder.py:736] No config specified, defaulting to first: cnn_dailymail/plain_text\n",
            "I0303 11:17:05.704790 139686367799168 dataset_builder.py:202] Load pre-computed datasetinfo (eg: splits) from bucket.\n",
            "[  FAILED  ] CnnDailymailTest.test_get_art_abs\n",
            "[ RUN      ] CnnDailymailTest.test_info\n",
            "I0303 11:17:05.710218 139686367799168 dataset_builder.py:736] No config specified, defaulting to first: cnn_dailymail/plain_text\n",
            "I0303 11:17:05.711796 139686367799168 dataset_builder.py:202] Load pre-computed datasetinfo (eg: splits) from bucket.\n",
            "[  FAILED  ] CnnDailymailTest.test_info\n",
            "[ RUN      ] CnnDailymailTest.test_registered\n",
            "I0303 11:17:05.717534 139686367799168 dataset_builder.py:736] No config specified, defaulting to first: cnn_dailymail/plain_text\n",
            "I0303 11:17:05.719311 139686367799168 dataset_builder.py:202] Load pre-computed datasetinfo (eg: splits) from bucket.\n",
            "[  FAILED  ] CnnDailymailTest.test_registered\n",
            "[ RUN      ] CnnDailymailTest.test_session\n",
            "[  SKIPPED ] CnnDailymailTest.test_session\n",
            "======================================================================\n",
            "ERROR: test_baseclass (__main__.CnnDailymailTest)\n",
            "test_baseclass (__main__.CnnDailymailTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_datasets/testing/dataset_builder_testing.py\", line 164, in setUp\n",
            "    raise ValueError(err_msg)\n",
            "ValueError: fake_examples dir /usr/local/lib/python3.6/dist-packages/tensorflow_datasets/testing/test_data/fake_examples/cnn_dailymail not found.\n",
            "\n",
            "======================================================================\n",
            "ERROR: test_download_and_prepare_as_dataset (__main__.CnnDailymailTest)\n",
            "test_download_and_prepare_as_dataset (__main__.CnnDailymailTest)\n",
            "Run the decorated test method.\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_datasets/testing/dataset_builder_testing.py\", line 164, in setUp\n",
            "    raise ValueError(err_msg)\n",
            "ValueError: fake_examples dir /usr/local/lib/python3.6/dist-packages/tensorflow_datasets/testing/test_data/fake_examples/cnn_dailymail not found.\n",
            "\n",
            "======================================================================\n",
            "ERROR: test_get_art_abs (__main__.CnnDailymailTest)\n",
            "test_get_art_abs (__main__.CnnDailymailTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_datasets/testing/dataset_builder_testing.py\", line 164, in setUp\n",
            "    raise ValueError(err_msg)\n",
            "ValueError: fake_examples dir /usr/local/lib/python3.6/dist-packages/tensorflow_datasets/testing/test_data/fake_examples/cnn_dailymail not found.\n",
            "\n",
            "======================================================================\n",
            "ERROR: test_info (__main__.CnnDailymailTest)\n",
            "test_info (__main__.CnnDailymailTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_datasets/testing/dataset_builder_testing.py\", line 164, in setUp\n",
            "    raise ValueError(err_msg)\n",
            "ValueError: fake_examples dir /usr/local/lib/python3.6/dist-packages/tensorflow_datasets/testing/test_data/fake_examples/cnn_dailymail not found.\n",
            "\n",
            "======================================================================\n",
            "ERROR: test_registered (__main__.CnnDailymailTest)\n",
            "test_registered (__main__.CnnDailymailTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_datasets/testing/dataset_builder_testing.py\", line 164, in setUp\n",
            "    raise ValueError(err_msg)\n",
            "ValueError: fake_examples dir /usr/local/lib/python3.6/dist-packages/tensorflow_datasets/testing/test_data/fake_examples/cnn_dailymail not found.\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 6 tests in 0.040s\n",
            "\n",
            "FAILED (errors=5, skipped=1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZVRt89jEPxq",
        "colab_type": "code",
        "outputId": "a92ff0f4-03ad-427f-fd44-cbd7d25b70f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "tfds_version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-1932d6f67994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfds_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tfds_version' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icvx7nLtIDKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Iw9i99hIDPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3r8C5kyOVNw",
        "colab_type": "text"
      },
      "source": [
        "#load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFqVQQZYIDRR",
        "colab_type": "code",
        "outputId": "d361a505-6349-4bab-f227-d9eb0a8c9859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f93H_R7RIDT9",
        "colab_type": "code",
        "outputId": "9dd8e43d-7408-4444-bf3c-13dae3c35748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!ls \"/content/drive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ASSIGNMENT2_ChaojieLing   NLP\t       'Statement 07-SEP-18 AC 93941396.pdf'\n",
            "'Colab Notebooks'\t   NLPproject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXy_mOTO5von",
        "colab_type": "text"
      },
      "source": [
        "#Vocab using tf.Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mpGC8ILOUKa",
        "colab_type": "code",
        "outputId": "e61b0e5a-ab3f-43c3-afdf-0a692e5e051d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import glob\n",
        "import random\n",
        "import struct\n",
        "import csv\n",
        "from tensorflow.core.example import example_pb2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzQVkab2SWjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SENTENCE_START = '<s>'      # <s> and </s> are used in the data files to segment the abstracts into sentences. They don't receive vocab ids.\n",
        "SENTENCE_END = '</s>'\n",
        "\n",
        "PAD_TOKEN = '[PAD]'         # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
        "UNKNOWN_TOKEN = '[UNK]'     # This has a vocab id, which is used to represent out-of-vocabulary words\n",
        "START_DECODING = '[START]'  # This has a vocab id, which is used at the start of every decoder input sequence\n",
        "STOP_DECODING = '[STOP]'    # This has a vocab id, which is used at the end of untruncated target sequences\n",
        "\n",
        "# Note: none of <s>, </s>, [PAD], [UNK], [START], [STOP] should appear in the vocab file."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJfMtUoOSWoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab(object):\n",
        "  \"\"\"Vocabulary class for mapping between words and ids (integers)\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_file, max_size):\n",
        "    \"\"\"Creates a vocab of up to max_size words, reading from the vocab_file. If max_size is 0, reads the entire vocab file.\n",
        "    Args:\n",
        "      vocab_file: path to the vocab file, which is assumed to contain \"<word> <frequency>\" on each line, sorted with most frequent word first. This code doesn't actually use the frequencies, though.\n",
        "      max_size: integer. The maximum size of the resulting Vocabulary.\"\"\"\n",
        "    self._word_to_id = {}         #dictionary with word as key\n",
        "    self._id_to_word = {}         #           with id as key\n",
        "    self._count = 0 # keeps track of total number of words in the Vocab\n",
        "\n",
        "    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
        "    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
        "      self._word_to_id[w] = self._count\n",
        "      self._id_to_word[self._count] = w\n",
        "      self._count += 1\n",
        "\n",
        "    # Read the vocab file and add words up to max_size\n",
        "    with open(vocab_file, 'r') as vocab_f:\n",
        "      for line in vocab_f:\n",
        "        pieces = line.split()                                                   #list with two elements, the first is word the other one is its id\n",
        "        if len(pieces) != 2:\n",
        "          print('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n",
        "          continue\n",
        "        w = pieces[0]                                                           #word\n",
        "        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:        #raise error\n",
        "          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
        "        if w in self._word_to_id:                                             \n",
        "          raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
        "        self._word_to_id[w] = self._count             #assign id to word\n",
        "        self._id_to_word[self._count] = w             #assign word to id\n",
        "        self._count += 1                              #id increment\n",
        "        if max_size != 0 and self._count >= max_size:         #stop assigning when max_size has been reached\n",
        "          print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
        "          break\n",
        "\n",
        "    print(\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
        "\n",
        "  def word2id(self, word):\n",
        "    \"\"\"Returns the id (integer) of a word (string). Returns [UNK] id if word is OOV.\"\"\"\n",
        "    if word not in self._word_to_id:\n",
        "      return self._word_to_id[UNKNOWN_TOKEN]\n",
        "    return self._word_to_id[word]\n",
        "\n",
        "  def id2word(self, word_id):\n",
        "    \"\"\"Returns the word (string) corresponding to an id (integer).\"\"\"\n",
        "    if word_id not in self._id_to_word:\n",
        "      raise ValueError('Id not found in vocab: %d' % word_id)\n",
        "    return self._id_to_word[word_id]\n",
        "\n",
        "  def size(self):\n",
        "    \"\"\"Returns the total size of the vocabulary\"\"\"\n",
        "    return self._count\n",
        "\n",
        "  def write_metadata(self, fpath):\n",
        "    \"\"\"Writes metadata file for Tensorboard word embedding visualizer as described here:\n",
        "      https://www.tensorflow.org/get_started/embedding_viz\n",
        "    Args:\n",
        "      fpath: place to write the metadata file\n",
        "    \"\"\"\n",
        "    print(\"Writing word embedding metadata file to %s...\" % (fpath))\n",
        "    with open(fpath, \"w\") as f:\n",
        "      fieldnames = ['word']\n",
        "      writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
        "      for i in range(self.size()):\n",
        "        writer.writerow({\"word\": self._id_to_word[i]})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def example_generator(data_path, single_pass):\n",
        "  \"\"\"Generates tf.Examples from data files.\n",
        "    Binary data format: <length><blob>. <length> represents the byte size\n",
        "    of <blob>. <blob> is serialized tf.Example proto. The tf.Example contains\n",
        "    the tokenized article text and summary.\n",
        "    https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
        "  Args:\n",
        "    data_path:\n",
        "      Path to tf.Example data files. Can include wildcards, e.g. if you have several training data chunk files train_001.bin, train_002.bin, etc, then pass data_path=train_* to access them all.\n",
        "    single_pass:\n",
        "      Boolean. If True, go through the dataset exactly once, generating examples in the order they appear, then return. Otherwise, generate random examples indefinitely.\n",
        "  Yields:\n",
        "    Deserialized tf.Example.\n",
        "  \"\"\"\n",
        "  while True:\n",
        "    filelist = glob.glob(data_path) # get the list of datafiles\n",
        "    assert filelist, ('Error: Empty filelist at %s' % data_path) # check filelist isn't empty\n",
        "    if single_pass:\n",
        "      filelist = sorted(filelist)\n",
        "    else:\n",
        "      random.shuffle(filelist)\n",
        "    for f in filelist:\n",
        "      reader = open(f, 'rb')\n",
        "      while True:\n",
        "        len_bytes = reader.read(8)\n",
        "        if not len_bytes: break # finished reading this file\n",
        "        str_len = struct.unpack('q', len_bytes)[0]\n",
        "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
        "        yield example_pb2.Example.FromString(example_str)\n",
        "    if single_pass:\n",
        "      print(\"example_generator completed reading all datafiles. No more data.\")\n",
        "      break\n",
        "\n",
        "\n",
        "def article2ids(article_words, vocab):\n",
        "  \"\"\"Map the article words to their ids. Also return a list of OOVs in the article.\n",
        "  Args:\n",
        "    article_words: list of words (strings)\n",
        "    vocab: Vocabulary object\n",
        "  Returns:\n",
        "    ids:\n",
        "      A list of word ids (integers); OOVs are represented by their temporary article OOV number. If the vocabulary size is 50k and the article has 3 OOVs, then these temporary OOV numbers will be 50000, 50001, 50002.\n",
        "    oovs:\n",
        "      A list of the OOV words in the article (strings), in the order corresponding to their temporary article OOV numbers.\"\"\"\n",
        "  ids = []\n",
        "  oovs = []\n",
        "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
        "  for w in article_words:\n",
        "    i = vocab.word2id(w)\n",
        "    if i == unk_id: # If w is OOV\n",
        "      if w not in oovs: # Add to list of OOVs\n",
        "        oovs.append(w)\n",
        "      oov_num = oovs.index(w) # This is 0 for the first article OOV, 1 for the second article OOV...\n",
        "      ids.append(vocab.size() + oov_num) # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
        "    else:\n",
        "      ids.append(i)\n",
        "  return ids, oovs\n",
        "\n",
        "\n",
        "def abstract2ids(abstract_words, vocab, article_oovs):\n",
        "  \"\"\"Map the abstract words to their ids. In-article OOVs are mapped to their temporary OOV numbers.\n",
        "  Args:\n",
        "    abstract_words: list of words (strings)\n",
        "    vocab: Vocabulary object\n",
        "    article_oovs: list of in-article OOV words (strings), in the order corresponding to their temporary article OOV numbers\n",
        "  Returns:\n",
        "    ids: List of ids (integers). In-article OOV words are mapped to their temporary OOV numbers. Out-of-article OOV words are mapped to the UNK token id.\"\"\"\n",
        "  ids = []\n",
        "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
        "  for w in abstract_words:\n",
        "    i = vocab.word2id(w)\n",
        "    if i == unk_id: # If w is an OOV word\n",
        "      if w in article_oovs: # If w is an in-article OOV\n",
        "        vocab_idx = vocab.size() + article_oovs.index(w) # Map to its temporary article OOV number\n",
        "        ids.append(vocab_idx)\n",
        "      else: # If w is an out-of-article OOV\n",
        "        ids.append(unk_id) # Map to the UNK token id\n",
        "    else:\n",
        "      ids.append(i)\n",
        "  return ids\n",
        "\n",
        "\n",
        "def outputids2words(id_list, vocab, article_oovs):\n",
        "  \"\"\"Maps output ids to words, including mapping in-article OOVs from their temporary ids to the original OOV string (applicable in pointer-generator mode).\n",
        "  Args:\n",
        "    id_list: list of ids (integers)\n",
        "    vocab: Vocabulary object\n",
        "    article_oovs: list of OOV words (strings) in the order corresponding to their temporary article OOV ids (that have been assigned in pointer-generator mode), or None (in baseline mode)\n",
        "  Returns:\n",
        "    words: list of words (strings)\n",
        "  \"\"\"\n",
        "  words = []\n",
        "  for i in id_list:\n",
        "    try:\n",
        "      w = vocab.id2word(i) # might be [UNK]\n",
        "    except ValueError as e: # w is OOV\n",
        "      assert article_oovs is not None, \"Error: model produced a word ID that isn't in the vocabulary. This should not happen in baseline (no pointer-generator) mode\"\n",
        "      article_oov_idx = i - vocab.size()\n",
        "      try:\n",
        "        w = article_oovs[article_oov_idx]\n",
        "      except ValueError as e: # i doesn't correspond to an article oov\n",
        "        raise ValueError('Error: model produced word ID %i which corresponds to article OOV %i but this example only has %i article OOVs' % (i, article_oov_idx, len(article_oovs)))\n",
        "    words.append(w) \n",
        "  return words\n",
        "\n",
        "\n",
        "def abstract2sents(abstract):\n",
        "  \"\"\"Splits abstract text from datafile into list of sentences.\n",
        "  Args:\n",
        "    abstract: string containing <s> and </s> tags for starts and ends of sentences\n",
        "  Returns:\n",
        "    sents: List of sentence strings (no tags)\"\"\"\n",
        "  cur = 0\n",
        "  sents = []\n",
        "  while True:\n",
        "    try:\n",
        "      start_p = abstract.index(SENTENCE_START, cur)\n",
        "      end_p = abstract.index(SENTENCE_END, start_p + 1)\n",
        "      cur = end_p + len(SENTENCE_END)\n",
        "      sents.append(abstract[start_p+len(SENTENCE_START):end_p])\n",
        "    except ValueError as e: # no more sentences\n",
        "      return sents\n",
        "\n",
        "\n",
        "def show_art_oovs(article, vocab):\n",
        "  \"\"\"Returns the article string, highlighting the OOVs by placing __underscores__ around them\"\"\"\n",
        "  unk_token = vocab.word2id(UNKNOWN_TOKEN)\n",
        "  words = article.split(' ')\n",
        "  words = [(\"__%s__\" % w) if vocab.word2id(w)==unk_token else w for w in words]\n",
        "  out_str = ' '.join(words)\n",
        "  return out_str\n",
        "\n",
        "\n",
        "def show_abs_oovs(abstract, vocab, article_oovs):\n",
        "  \"\"\"Returns the abstract string, highlighting the article OOVs with __underscores__.\n",
        "  If a list of article_oovs is provided, non-article OOVs are differentiated like !!__this__!!.\n",
        "  Args:\n",
        "    abstract: string\n",
        "    vocab: Vocabulary object\n",
        "    article_oovs: list of words (strings), or None (in baseline mode)\n",
        "  \"\"\"\n",
        "  unk_token = vocab.word2id(UNKNOWN_TOKEN)\n",
        "  words = abstract.split(' ')\n",
        "  new_words = []\n",
        "  for w in words:\n",
        "    if vocab.word2id(w) == unk_token: # w is oov\n",
        "      if article_oovs is None: # baseline mode\n",
        "        new_words.append(\"__%s__\" % w)\n",
        "      else: # pointer-generator mode\n",
        "        if w in article_oovs:\n",
        "          new_words.append(\"__%s__\" % w)\n",
        "        else:\n",
        "          new_words.append(\"!!__%s__!!\" % w)\n",
        "    else: # w is in-vocab word\n",
        "      new_words.append(w)\n",
        "  out_str = ' '.join(new_words)\n",
        "  return out_str"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqx-bKX-SWqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-5gL-VDSWtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab(\"/content/drive/My Drive/NLPproject/CNNDM/finished_files/finished_files/vocab\", 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5-OHIcV6NZC",
        "colab_type": "code",
        "outputId": "f912f29b-cc99-4646-afac-10387147494d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.Vocab"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu3NooJpSWu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = example_generator(\"/content/drive/My Drive/NLPproject/CNNDM/finished_files/finished_files/chunked/train_000.bin\", single_pass = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl6SbhNCSWzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in a:\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OLSpHWoSWx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szmMzSQLSWnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHWBFyRm526o",
        "colab_type": "text"
      },
      "source": [
        "#Using pytorch    (Nonesense)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHBnjIn1SWhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" CNN/DM dataset\"\"\"\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "from os.path import join\n",
        "\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5lXaz9y55Tq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CnnDmDataset(Dataset):\n",
        "    def __init__(self, split: str, path: str) -> None:\n",
        "        assert split in ['train', 'val', 'test']\n",
        "        self._data_path = join(path, split)\n",
        "        self._n_data = _count_data(self._data_path)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self._n_data\n",
        "\n",
        "    def __getitem__(self, i: int):\n",
        "        with open(join(self._data_path, '{}.json'.format(i))) as f:\n",
        "            js = json.loads(f.read())\n",
        "        return js\n",
        "\n",
        "\n",
        "def _count_data(path):\n",
        "    \"\"\" count number of data in the given path\"\"\"\n",
        "    matcher = re.compile(r'[0-9]+\\.json')\n",
        "    match = lambda name: bool(matcher.match(name))\n",
        "    names = os.listdir(path)\n",
        "    n_data = len(list(filter(match, names)))\n",
        "    return n_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvMXTInuegYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWdvrQmeegbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n2ooJWtuSfY",
        "colab_type": "text"
      },
      "source": [
        "#Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC5y6CD4uWPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_path = \t\"drive/My Drive/NLPproject/CNNDM/finished_files/finished_files/chunked/train_*\"\n",
        "valid_data_path = \t\"drive/My Drive/NLPproject/CNNDM/finished_files/finished_files/val.bin\"\n",
        "test_data_path = \t\"drive/My Drive/NLPproject/CNNDM/finished_files/finished_files/chunked/test_*\"\n",
        "vocab_path = \t\t\"drive/My Drive/NLPproject/CNNDM/finished_files/finished_files/vocab\"\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_dim = 512\n",
        "emb_dim = 256\n",
        "batch_size = 200\n",
        "max_enc_steps = 55\t\t#99% of the articles are within length 55\n",
        "max_dec_steps = 15\t\t#99% of the titles are within length 15\n",
        "beam_size = 4\n",
        "min_dec_steps= 3\n",
        "vocab_size = 50000\n",
        "\n",
        "lr = 0.001\n",
        "rand_unif_init_mag = 0.02\n",
        "trunc_norm_init_std = 1e-4\n",
        "\n",
        "eps = 1e-12\n",
        "max_iterations = 10000\n",
        "\n",
        "\n",
        "save_model_path = \"drive/My Drive/NLPproject/CNNDM\"\n",
        "\n",
        "intra_encoder = True\n",
        "intra_decoder = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOqP4eYluWSD",
        "colab_type": "code",
        "outputId": "56ddd712-8168-4dd3-9a4f-535959e2d46f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls 'drive/My Drive/NLPproject/CNNDM/finished_files/finished_files'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chunked  test.bin  train.bin  val.bin  vocab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFySWmk3yhZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qydK3JD_ykDL",
        "colab_type": "text"
      },
      "source": [
        "#Batcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-PGEr7Dyhb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import queue as Queue\n",
        "import time\n",
        "from random import shuffle\n",
        "from threading import Thread\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "import random\n",
        "random.seed(1234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snkMCNNyylcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Example(object):\n",
        "\n",
        "  def __init__(self, article, abstract_sentences, vocab):\n",
        "    # Get ids of special tokens\n",
        "    start_decoding = vocab.word2id(START_DECODING)      #data\n",
        "    stop_decoding = vocab.word2id(STOP_DECODING)        #data\n",
        "\n",
        "    # Process the article\n",
        "    article_words = article.split()\n",
        "    if len(article_words) > max_enc_steps:              #config\n",
        "      article_words = article_words[ : max_enc_steps]   #config\n",
        "    self.enc_len = len(article_words)                   # store the length after truncation but before padding\n",
        "    self.enc_input = [vocab.word2id(w) for w in article_words] # list of word ids; OOVs are represented by the id for UNK token\n",
        "\n",
        "    # Process the abstract\n",
        "    abstract = ' '.join(abstract_sentences)                 # string\n",
        "    abstract_words = abstract.split()                       # list of strings, split by space\n",
        "    abs_ids = [vocab.word2id(w) for w in abstract_words]    # list of word ids; OOVs are represented by the id for UNK token\n",
        "\n",
        "    # Get the decoder input sequence and target sequence\n",
        "    self.dec_input, _ = self.get_dec_inp_targ_seqs(abs_ids, max_dec_steps, start_decoding, stop_decoding)\n",
        "    self.dec_len = len(self.dec_input)\n",
        "\n",
        "    # If using pointer-generator mode, we need to store some extra info\n",
        "    # Store a version of the enc_input where in-article OOVs are represented by their temporary OOV id; also store the in-article OOVs words themselves\n",
        "    self.enc_input_extend_vocab, self.article_oovs = article2ids(article_words, vocab)     #data\n",
        "\n",
        "    # Get a verison of the reference summary where in-article OOVs are represented by their temporary article OOV id\n",
        "    abs_ids_extend_vocab = abstract2ids(abstract_words, vocab, self.article_oovs)           #data\n",
        "\n",
        "    # Get decoder target sequence\n",
        "    _, self.target = self.get_dec_inp_targ_seqs(abs_ids_extend_vocab, max_dec_steps, start_decoding, stop_decoding)\n",
        "\n",
        "    # Store the original strings\n",
        "    self.original_article = article\n",
        "    self.original_abstract = abstract\n",
        "    self.original_abstract_sents = abstract_sentences\n",
        "\n",
        "\n",
        "\n",
        "  def get_dec_inp_targ_seqs(self, sequence, max_len, start_id, stop_id):\n",
        "    inp = [start_id] + sequence[:]\n",
        "    target = sequence[:]\n",
        "    if len(inp) > max_len: # truncate\n",
        "      inp = inp[:max_len]\n",
        "      target = target[:max_len] # no end_token\n",
        "    else: # no truncation\n",
        "      target.append(stop_id) # end token\n",
        "    assert len(inp) == len(target)\n",
        "    return inp, target\n",
        "\n",
        "\n",
        "  def pad_decoder_inp_targ(self, max_len, pad_id):\n",
        "    while len(self.dec_input) < max_len:\n",
        "      self.dec_input.append(pad_id)\n",
        "    while len(self.target) < max_len:\n",
        "      self.target.append(pad_id)\n",
        "\n",
        "\n",
        "  def pad_encoder_input(self, max_len, pad_id):\n",
        "    while len(self.enc_input) < max_len:\n",
        "      self.enc_input.append(pad_id)\n",
        "    while len(self.enc_input_extend_vocab) < max_len:\n",
        "      self.enc_input_extend_vocab.append(pad_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhnb8bmjylfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batch(object):\n",
        "  def __init__(self, example_list, vocab, batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.pad_id = vocab.word2id(PAD_TOKEN) # id of the PAD token used to pad sequences\n",
        "    self.init_encoder_seq(example_list) # initialize the input to the encoder\n",
        "    self.init_decoder_seq(example_list) # initialize the input and targets for the decoder\n",
        "    self.store_orig_strings(example_list) # store the original strings\n",
        "\n",
        "\n",
        "  def init_encoder_seq(self, example_list):\n",
        "    # Determine the maximum length of the encoder input sequence in this batch\n",
        "    max_enc_seq_len = max([ex.enc_len for ex in example_list])\n",
        "\n",
        "    # Pad the encoder input sequences up to the length of the longest sequence\n",
        "    for ex in example_list:\n",
        "      ex.pad_encoder_input(max_enc_seq_len, self.pad_id)\n",
        "\n",
        "    # Initialize the numpy arrays\n",
        "    # Note: our enc_batch can have different length (second dimension) for each batch because we use dynamic_rnn for the encoder.\n",
        "    self.enc_batch = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.int32)\n",
        "    self.enc_lens = np.zeros((self.batch_size), dtype=np.int32)\n",
        "    self.enc_padding_mask = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.float32)\n",
        "\n",
        "    # Fill in the numpy arrays\n",
        "    for i, ex in enumerate(example_list):\n",
        "      self.enc_batch[i, :] = ex.enc_input[:]\n",
        "      self.enc_lens[i] = ex.enc_len\n",
        "      for j in range(ex.enc_len):\n",
        "        self.enc_padding_mask[i][j] = 1\n",
        "\n",
        "    # For pointer-generator mode, need to store some extra info\n",
        "    # Determine the max number of in-article OOVs in this batch\n",
        "    self.max_art_oovs = max([len(ex.article_oovs) for ex in example_list])\n",
        "    # Store the in-article OOVs themselves\n",
        "    self.art_oovs = [ex.article_oovs for ex in example_list]\n",
        "    # Store the version of the enc_batch that uses the article OOV ids\n",
        "    self.enc_batch_extend_vocab = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.int32)\n",
        "    for i, ex in enumerate(example_list):\n",
        "      self.enc_batch_extend_vocab[i, :] = ex.enc_input_extend_vocab[:]\n",
        "\n",
        "  def init_decoder_seq(self, example_list):\n",
        "    # Pad the inputs and targets\n",
        "    for ex in example_list:\n",
        "      ex.pad_decoder_inp_targ(max_dec_steps, self.pad_id)           #config\n",
        "\n",
        "    # Initialize the numpy arrays.\n",
        "    self.dec_batch = np.zeros((self.batch_size, max_dec_steps), dtype=np.int32)         #config\n",
        "    self.target_batch = np.zeros((self.batch_size, max_dec_steps), dtype=np.int32)      #config\n",
        "    # self.dec_padding_mask = np.zeros((self.batch_size, config.max_dec_steps), dtype=np.float32)\n",
        "    self.dec_lens = np.zeros((self.batch_size), dtype=np.int32)\n",
        "\n",
        "    # Fill in the numpy arrays\n",
        "    for i, ex in enumerate(example_list):\n",
        "      self.dec_batch[i, :] = ex.dec_input[:]\n",
        "      self.target_batch[i, :] = ex.target[:]\n",
        "      self.dec_lens[i] = ex.dec_len\n",
        "      # for j in range(ex.dec_len):\n",
        "      #   self.dec_padding_mask[i][j] = 1\n",
        "\n",
        "  def store_orig_strings(self, example_list):\n",
        "    self.original_articles = [ex.original_article for ex in example_list] # list of lists\n",
        "    self.original_abstracts = [ex.original_abstract for ex in example_list] # list of lists\n",
        "    self.original_abstracts_sents = [ex.original_abstract_sents for ex in example_list] # list of list of lists\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWAwkoM9ylhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batcher(object):\n",
        "  BATCH_QUEUE_MAX = 1000 # max number of batches the batch_queue can hold\n",
        "\n",
        "  def __init__(self, data_path, vocab, mode, batch_size, single_pass):\n",
        "    self._data_path = data_path\n",
        "    self._vocab = vocab\n",
        "    self._single_pass = single_pass\n",
        "    self.mode = mode\n",
        "    self.batch_size = batch_size\n",
        "    # Initialize a queue of Batches waiting to be used, and a queue of Examples waiting to be batched\n",
        "    self._batch_queue = Queue.Queue(self.BATCH_QUEUE_MAX)\n",
        "    self._example_queue = Queue.Queue(self.BATCH_QUEUE_MAX * self.batch_size)\n",
        "\n",
        "    # Different settings depending on whether we're in single_pass mode or not\n",
        "    if single_pass:\n",
        "      self._num_example_q_threads = 1 # just one thread, so we read through the dataset just once\n",
        "      self._num_batch_q_threads = 1  # just one thread to batch examples\n",
        "      self._bucketing_cache_size = 1 # only load one batch's worth of examples before bucketing; this essentially means no bucketing\n",
        "      self._finished_reading = False # this will tell us when we're finished reading the dataset\n",
        "    else:\n",
        "      self._num_example_q_threads = 1 #16 # num threads to fill example queue\n",
        "      self._num_batch_q_threads = 1 #4  # num threads to fill batch queue\n",
        "      self._bucketing_cache_size = 1 #100 # how many batches-worth of examples to load into cache before bucketing\n",
        "\n",
        "    # Start the threads that load the queues\n",
        "    self._example_q_threads = []\n",
        "    for _ in range(self._num_example_q_threads):\n",
        "      self._example_q_threads.append(Thread(target=self.fill_example_queue))\n",
        "      self._example_q_threads[-1].daemon = True\n",
        "      self._example_q_threads[-1].start()\n",
        "    self._batch_q_threads = []\n",
        "    for _ in range(self._num_batch_q_threads):\n",
        "      self._batch_q_threads.append(Thread(target=self.fill_batch_queue))\n",
        "      self._batch_q_threads[-1].daemon = True\n",
        "      self._batch_q_threads[-1].start()\n",
        "\n",
        "    # Start a thread that watches the other threads and restarts them if they're dead\n",
        "    if not single_pass: # We don't want a watcher in single_pass mode because the threads shouldn't run forever\n",
        "      self._watch_thread = Thread(target=self.watch_threads)\n",
        "      self._watch_thread.daemon = True\n",
        "      self._watch_thread.start()\n",
        "\n",
        "  def next_batch(self):\n",
        "    # If the batch queue is empty, print a warning\n",
        "    if self._batch_queue.qsize() == 0:\n",
        "      # tf.logging.warning('Bucket input queue is empty when calling next_batch. Bucket queue size: %i, Input queue size: %i', self._batch_queue.qsize(), self._example_queue.qsize())\n",
        "      if self._single_pass and self._finished_reading:\n",
        "        tf.logging.info(\"Finished reading dataset in single_pass mode.\")\n",
        "        return None\n",
        "\n",
        "    batch = self._batch_queue.get() # get the next Batch\n",
        "    return batch\n",
        "\n",
        "  def fill_example_queue(self):\n",
        "    input_gen = self.text_generator(example_generator(self._data_path, self._single_pass))\n",
        "\n",
        "    while True:\n",
        "      try:\n",
        "        (article, abstract) = next(input_gen) # read the next example from file. article and abstract are both strings.\n",
        "      except StopIteration: # if there are no more examples:\n",
        "        tf.logging.info(\"The example generator for this example queue filling thread has exhausted data.\")\n",
        "        if self._single_pass:\n",
        "          tf.logging.info(\"single_pass mode is on, so we've finished reading dataset. This thread is stopping.\")\n",
        "          self._finished_reading = True\n",
        "          break\n",
        "        else:\n",
        "          raise Exception(\"single_pass mode is off but the example generator is out of data; error.\")\n",
        "\n",
        "      # abstract_sentences = [sent.strip() for sent in data.abstract2sents(abstract)] # Use the <s> and </s> tags in abstract to get a list of sentences.\n",
        "      abstract_sentences = [abstract.strip()]\n",
        "      example = Example(article, abstract_sentences, self._vocab) # Process into an Example.\n",
        "      self._example_queue.put(example) # place the Example in the example queue.\n",
        "\n",
        "  def fill_batch_queue(self):\n",
        "    while True:\n",
        "      if self.mode == 'decode':\n",
        "        # beam search decode mode single example repeated in the batch\n",
        "        ex = self._example_queue.get()\n",
        "        b = [ex for _ in range(self.batch_size)]\n",
        "        self._batch_queue.put(Batch(b, self._vocab, self.batch_size))\n",
        "      else:\n",
        "        # Get bucketing_cache_size-many batches of Examples into a list, then sort\n",
        "        inputs = []\n",
        "        for _ in range(self.batch_size * self._bucketing_cache_size):\n",
        "          inputs.append(self._example_queue.get())\n",
        "        inputs = sorted(inputs, key=lambda inp: inp.enc_len, reverse=True) # sort by length of encoder sequence\n",
        "\n",
        "        # Group the sorted Examples into batches, optionally shuffle the batches, and place in the batch queue.\n",
        "        batches = []\n",
        "        for i in range(0, len(inputs), self.batch_size):\n",
        "          batches.append(inputs[i:i + self.batch_size])\n",
        "        if not self._single_pass:\n",
        "          shuffle(batches)\n",
        "        for b in batches:  # each b is a list of Example objects\n",
        "          self._batch_queue.put(Batch(b, self._vocab, self.batch_size))\n",
        "\n",
        "  def watch_threads(self):\n",
        "    while True:\n",
        "      tf.logging.info(\n",
        "        'Bucket queue size: %i, Input queue size: %i',\n",
        "        self._batch_queue.qsize(), self._example_queue.qsize())\n",
        "\n",
        "      time.sleep(60)\n",
        "      for idx,t in enumerate(self._example_q_threads):\n",
        "        if not t.is_alive(): # if the thread is dead\n",
        "          tf.logging.error('Found example queue thread dead. Restarting.')\n",
        "          new_t = Thread(target=self.fill_example_queue)\n",
        "          self._example_q_threads[idx] = new_t\n",
        "          new_t.daemon = True\n",
        "          new_t.start()\n",
        "      for idx,t in enumerate(self._batch_q_threads):\n",
        "        if not t.is_alive(): # if the thread is dead\n",
        "          tf.logging.error('Found batch queue thread dead. Restarting.')\n",
        "          new_t = Thread(target=self.fill_batch_queue)\n",
        "          self._batch_q_threads[idx] = new_t\n",
        "          new_t.daemon = True\n",
        "          new_t.start()\n",
        "\n",
        "\n",
        "  def text_generator(self, example_generator):\n",
        "    while True:\n",
        "      e = next(example_generator) # e is a tf.Example\n",
        "      try:\n",
        "        article_text = e.features.feature['article'].bytes_list.value[0] # the article text was saved under the key 'article' in the data files\n",
        "        abstract_text = e.features.feature['abstract'].bytes_list.value[0] # the abstract text was saved under the key 'abstract' in the data files\n",
        "        article_text = article_text.decode()\n",
        "        abstract_text = abstract_text.decode()\n",
        "      except ValueError:\n",
        "        tf.logging.error('Failed to get article or abstract from example')\n",
        "        continue\n",
        "      if len(article_text)==0: # See https://github.com/abisee/pointer-generator/issues/1\n",
        "        #tf.logging.warning('Found an example with empty article text. Skipping it.')\n",
        "        continue\n",
        "      else:\n",
        "        yield (article_text, abstract_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADWyLPCc9Q9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwlnlghI9RCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzJgWpDMDH3d",
        "colab_type": "text"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T58cEFYX9REw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch as T\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def init_lstm_wt(lstm):\n",
        "    for name, _ in lstm.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            wt = getattr(lstm, name)\n",
        "            #print(wt)\n",
        "            wt.data = wt.data.uniform_(-rand_unif_init_mag, rand_unif_init_mag)\n",
        "            #wt.uniform_(-rand_unif_init_mag, rand_unif_init_mag)     #commit\n",
        "        elif 'bias' in name:\n",
        "            # set forget bias to 1\n",
        "            bias = getattr(lstm, name)\n",
        "            n = bias.size(0)\n",
        "            start, end = n // 4, n // 2\n",
        "\n",
        "            bias.data = bias.data.fill_(0.)\n",
        "            #bias.fill_(0.)\n",
        "            bias.data[start:end].fill_(1.)\n",
        "\n",
        "def init_linear_wt(linear):\n",
        "\n",
        "    linear.weight.data = linear.weight.data.normal_(std=trunc_norm_init_std)\n",
        "    #linear.weight.normal_(std=trunc_norm_init_std)\n",
        "    if linear.bias is not None:\n",
        "        linear.bias.data = linear.bias.data.normal_(std=trunc_norm_init_std)\n",
        "        #linear.bias.normal_(std=trunc_norm_init_std)\n",
        "\n",
        "def init_wt_normal(wt):\n",
        "    wt.data = wt.data.normal_(std=trunc_norm_init_std)\n",
        "    #wt.normal_(std=trunc_norm_init_std)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=1, batch_first=True, bidirectional=True)    #batch_true: If True, then the input and output tensors are provided as (batch, seq, feature)\n",
        "        init_lstm_wt(self.lstm)\n",
        "\n",
        "        self.reduce_h = nn.Linear(hidden_dim * 2, hidden_dim)        #hidden state \n",
        "        init_linear_wt(self.reduce_h)\n",
        "        self.reduce_c = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        init_linear_wt(self.reduce_c)\n",
        "\n",
        "    def forward(self, x, seq_lens):\n",
        "        packed = pack_padded_sequence(x, seq_lens, batch_first=True)    #accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly\n",
        "        enc_out, enc_hid = self.lstm(packed)                            # tensor containing the hidden state for t = seq_len. and tensor containing the cell state for t = seq_len.\n",
        "        enc_out,_ = pad_packed_sequence(enc_out, batch_first=True)      \n",
        "        enc_out = enc_out.contiguous()                              #bs, n_seq, 2*n_hid\n",
        "        h, c = enc_hid                                              #shape of h: 2, bs, n_hid\n",
        "        h = T.cat(list(h), dim=1)                                   #bs, 2*n_hid\n",
        "        c = T.cat(list(c), dim=1)\n",
        "        h_reduced = F.relu(self.reduce_h(h))                        #bs,n_hid\n",
        "        c_reduced = F.relu(self.reduce_c(c))\n",
        "        return enc_out, (h_reduced, c_reduced)\n",
        "\n",
        "\n",
        "class encoder_attention(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(encoder_attention, self).__init__()\n",
        "        self.W_h = nn.Linear(hidden_dim * 2, hidden_dim * 2, bias=False)    #no bias just the weight matrix\n",
        "        self.W_s = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
        "        self.v = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, st_hat, h, enc_padding_mask, sum_temporal_srcs):              #INTRA-TEMPORAL ATTENTION ON INPUT SEQUENCE in paper?\n",
        "        ''' Perform attention over encoder hidden states\n",
        "        :param st_hat: decoder hidden state at current time step\n",
        "        :param h: encoder hidden states\n",
        "        :param enc_padding_mask:\n",
        "        :param sum_temporal_srcs: if using intra-temporal attention, contains summation of attention weights from previous decoder time steps\n",
        "        '''\n",
        "\n",
        "        # Standard attention technique (eq 1 in https://arxiv.org/pdf/1704.04368.pdf)\n",
        "        et = self.W_h(h)                        # bs,n_seq,2*n_hid\n",
        "        dec_fea = self.W_s(st_hat).unsqueeze(1) # bs,1,2*n_hid\n",
        "        et = et + dec_fea\n",
        "        et = T.tanh(et)                         # bs,n_seq,2*n_hid\n",
        "        et = self.v(et).squeeze(2)              # bs,n_seq\n",
        "\n",
        "        # intra-temporal attention     (eq 3 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "        if intra_encoder:\n",
        "            exp_et = T.exp(et)\n",
        "            if sum_temporal_srcs is None:\n",
        "                et1 = exp_et\n",
        "                sum_temporal_srcs  = get_cuda(T.FloatTensor(et.size()).fill_(1e-10)) + exp_et           #defined temporal score\n",
        "            else:\n",
        "                et1 = exp_et/sum_temporal_srcs  #bs, n_seq\n",
        "                sum_temporal_srcs = sum_temporal_srcs + exp_et\n",
        "        else:\n",
        "            et1 = F.softmax(et, dim=1)\n",
        "\n",
        "        # assign 0 probability for padded elements\n",
        "        at = et1 * enc_padding_mask\n",
        "        normalization_factor = at.sum(1, keepdim=True)\n",
        "        at = at / normalization_factor\n",
        "\n",
        "        at = at.unsqueeze(1)                    #bs,1,n_seq\n",
        "        # Compute encoder context vector\n",
        "        ct_e = T.bmm(at, h)                     #bs, 1, 2*n_hid, batch matrix-matrix product of matrices (temporal score and encoder hidden state)\n",
        "        ct_e = ct_e.squeeze(1)\n",
        "        at = at.squeeze(1)\n",
        "\n",
        "        return ct_e, at, sum_temporal_srcs\n",
        "\n",
        "class decoder_attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(decoder_attention, self).__init__()\n",
        "        if intra_decoder:\n",
        "            self.W_prev = nn.Linear(hidden_dim, hidden_dim, bias=False)        #weight\n",
        "            self.W_s = nn.Linear(hidden_dim, hidden_dim)\n",
        "            self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, s_t, prev_s):\n",
        "        '''Perform intra_decoder attention\n",
        "        Args\n",
        "        :param s_t: hidden state of decoder at current time step\n",
        "        :param prev_s: If intra_decoder attention, contains list of previous decoder hidden states\n",
        "        '''\n",
        "        if intra_decoder is False:\n",
        "            ct_d = get_cuda(T.zeros(s_t.size()))\n",
        "        elif prev_s is None:\n",
        "            ct_d = get_cuda(T.zeros(s_t.size()))\n",
        "            prev_s = s_t.unsqueeze(1)               #bs, 1, n_hid\n",
        "        else:\n",
        "            # Standard attention technique (eq 1 in https://arxiv.org/pdf/1704.04368.pdf)      e = v tanh(Wh + Ws + b)\n",
        "            et = self.W_prev(prev_s)                # bs,t-1,n_hid\n",
        "            dec_fea = self.W_s(s_t).unsqueeze(1)    # bs,1,n_hid\n",
        "            et = et + dec_fea\n",
        "            et = T.tanh(et)                         # bs,t-1,n_hid\n",
        "            et = self.v(et).squeeze(2)              # bs,t-1\n",
        "            # intra-decoder attention     (eq 7 & 8 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "            at = F.softmax(et, dim=1).unsqueeze(1)  #bs, 1, t-1,  alpha\n",
        "            ct_d = T.bmm(at, prev_s).squeeze(1)     #bs, n_hid\n",
        "            prev_s = T.cat([prev_s, s_t.unsqueeze(1)], dim=1)    #bs, t, n_hid, keep adding previous hidden state \n",
        "\n",
        "        return ct_d, prev_s                 #decoder context vector, previous decoder hidden states\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.enc_attention = encoder_attention()\n",
        "        self.dec_attention = decoder_attention()\n",
        "        self.x_context = nn.Linear(hidden_dim*2 + emb_dim, emb_dim)\n",
        "\n",
        "        self.lstm = nn.LSTMCell(emb_dim, hidden_dim)\n",
        "        init_lstm_wt(self.lstm)\n",
        "\n",
        "        self.p_gen_linear = nn.Linear(hidden_dim * 5 + emb_dim, 1)\n",
        "\n",
        "        #p_vocab\n",
        "        self.V = nn.Linear(hidden_dim*4, hidden_dim)\n",
        "        self.V1 = nn.Linear(hidden_dim, vocab_size)\n",
        "        init_linear_wt(self.V1)\n",
        "\n",
        "    def forward(self, x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s):\n",
        "        x = self.x_context(T.cat([x_t, ct_e], dim=1))\n",
        "        s_t = self.lstm(x, s_t)\n",
        "\n",
        "        dec_h, dec_c = s_t\n",
        "        st_hat = T.cat([dec_h, dec_c], dim=1)\n",
        "        ct_e, attn_dist, sum_temporal_srcs = self.enc_attention(st_hat, enc_out, enc_padding_mask, sum_temporal_srcs)\n",
        "\n",
        "        ct_d, prev_s = self.dec_attention(dec_h, prev_s)        #intra-decoder attention\n",
        "\n",
        "        p_gen = T.cat([ct_e, ct_d, st_hat, x], 1)\n",
        "        p_gen = self.p_gen_linear(p_gen)            # bs,1\n",
        "        p_gen = T.sigmoid(p_gen)                    # bs,1\n",
        "\n",
        "        out = T.cat([dec_h, ct_e, ct_d], dim=1)     # bs, 4*n_hid\n",
        "        out = self.V(out)                           # bs,n_hid\n",
        "        out = self.V1(out)                          # bs, n_vocab\n",
        "        vocab_dist = F.softmax(out, dim=1)\n",
        "        vocab_dist = p_gen * vocab_dist\n",
        "        attn_dist_ = (1 - p_gen) * attn_dist\n",
        "\n",
        "        # pointer mechanism (as suggested in eq 9 https://arxiv.org/pdf/1704.04368.pdf)\n",
        "        if extra_zeros is not None:\n",
        "            vocab_dist = T.cat([vocab_dist, extra_zeros], dim=1)\n",
        "        final_dist = vocab_dist.scatter_add(1, enc_batch_extend_vocab, attn_dist_)\n",
        "\n",
        "        return final_dist, s_t, ct_e, sum_temporal_srcs, prev_s\n",
        "\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        self.embeds = nn.Embedding(vocab_size, emb_dim)\n",
        "        init_wt_normal(self.embeds.weight)\n",
        "\n",
        "        self.encoder = get_cuda(self.encoder)\n",
        "        self.decoder = get_cuda(self.decoder)\n",
        "        self.embeds = get_cuda(self.embeds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reyzRSsHDeSU",
        "colab_type": "text"
      },
      "source": [
        "#Training util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg1KAqN79RAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch as T\n",
        "\n",
        "\n",
        "def get_cuda(tensor):\n",
        "    if T.cuda.is_available():\n",
        "        tensor = tensor.cuda()\n",
        "    return tensor\n",
        "\n",
        "def get_enc_data(batch):\n",
        "    batch_size = len(batch.enc_lens)\n",
        "    enc_batch = T.from_numpy(batch.enc_batch).long()\n",
        "    enc_padding_mask = T.from_numpy(batch.enc_padding_mask).float()\n",
        "\n",
        "    enc_lens = batch.enc_lens\n",
        "\n",
        "    ct_e = T.zeros(batch_size, 2*   hidden_dim)   #config.hidden_dim\n",
        "\n",
        "    enc_batch = get_cuda(enc_batch)\n",
        "    enc_padding_mask = get_cuda(enc_padding_mask)\n",
        "\n",
        "    ct_e = get_cuda(ct_e)\n",
        "\n",
        "    enc_batch_extend_vocab = None\n",
        "    if batch.enc_batch_extend_vocab is not None:\n",
        "        enc_batch_extend_vocab = T.from_numpy(batch.enc_batch_extend_vocab).long()\n",
        "        enc_batch_extend_vocab = get_cuda(enc_batch_extend_vocab)\n",
        "\n",
        "    extra_zeros = None\n",
        "    if batch.max_art_oovs > 0:\n",
        "        extra_zeros = T.zeros(batch_size, batch.max_art_oovs)\n",
        "        extra_zeros = get_cuda(extra_zeros)\n",
        "\n",
        "\n",
        "    return enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, ct_e\n",
        "\n",
        "\n",
        "def get_dec_data(batch):\n",
        "    dec_batch = T.from_numpy(batch.dec_batch).long()\n",
        "    dec_lens = batch.dec_lens\n",
        "    max_dec_len = np.max(dec_lens)\n",
        "    dec_lens = T.from_numpy(batch.dec_lens).float()\n",
        "\n",
        "    target_batch = T.from_numpy(batch.target_batch).long()\n",
        "\n",
        "    dec_batch = get_cuda(dec_batch)\n",
        "    dec_lens = get_cuda(dec_lens)\n",
        "    target_batch = get_cuda(target_batch)\n",
        "\n",
        "    return dec_batch, max_dec_len, dec_lens, target_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXzuhqacDo30",
        "colab_type": "code",
        "outputId": "19b5b535-b15c-412d-8fc4-78920c468760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "T.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWeh2gEgHdae",
        "colab_type": "text"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QlvbvITDo6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"    #Set cuda device\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from model import Model\n",
        "\n",
        "#from data_util import config, data\n",
        "#from data_util.batcher import Batcher\n",
        "#from data_util.data import Vocab\n",
        "#from train_util import *\n",
        "from torch.distributions import Categorical\n",
        "from rouge import Rouge\n",
        "from numpy import random\n",
        "import argparse\n",
        "\n",
        "random.seed(123)\n",
        "T.manual_seed(123)\n",
        "if T.cuda.is_available():\n",
        "    T.cuda.manual_seed_all(123)\n",
        "\n",
        "class Train(object):\n",
        "    def __init__(self, opt):\n",
        "        self.vocab = Vocab(vocab_path, vocab_size)\n",
        "        self.batcher = Batcher(train_data_path, self.vocab, mode='train',\n",
        "                               batch_size=batch_size, single_pass=False)            #config\n",
        "        self.opt = opt\n",
        "        self.start_id = self.vocab.word2id(START_DECODING)          #data.\n",
        "        self.end_id = self.vocab.word2id(STOP_DECODING)\n",
        "        self.pad_id = self.vocab.word2id(PAD_TOKEN)\n",
        "        self.unk_id = self.vocab.word2id(UNKNOWN_TOKEN)\n",
        "        time.sleep(5)\n",
        "\n",
        "    def save_model(self, iter):\n",
        "        save_path = save_model_path + \"/%07d.tar\" % iter\n",
        "        T.save({\n",
        "            \"iter\": iter + 1,\n",
        "            \"model_dict\": self.model.state_dict(),\n",
        "            \"trainer_dict\": self.trainer.state_dict()\n",
        "        }, save_path)\n",
        "\n",
        "    def setup_train(self):\n",
        "        self.model = Model()\n",
        "        self.model = get_cuda(self.model)\n",
        "        self.trainer = T.optim.Adam(self.model.parameters(), lr=lr)\n",
        "        start_iter = 0\n",
        "        if self.opt.load_model is not None:\n",
        "            load_model_path = os.path.join(save_model_path, self.opt.load_model)\n",
        "            checkpoint = T.load(load_model_path)\n",
        "            start_iter = checkpoint[\"iter\"]\n",
        "            self.model.load_state_dict(checkpoint[\"model_dict\"])\n",
        "            self.trainer.load_state_dict(checkpoint[\"trainer_dict\"])\n",
        "            print(\"Loaded model at \" + load_model_path)\n",
        "        if self.opt.new_lr is not None:\n",
        "            self.trainer = T.optim.Adam(self.model.parameters(), lr=self.opt.new_lr)\n",
        "        return start_iter\n",
        "\n",
        "    def train_batch_MLE(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, batch):\n",
        "        ''' Calculate Negative Log Likelihood Loss for the given batch. In order to reduce exposure bias,\n",
        "                pass the previous generated token as input with a probability of 0.25 instead of ground truth label\n",
        "        Args:\n",
        "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
        "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
        "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
        "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
        "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
        "        :param batch: batch object\n",
        "        '''\n",
        "        dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
        "        step_losses = []\n",
        "        s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
        "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))                             #Input to the decoder\n",
        "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "        for t in range(min(max_dec_len, max_dec_steps)):\n",
        "            use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
        "            x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
        "            x_t = self.model.embeds(x_t)\n",
        "            final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
        "            target = target_batch[:, t]\n",
        "            log_probs = T.log(final_dist + eps)\n",
        "            step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=self.pad_id)\n",
        "            step_losses.append(step_loss)\n",
        "            x_t = T.multinomial(final_dist, 1).squeeze()                                            #Sample words from final distribution which can be used as input in next time step\n",
        "            is_oov = (x_t >= vocab_size).long()                                              #Mask indicating whether sampled word is OOV\n",
        "            x_t = (1 - is_oov) * x_t.detach() + (is_oov) * self.unk_id                              #Replace OOVs with [UNK] token\n",
        "\n",
        "        losses = T.sum(T.stack(step_losses, 1), 1)                                                  #unnormalized losses for each example in the batch; (batch_size)\n",
        "        batch_avg_loss = losses / dec_lens                                                          #Normalized losses; (batch_size)\n",
        "        mle_loss = T.mean(batch_avg_loss)                                                           #Average batch loss\n",
        "        return mle_loss\n",
        "\n",
        "    def train_batch_RL(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, article_oovs, greedy):\n",
        "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
        "        Args\n",
        "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
        "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
        "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
        "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
        "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
        "        :param article_oovs: Batch containing list of OOVs in each example\n",
        "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
        "        Returns:\n",
        "        :decoded_strs: List of decoded sentences\n",
        "        :log_probs: Log probabilities of sampled words\n",
        "        '''\n",
        "        s_t = enc_hidden                                                                            #Decoder hidden states\n",
        "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))                             #Input to the decoder\n",
        "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "        inds = []                                                                                   #Stores sampled indices for each time step\n",
        "        decoder_padding_mask = []                                                                   #Stores padding masks of generated samples\n",
        "        log_probs = []                                                                              #Stores log probabilites of generated samples\n",
        "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(1))                                        #Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
        "\n",
        "        for t in range(max_dec_steps):\n",
        "            x_t = self.model.embeds(x_t)\n",
        "            probs, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
        "            if greedy is False:\n",
        "                multi_dist = Categorical(probs)\n",
        "                x_t = multi_dist.sample()                                                           #perform multinomial sampling\n",
        "                log_prob = multi_dist.log_prob(x_t)\n",
        "                log_probs.append(log_prob)\n",
        "            else:\n",
        "                _, x_t = T.max(probs, dim=1)                                                        #perform greedy sampling\n",
        "            x_t = x_t.detach()\n",
        "            inds.append(x_t)\n",
        "            mask_t = get_cuda(T.zeros(len(enc_out)))                                                #Padding mask of batch for current time step\n",
        "            mask_t[mask == 1] = 1                                                                   #If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
        "            mask[(mask == 1) + (x_t == self.end_id) == 2] = 0                                       #If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
        "            decoder_padding_mask.append(mask_t)\n",
        "            is_oov = (x_t>=vocab_size).long()                                                #Mask indicating whether sampled word is OOV\n",
        "            x_t = (1-is_oov)*x_t + (is_oov)*self.unk_id                                             #Replace OOVs with [UNK] token\n",
        "\n",
        "        inds = T.stack(inds, dim=1)\n",
        "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
        "        if greedy is False:                                                                         #If multinomial based sampling, compute log probabilites of sampled words\n",
        "            log_probs = T.stack(log_probs, dim=1)\n",
        "            log_probs = log_probs * decoder_padding_mask                                            #Not considering sampled words with padding mask = 0\n",
        "            lens = T.sum(decoder_padding_mask, dim=1)                                               #Length of sampled sentence\n",
        "            log_probs = T.sum(log_probs, dim=1) / lens  # (bs,)                                     #compute normalizied log probability of a sentence\n",
        "        decoded_strs = []\n",
        "        for i in range(len(enc_out)):\n",
        "            id_list = inds[i].cpu().numpy()\n",
        "            oovs = article_oovs[i]\n",
        "            S = outputids2words(id_list, self.vocab, oovs)                                     #Generate sentence corresponding to sampled words\n",
        "            try:\n",
        "                end_idx = S.index(STOP_DECODING)\n",
        "                S = S[:end_idx]\n",
        "            except ValueError:\n",
        "                S = S\n",
        "            if len(S) < 2:                                                                           #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
        "                S = [\"xxx\"]\n",
        "            S = \" \".join(S)\n",
        "            decoded_strs.append(S)\n",
        "\n",
        "        return decoded_strs, log_probs\n",
        "\n",
        "    def reward_function(self, decoded_sents, original_sents):\n",
        "        rouge = Rouge()\n",
        "        try:\n",
        "            scores = rouge.get_scores(decoded_sents, original_sents)\n",
        "        except Exception:\n",
        "            print(\"Rouge failed for multi sentence evaluation.. Finding exact pair\")\n",
        "            scores = []\n",
        "            for i in range(len(decoded_sents)):\n",
        "                try:\n",
        "                    score = rouge.get_scores(decoded_sents[i], original_sents[i])\n",
        "                except Exception:\n",
        "                    print(\"Error occured at:\")\n",
        "                    print(\"decoded_sents:\", decoded_sents[i])\n",
        "                    print(\"original_sents:\", original_sents[i])\n",
        "                    score = [{\"rouge-l\":{\"f\":0.0}}]\n",
        "                scores.append(score[0])\n",
        "        rouge_l_f1 = [score[\"rouge-l\"][\"f\"] for score in scores]\n",
        "        rouge_l_f1 = get_cuda(T.FloatTensor(rouge_l_f1))\n",
        "        return rouge_l_f1\n",
        "\n",
        "    # def write_to_file(self, decoded, max, original, sample_r, baseline_r, iter):\n",
        "    #     with open(\"temp.txt\", \"w\") as f:\n",
        "    #         f.write(\"iter:\"+str(iter)+\"\\n\")\n",
        "    #         for i in range(len(original)):\n",
        "    #             f.write(\"dec: \"+decoded[i]+\"\\n\")\n",
        "    #             f.write(\"max: \"+max[i]+\"\\n\")\n",
        "    #             f.write(\"org: \"+original[i]+\"\\n\")\n",
        "    #             f.write(\"Sample_R: %.4f, Baseline_R: %.4f\\n\\n\"%(sample_r[i].item(), baseline_r[i].item()))\n",
        "\n",
        "\n",
        "    def train_one_batch(self, batch, iter):\n",
        "        enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
        "\n",
        "        enc_batch = self.model.embeds(enc_batch)                                                    #Get embeddings for encoder input\n",
        "        enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n",
        "\n",
        "        # -------------------------------Summarization-----------------------\n",
        "        if self.opt.train_mle == \"yes\":                                                             #perform MLE training\n",
        "            mle_loss = self.train_batch_MLE(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch)\n",
        "        else:\n",
        "            mle_loss = get_cuda(T.FloatTensor([0]))\n",
        "        # --------------RL training-----------------------------------------------------\n",
        "        if self.opt.train_rl == \"yes\":                                                              #perform reinforcement learning training\n",
        "            # multinomial sampling\n",
        "            sample_sents, RL_log_probs = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch.art_oovs, greedy=False)\n",
        "            with T.autograd.no_grad():\n",
        "                # greedy sampling\n",
        "                greedy_sents, _ = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch.art_oovs, greedy=True)\n",
        "\n",
        "            sample_reward = self.reward_function(sample_sents, batch.original_abstracts)\n",
        "            baseline_reward = self.reward_function(greedy_sents, batch.original_abstracts)\n",
        "            # if iter%200 == 0:\n",
        "            #     self.write_to_file(sample_sents, greedy_sents, batch.original_abstracts, sample_reward, baseline_reward, iter)\n",
        "            rl_loss = -(sample_reward - baseline_reward) * RL_log_probs                             #Self-critic policy gradient training (eq 15 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "            rl_loss = T.mean(rl_loss)\n",
        "\n",
        "            batch_reward = T.mean(sample_reward).item()\n",
        "        else:\n",
        "            rl_loss = get_cuda(T.FloatTensor([0]))\n",
        "            batch_reward = 0\n",
        "\n",
        "    # ------------------------------------------------------------------------------------\n",
        "        self.trainer.zero_grad()\n",
        "        (self.opt.mle_weight * mle_loss + self.opt.rl_weight * rl_loss).backward()\n",
        "        self.trainer.step()\n",
        "\n",
        "        return mle_loss.item(), batch_reward\n",
        "\n",
        "    def trainIters(self):                                       #training step\n",
        "        iter = self.setup_train()\n",
        "        count = mle_total = r_total = 0\n",
        "        while iter <= max_iterations:\n",
        "            batch = self.batcher.next_batch()\n",
        "            try:\n",
        "                mle_loss, r = self.train_one_batch(batch, iter)\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"-------------------Keyboard Interrupt------------------\")\n",
        "                sys.exit\n",
        "\n",
        "            mle_total += mle_loss\n",
        "            r_total += r\n",
        "            count += 1\n",
        "            iter += 1\n",
        "\n",
        "            if iter % 200 == 0:\n",
        "                mle_avg = mle_total / count\n",
        "                r_avg = r_total / count\n",
        "                print(\"iter:\", iter, \"mle_loss:\", \"%.3f\" % mle_avg, \"reward:\", \"%.4f\" % r_avg)\n",
        "                count = mle_total = r_total = 0\n",
        "\n",
        "            if iter % 1000 == 0:\n",
        "                self.save_model(iter)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNuWXL5xjrsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--train_mle', type=str, default=\"yes\")\n",
        "    parser.add_argument('--train_rl', type=str, default=\"no\")\n",
        "    parser.add_argument('--mle_weight', type=float, default=1.0)\n",
        "    parser.add_argument('--load_model', type=str, default=None)\n",
        "    parser.add_argument('--new_lr', type=float, default=None)\n",
        "\n",
        "    opt, unknown = parser.parse_known_args()\n",
        "    #opt = parser.parse_args()\n",
        "    opt.rl_weight = 1 - opt.mle_weight\n",
        "    print(\"Training mle: %s, Training rl: %s, mle weight: %.2f, rl weight: %.2f\"%(opt.train_mle, opt.train_rl, opt.mle_weight, opt.rl_weight))\n",
        "    print(\"intra_encoder:\", intra_encoder, \"intra_decoder:\",intra_decoder)          #config\n",
        "\n",
        "    train_processor = Train(opt)\n",
        "    train_processor.trainIters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFpoy4cnjyAU",
        "colab_type": "text"
      },
      "source": [
        "#find the best saved model on validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYtsQeqKj_-T",
        "colab_type": "text"
      },
      "source": [
        "##Beam search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZvqg1a8kBrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch as T\n",
        "\n",
        "\n",
        "class Beam(object):\n",
        "    def __init__(self, start_id, end_id, unk_id, hidden_state, context):\n",
        "        h,c = hidden_state                                              #(n_hid,)\n",
        "        self.tokens = T.LongTensor(beam_size,1).fill_(start_id)  #(beam, t) after t time steps\n",
        "        self.scores = T.FloatTensor(beam_size,1).fill_(-30)      #beam,1; Initial score of beams = -30\n",
        "        self.tokens, self.scores = get_cuda(self.tokens), get_cuda(self.scores)\n",
        "        self.scores[0][0] = 0                                           #At time step t=0, all beams should extend from a single beam. So, I am giving high initial score to 1st beam\n",
        "        self.hid_h = h.unsqueeze(0).repeat(beam_size, 1)         #beam, n_hid\n",
        "        self.hid_c = c.unsqueeze(0).repeat(beam_size, 1)         #beam, n_hid\n",
        "        self.context = context.unsqueeze(0).repeat(beam_size, 1) #beam, 2*n_hid\n",
        "        self.sum_temporal_srcs = None\n",
        "        self.prev_s = None\n",
        "        self.done = False\n",
        "        self.end_id = end_id\n",
        "        self.unk_id = unk_id\n",
        "\n",
        "    def get_current_state(self):\n",
        "        tokens = self.tokens[:,-1].clone()\n",
        "        for i in range(len(tokens)):\n",
        "            if tokens[i].item() >= vocab_size:\n",
        "                tokens[i] = self.unk_id\n",
        "        return tokens\n",
        "\n",
        "\n",
        "    def advance(self, prob_dist, hidden_state, context, sum_temporal_srcs, prev_s):\n",
        "        '''Perform beam search: Considering the probabilites of given n_beam x n_extended_vocab words, select first n_beam words that give high total scores\n",
        "        :param prob_dist: (beam, n_extended_vocab)\n",
        "        :param hidden_state: Tuple of (beam, n_hid) tensors\n",
        "        :param context:   (beam, 2*n_hidden)\n",
        "        :param sum_temporal_srcs:   (beam, n_seq)\n",
        "        :param prev_s:  (beam, t, n_hid)\n",
        "        '''\n",
        "        n_extended_vocab = prob_dist.size(1)\n",
        "        h, c = hidden_state\n",
        "        log_probs = T.log(prob_dist+eps)                         #beam, n_extended_vocab\n",
        "\n",
        "        scores = log_probs + self.scores                                #beam, n_extended_vocab\n",
        "        scores = scores.view(-1,1)                                      #beam*n_extended_vocab, 1\n",
        "        best_scores, best_scores_id = T.topk(input=scores, k=beam_size, dim=0)   #will be sorted in descending order of scores\n",
        "        self.scores = best_scores                                       #(beam,1); sorted\n",
        "        beams_order = best_scores_id.squeeze(1)/n_extended_vocab        #(beam,); sorted\n",
        "        best_words = best_scores_id%n_extended_vocab                    #(beam,1); sorted\n",
        "        self.hid_h = h[beams_order]                                     #(beam, n_hid); sorted\n",
        "        self.hid_c = c[beams_order]                                     #(beam, n_hid); sorted\n",
        "        self.context = context[beams_order]\n",
        "        if sum_temporal_srcs is not None:\n",
        "            self.sum_temporal_srcs = sum_temporal_srcs[beams_order]     #(beam, n_seq); sorted\n",
        "        if prev_s is not None:\n",
        "            self.prev_s = prev_s[beams_order]                           #(beam, t, n_hid); sorted\n",
        "        self.tokens = self.tokens[beams_order]                          #(beam, t); sorted\n",
        "        self.tokens = T.cat([self.tokens, best_words], dim=1)           #(beam, t+1); sorted\n",
        "\n",
        "        #End condition is when top-of-beam is EOS.\n",
        "        if best_words[0][0] == self.end_id:\n",
        "            self.done = True\n",
        "\n",
        "    def get_best(self):\n",
        "        best_token = self.tokens[0].cpu().numpy().tolist()              #Since beams are always in sorted (descending) order, 1st beam is the best beam\n",
        "        try:\n",
        "            end_idx = best_token.index(self.end_id)\n",
        "        except ValueError:\n",
        "            end_idx = len(best_token)\n",
        "        best_token = best_token[1:end_idx]\n",
        "        return best_token\n",
        "\n",
        "    def get_all(self):\n",
        "        all_tokens = []\n",
        "        for i in range(len(self.tokens)):\n",
        "            all_tokens.append(self.tokens[i].cpu().numpy())\n",
        "        return all_tokens\n",
        "\n",
        "\n",
        "def beam_search(enc_hid, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, model, start_id, end_id, unk_id):\n",
        "\n",
        "    batch_size = len(enc_hid[0])\n",
        "    beam_idx = T.LongTensor(list(range(batch_size)))\n",
        "    beams = [Beam(start_id, end_id, unk_id, (enc_hid[0][i], enc_hid[1][i]), ct_e[i]) for i in range(batch_size)]   #For each example in batch, create Beam object\n",
        "    n_rem = batch_size                                                  #Index of beams that are active, i.e: didn't generate [STOP] yet\n",
        "    sum_temporal_srcs = None                                            #Number of examples in batch that didn't generate [STOP] yet\n",
        "    prev_s = None\n",
        "\n",
        "    for t in range(max_dec_steps):\n",
        "        x_t = T.stack(\n",
        "            [beam.get_current_state() for beam in beams if beam.done == False]      #remaining(rem),beam\n",
        "        ).contiguous().view(-1)                                                     #(rem*beam,)\n",
        "        x_t = model.embeds(x_t)                                                 #rem*beam, n_emb\n",
        "\n",
        "        dec_h = T.stack(\n",
        "            [beam.hid_h for beam in beams if beam.done == False]                    #rem*beam,n_hid\n",
        "        ).contiguous().view(-1,hidden_dim)\n",
        "        dec_c = T.stack(\n",
        "            [beam.hid_c for beam in beams if beam.done == False]                    #rem,beam,n_hid\n",
        "        ).contiguous().view(-1,hidden_dim)                                   #rem*beam,n_hid\n",
        "\n",
        "        ct_e = T.stack(\n",
        "            [beam.context for beam in beams if beam.done == False]                  #rem,beam,n_hid\n",
        "        ).contiguous().view(-1,2*hidden_dim)                                 #rem,beam,n_hid\n",
        "\n",
        "        if sum_temporal_srcs is not None:\n",
        "            sum_temporal_srcs = T.stack(\n",
        "                [beam.sum_temporal_srcs for beam in beams if beam.done == False]\n",
        "            ).contiguous().view(-1, enc_out.size(1))                                #rem*beam, n_seq\n",
        "\n",
        "        if prev_s is not None:\n",
        "            prev_s = T.stack(\n",
        "                [beam.prev_s for beam in beams if beam.done == False]\n",
        "            ).contiguous().view(-1, t, hidden_dim)                           #rem*beam, t-1, n_hid\n",
        "\n",
        "\n",
        "        s_t = (dec_h, dec_c)\n",
        "        enc_out_beam = enc_out[beam_idx].view(n_rem,-1).repeat(1, beam_size).view(-1, enc_out.size(1), enc_out.size(2))\n",
        "        enc_pad_mask_beam = enc_padding_mask[beam_idx].repeat(1, beam_size).view(-1, enc_padding_mask.size(1))\n",
        "\n",
        "        extra_zeros_beam = None\n",
        "        if extra_zeros is not None:\n",
        "            extra_zeros_beam = extra_zeros[beam_idx].repeat(1, beam_size).view(-1, extra_zeros.size(1))\n",
        "        enc_extend_vocab_beam = enc_batch_extend_vocab[beam_idx].repeat(1, beam_size).view(-1, enc_batch_extend_vocab.size(1))\n",
        "\n",
        "        final_dist, (dec_h, dec_c), ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out_beam, enc_pad_mask_beam, ct_e, extra_zeros_beam, enc_extend_vocab_beam, sum_temporal_srcs, prev_s)              #final_dist: rem*beam, n_extended_vocab\n",
        "\n",
        "        final_dist = final_dist.view(n_rem, beam_size, -1)                   #final_dist: rem, beam, n_extended_vocab\n",
        "        dec_h = dec_h.view(n_rem, beam_size, -1)                             #rem, beam, n_hid\n",
        "        dec_c = dec_c.view(n_rem, beam_size, -1)                             #rem, beam, n_hid\n",
        "        ct_e = ct_e.view(n_rem, beam_size, -1)                             #rem, beam, 2*n_hid\n",
        "\n",
        "        if sum_temporal_srcs is not None:\n",
        "            sum_temporal_srcs = sum_temporal_srcs.view(n_rem, beam_size, -1) #rem, beam, n_seq\n",
        "\n",
        "        if prev_s is not None:\n",
        "            prev_s = prev_s.view(n_rem, beam_size, -1, hidden_dim)    #rem, beam, t\n",
        "\n",
        "        # For all the active beams, perform beam search\n",
        "        active = []         #indices of active beams after beam search\n",
        "\n",
        "        for i in range(n_rem):\n",
        "            b = beam_idx[i].item()\n",
        "            beam = beams[b]\n",
        "            if beam.done:\n",
        "                continue\n",
        "\n",
        "            sum_temporal_srcs_i = prev_s_i = None\n",
        "            if sum_temporal_srcs is not None:\n",
        "                sum_temporal_srcs_i = sum_temporal_srcs[i]                              #beam, n_seq\n",
        "            if prev_s is not None:\n",
        "                prev_s_i = prev_s[i]                                                #beam, t, n_hid\n",
        "            beam.advance(final_dist[i], (dec_h[i], dec_c[i]), ct_e[i], sum_temporal_srcs_i, prev_s_i)\n",
        "            if beam.done == False:\n",
        "                active.append(b)\n",
        "\n",
        "        if len(active) == 0:\n",
        "            break\n",
        "\n",
        "        beam_idx = T.LongTensor(active)\n",
        "        n_rem = len(beam_idx)\n",
        "\n",
        "    predicted_words = []\n",
        "    for beam in beams:\n",
        "        predicted_words.append(beam.get_best())\n",
        "\n",
        "    return predicted_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcXY4YgLkBxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX2zWjBjkB2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaN0AznskB0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W61bTdaukCu5",
        "colab_type": "text"
      },
      "source": [
        "##Eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkQkxkoSjru8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import time\n",
        "\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from rouge import Rouge\n",
        "import argparse\n",
        "\n",
        "def get_cuda(tensor):\n",
        "    if T.cuda.is_available():\n",
        "        tensor = tensor.cuda()\n",
        "    return tensor\n",
        "\n",
        "class Evaluate(object):\n",
        "    def __init__(self, data_path, opt, batch_size = batch_size):\n",
        "        self.vocab = Vocab(vocab_path, vocab_size)\n",
        "        self.batcher = Batcher(data_path, self.vocab, mode='eval',\n",
        "                               batch_size=batch_size, single_pass=True)\n",
        "        self.opt = opt\n",
        "        time.sleep(5)\n",
        "\n",
        "    def setup_valid(self):\n",
        "        self.model = Model()\n",
        "        self.model = get_cuda(self.model)\n",
        "        checkpoint = T.load(os.path.join(save_model_path, self.opt.load_model))\n",
        "        self.model.load_state_dict(checkpoint[\"model_dict\"])\n",
        "\n",
        "\n",
        "    def print_original_predicted(self, decoded_sents, ref_sents, article_sents, loadfile):\n",
        "        filename = \"test_\"+loadfile.split(\".\")[0]+\".txt\"\n",
        "    \n",
        "        with open(os.path.join(\"data\",filename), \"w\") as f:\n",
        "            for i in range(len(decoded_sents)):\n",
        "                f.write(\"article: \"+article_sents[i] + \"\\n\")\n",
        "                f.write(\"ref: \" + ref_sents[i] + \"\\n\")\n",
        "                f.write(\"dec: \" + decoded_sents[i] + \"\\n\\n\")\n",
        "\n",
        "    def evaluate_batch(self, print_sents = False):\n",
        "\n",
        "        self.setup_valid()\n",
        "        batch = self.batcher.next_batch()\n",
        "        start_id = self.vocab.word2id(START_DECODING)\n",
        "        end_id = self.vocab.word2id(STOP_DECODING)\n",
        "        unk_id = self.vocab.word2id(UNKNOWN_TOKEN)\n",
        "        decoded_sents = []\n",
        "        ref_sents = []\n",
        "        article_sents = []\n",
        "        rouge = Rouge()\n",
        "        while batch is not None:\n",
        "            enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, ct_e = get_enc_data(batch)\n",
        "\n",
        "            with T.autograd.no_grad():\n",
        "                enc_batch = self.model.embeds(enc_batch)\n",
        "                enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n",
        "\n",
        "            #-----------------------Summarization----------------------------------------------------\n",
        "            with T.autograd.no_grad():\n",
        "                pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, self.model, start_id, end_id, unk_id)\n",
        "\n",
        "            for i in range(len(pred_ids)):\n",
        "                decoded_words = outputids2words(pred_ids[i], self.vocab, batch.art_oovs[i])\n",
        "                if len(decoded_words) < 2:\n",
        "                    decoded_words = \"xxx\"\n",
        "                else:\n",
        "                    decoded_words = \" \".join(decoded_words)\n",
        "                decoded_sents.append(decoded_words)\n",
        "                abstract = batch.original_abstracts[i]\n",
        "                article = batch.original_articles[i]\n",
        "                ref_sents.append(abstract)\n",
        "                article_sents.append(article)\n",
        "\n",
        "            batch = self.batcher.next_batch()\n",
        "\n",
        "        load_file = self.opt.load_model\n",
        "\n",
        "        if print_sents:\n",
        "            self.print_original_predicted(decoded_sents, ref_sents, article_sents, load_file)\n",
        "\n",
        "        scores = rouge.get_scores(decoded_sents, ref_sents, avg = True)\n",
        "        if self.opt.task == \"test\":\n",
        "            print(load_file, \"scores:\", scores)\n",
        "        else:\n",
        "            rouge_l = scores[\"rouge-l\"][\"f\"]\n",
        "            print(load_file, \"rouge_l:\", \"%.4f\" % rouge_l)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpIyvn-Iczki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--task\", type=str, default=\"validate\", choices=[\"validate\",\"test\"])\n",
        "    parser.add_argument(\"--start_from\", type=str, default=\"0002000.tar\")\n",
        "    parser.add_argument(\"--load_model\", type=str, default=None)\n",
        "    #opt = parser.parse_args()\n",
        "    opt, unknown = parser.parse_known_args()\n",
        "\n",
        "    if opt.task == \"validate\":\n",
        "        saved_models = os.listdir(save_model_path)\n",
        "        saved_models.sort()\n",
        "        file_idx = saved_models.index(opt.start_from)\n",
        "        saved_models = saved_models[file_idx:]\n",
        "        for f in saved_models:\n",
        "            opt.load_model = f\n",
        "            eval_processor = Evaluate(valid_data_path, opt)\n",
        "            eval_processor.evaluate_batch()\n",
        "    else:   #test\n",
        "        eval_processor = Evaluate(test_data_path, opt)\n",
        "        eval_processor.evaluate_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxetG9tfn_dB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   ROUGE-1 refers to the overlap of unigram (each word) between the system and reference summaries.\n",
        "\n",
        "2.   ROUGE-2 refers to the overlap of bigrams between the system and reference summaries.\n",
        "\n",
        "3.   ROUGE-L: Longest Common Subsequence (LCS) based statistics. Longest common subsequence problem takes into account sentence level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically.\n",
        "\n",
        "4.   ROUGE-W: Weighted LCS-based statistics that favors consecutive LCSes .\n",
        "\n",
        "5.   ROUGE-S: Skip-bigram based co-occurrence statistics. Skip-bigram is any pair of words in their sentence order.\n",
        "\n",
        "6.   ROUGE-SU: Skip-bigram plus unigram-based co-occurrence statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-7wo_xyHxqr",
        "colab_type": "text"
      },
      "source": [
        "best model: 0006000.tar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jn5oGO0jrp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeXnOaB-DcPv",
        "colab_type": "text"
      },
      "source": [
        "#Train the best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDyTUhOfDenw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "405474a4-2c58-44bb-c14e-2e1de263ab4d"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--train_mle', type=str, default=\"yes\")\n",
        "    parser.add_argument('--train_rl', type=str, default=\"yes\")\n",
        "    parser.add_argument('--mle_weight', type=float, default=0.25)\n",
        "    parser.add_argument('--load_model', type=str, default='0006000.tar')\n",
        "    parser.add_argument('--new_lr', type=float, default=0.0001)\n",
        "\n",
        "    opt, unknown = parser.parse_known_args()\n",
        "    #opt = parser.parse_args()\n",
        "    opt.rl_weight = 1 - opt.mle_weight\n",
        "    print(\"Training mle: %s, Training rl: %s, mle weight: %.2f, rl weight: %.2f\"%(opt.train_mle, opt.train_rl, opt.mle_weight, opt.rl_weight))\n",
        "    print(\"intra_encoder:\", intra_encoder, \"intra_decoder:\",intra_decoder)          #config\n",
        "\n",
        "    train_processor = Train(opt)\n",
        "    train_processor.trainIters()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training mle: yes, Training rl: yes, mle weight: 0.25, rl weight: 0.75\n",
            "intra_encoder: True intra_decoder: True\n",
            "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n",
            "\n",
            "\n",
            "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 139\n",
            "\n",
            "\n",
            "Warning: incorrectly formatted line in vocabulary file: 2 1/2 124\n",
            "\n",
            "\n",
            "Warning: incorrectly formatted line in vocabulary file: 3 1/2 86\n",
            "\n",
            "\n",
            "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
            "Finished constructing vocabulary of 50000 total words. Last word added: perisic\n",
            "INFO:tensorflow:Bucket queue size: 0, Input queue size: 0\n",
            "Loaded model at drive/My Drive/NLPproject/CNNDM/0006000.tar\n",
            "INFO:tensorflow:Bucket queue size: 321, Input queue size: 0\n",
            "INFO:tensorflow:Bucket queue size: 658, Input queue size: 0\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 1147\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 94923\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 6200 mle_loss: 3.573 reward: 0.0907\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 6400 mle_loss: 3.532 reward: 0.0918\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 199861\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 6600 mle_loss: 3.596 reward: 0.0911\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 6800 mle_loss: 3.492 reward: 0.0912\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 7000 mle_loss: 3.535 reward: 0.0909\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 7200 mle_loss: 3.560 reward: 0.0911\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 7400 mle_loss: 3.514 reward: 0.0928\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 7600 mle_loss: 3.478 reward: 0.0920\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 7800 mle_loss: 3.433 reward: 0.0943\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 199932\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 8000 mle_loss: 3.496 reward: 0.0923\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 8200 mle_loss: 3.458 reward: 0.0922\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 199941\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 8400 mle_loss: 3.548 reward: 0.0913\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 8600 mle_loss: 3.528 reward: 0.0905\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 8800 mle_loss: 3.431 reward: 0.0940\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 9000 mle_loss: 3.518 reward: 0.0913\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 9200 mle_loss: 3.454 reward: 0.0929\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 9400 mle_loss: 3.423 reward: 0.0938\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 9600 mle_loss: 3.496 reward: 0.0928\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 9800 mle_loss: 3.380 reward: 0.0937\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
            "iter: 10000 mle_loss: 3.430 reward: 0.0931\n",
            "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 199824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pJwb9YDDetL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S0q6xZhDev3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIEQPnsEd4v4",
        "colab_type": "text"
      },
      "source": [
        "#Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4urZDFRDerR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "7a111465-abf6-4a46-ea38-676f6e6254c8"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n",
        "    #parser.add_argument(\"--start_from\", type=str, default=\"0002000.tar\")\n",
        "    parser.add_argument(\"--load_model\", type=str, default='0010000.tar')\n",
        "    #opt = parser.parse_args()\n",
        "    opt, unknown = parser.parse_known_args()\n",
        "\n",
        "    #testing\n",
        "    eval_processor = Evaluate(test_data_path, opt)\n",
        "    eval_processor.evaluate_batch()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n",
            "\n",
            "\n",
            "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 139\n",
            "\n",
            "\n",
            "Warning: incorrectly formatted line in vocabulary file: 2 1/2 124\n",
            "\n",
            "\n",
            "Warning: incorrectly formatted line in vocabulary file: 3 1/2 86\n",
            "\n",
            "\n",
            "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
            "Finished constructing vocabulary of 50000 total words. Last word added: perisic\n",
            "example_generator completed reading all datafiles. No more data.\n",
            "INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n",
            "INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: DeprecationWarning: generator 'Batcher.text_generator' raised StopIteration\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished reading dataset in single_pass mode.\n",
            "0010000.tar scores: {'rouge-1': {'f': 0.22320115220007586, 'p': 0.4946582093517118, 'r': 0.1470925694251189}, 'rouge-2': {'f': 0.08667917291729682, 'p': 0.22462902020293923, 'r': 0.05491637977024987}, 'rouge-l': {'f': 0.1469356427912949, 'p': 0.46404944283872424, 'r': 0.13780387843918562}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XxU-zTtd6iM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "7c7c2241-ef4d-4780-c183-375828618fa3"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n",
        "    #parser.add_argument(\"--start_from\", type=str, default=\"0002000.tar\")\n",
        "    parser.add_argument(\"--load_model\", type=str, default='0009000.tar')\n",
        "    #opt = parser.parse_args()\n",
        "    opt, unknown = parser.parse_known_args()\n",
        "\n",
        "    #testing\n",
        "    eval_processor = Evaluate(test_data_path, opt)\n",
        "    eval_processor.evaluate_batch()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n",
            "\n",
            "\n",
            "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 139\n",
            "\n",
            "\n",
            "Warning: incorrectly formatted line in vocabulary file: 2 1/2 124\n",
            "\n",
            "\n",
            "Warning: incorrectly formatted line in vocabulary file: 3 1/2 86\n",
            "\n",
            "\n",
            "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
            "Finished constructing vocabulary of 50000 total words. Last word added: perisic\n",
            "example_generator completed reading all datafiles. No more data.\n",
            "INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n",
            "INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: DeprecationWarning: generator 'Batcher.text_generator' raised StopIteration\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished reading dataset in single_pass mode.\n",
            "0009000.tar scores: {'rouge-1': {'f': 0.22296578735198588, 'p': 0.49544114242740755, 'r': 0.14682477954214918}, 'rouge-2': {'f': 0.08667120124120707, 'p': 0.22473923540197008, 'r': 0.05489997124995751}, 'rouge-l': {'f': 0.14676606074797655, 'p': 0.46513765092693166, 'r': 0.13767707426506112}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLjwpCGnQxA9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "d4482228-3c0a-4137-ba45-b722057b448c"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n",
        "    #parser.add_argument(\"--start_from\", type=str, default=\"0002000.tar\")\n",
        "    parser.add_argument(\"--load_model\", type=str, default='0006000.tar')\n",
        "    #opt = parser.parse_args()\n",
        "    opt, unknown = parser.parse_known_args()\n",
        "\n",
        "    #testing\n",
        "    eval_processor = Evaluate(test_data_path, opt)\n",
        "    eval_processor.evaluate_batch()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n",
            "\n",
            "\n",
            "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 139\n",
            "\n",
            "\n",
            "Warning: incorrectly formatted line in vocabulary file: 2 1/2 124\n",
            "\n",
            "\n",
            "Warning: incorrectly formatted line in vocabulary file: 3 1/2 86\n",
            "\n",
            "\n",
            "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
            "Finished constructing vocabulary of 50000 total words. Last word added: perisic\n",
            "example_generator completed reading all datafiles. No more data.\n",
            "INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n",
            "INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: DeprecationWarning: generator 'Batcher.text_generator' raised StopIteration\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished reading dataset in single_pass mode.\n",
            "0006000.tar scores: {'rouge-1': {'f': 0.2229755586574573, 'p': 0.4927210948522875, 'r': 0.1471751524075192}, 'rouge-2': {'f': 0.08568336230632997, 'p': 0.22169555004249444, 'r': 0.05434577590263777}, 'rouge-l': {'f': 0.14709419346941446, 'p': 0.46220498507340574, 'r': 0.13787993156598088}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyD7LPQxYpJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
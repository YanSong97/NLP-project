{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP project demo GROUP 15.ipynb","provenance":[],"collapsed_sections":["xVyknL418Tc1","qN3QmLxZ8x2z","iNJUK3Fc80I7","YD3rTR85871V","xOQ9RERd9DdN","LS3Nj4GX9r9x","761Spz8zswwv","c1UQijFnzobu","5W10lT0G1VkO","OOoOGIdfJBkW"],"authorship_tag":"ABX9TyMWEEkhaYuCNKjKtHJ/PWdv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4dUcG_2a4_LI","colab_type":"text"},"source":["#Paper: Model Explorations in Abstractive Summarisation"]},{"cell_type":"markdown","metadata":{"id":"2VUITBqk5Cyd","colab_type":"text"},"source":["GROUP 15:  \n","Youning Xia (19054254)\n","Yan Song (9898418)\n","Shuyi Han (19060915)\n","Wan Jing Song (17130843)\n"]},{"cell_type":"markdown","metadata":{"id":"nNXjjwP85bRM","colab_type":"text"},"source":["This paper sets out to assess the performanceof Deep Reinforcement Learning (DRL) basedabstractive summarization models.  4 differentmodel variants are applied on 3 datasets andevaluate on ROUGE and BERTScores. Work-ing  on  the  novel  WikiHow  dataset  (Koupaeeand Wang, 2018) which is slightly more com-plex to train on has magnified the characteris-tics of the models.   It exposes the instabilityof  training  on  ROUGE-L  scores  and  suggestBERTScore as an alternative."]},{"cell_type":"markdown","metadata":{"id":"fAo1-POI5cNC","colab_type":"text"},"source":["This notebook is to demonstrate the reproducibility of the testing result on Gigaword, CNN-DM and Wikihow dataset. However, given the 5-mins limits, it is recommended to run on **Gigaword dataset** only. The results on other two dataset have already been printed out."]},{"cell_type":"markdown","metadata":{"id":"W5p_J1i_6B03","colab_type":"text"},"source":["#CODE"]},{"cell_type":"code","metadata":{"id":"1BffoWrABKgd","colab_type":"code","outputId":"e9e25c59-4d5f-4035-c5c4-bed25179f71f","executionInfo":{"status":"ok","timestamp":1585870045379,"user_tz":-60,"elapsed":24057,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eR_tBrtv6Wkl","colab_type":"text"},"source":["##Package"]},{"cell_type":"code","metadata":{"id":"dXQ_3xkG6Hs3","colab_type":"code","outputId":"12d0d641-a577-4454-8ded-d9781efb03c0","executionInfo":{"status":"ok","timestamp":1585870056344,"user_tz":-60,"elapsed":4729,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["pip install rouge"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting rouge\n","  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.12.0)\n","Installing collected packages: rouge\n","Successfully installed rouge-1.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gOcuWDv16Hvz","colab_type":"code","outputId":"92a775a0-accc-4dde-9b27-cf283be0b22a","executionInfo":{"status":"ok","timestamp":1585870094715,"user_tz":-60,"elapsed":7053,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}},"colab":{"base_uri":"https://localhost:8080/","height":907}},"source":["pip install bert_score"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting bert_score\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/cd/95f08447e6c773fd2580e7734b00dec0e36319b1147a1aa8028ac78d3eb4/bert_score-0.3.1-py3-none-any.whl (51kB)\n","\r\u001b[K     |██████▎                         | 10kB 19.2MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.0MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.6/dist-packages (from bert_score) (4.38.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bert_score) (1.18.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from bert_score) (3.2.1)\n","Collecting transformers>=2.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/ba/dda44bbf35b071441635708a3dd568a5ca6bf29f77389f7c7c6818ae9498/transformers-2.7.0-py3-none-any.whl (544kB)\n","\u001b[K     |████████████████████████████████| 552kB 7.0MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bert_score) (2.21.0)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from bert_score) (1.0.3)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from bert_score) (1.4.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->bert_score) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->bert_score) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->bert_score) (2.4.6)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->bert_score) (2.8.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n","\u001b[K     |████████████████████████████████| 870kB 20.7MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.2.0->bert_score) (3.0.12)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 47.2MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.2.0->bert_score) (0.7)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.2.0->bert_score) (1.12.31)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.2.0->bert_score) (2019.12.20)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 38.7MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bert_score) (2019.11.28)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bert_score) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bert_score) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bert_score) (2.8)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.1->bert_score) (2018.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->bert_score) (46.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->bert_score) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.2.0->bert_score) (7.1.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.2.0->bert_score) (0.14.1)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.2.0->bert_score) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.2.0->bert_score) (0.9.5)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.31 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.2.0->bert_score) (1.15.31)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers>=2.2.0->bert_score) (0.15.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=db56a33ca61e5ac85709f778fe3d8d07f8cc23c0d72694cbc07edfa371fe4313\n","  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers, bert-score\n","Successfully installed bert-score-0.3.1 sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.7.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZbT7C7Oe6BTA","colab_type":"code","colab":{}},"source":["import glob\n","import random\n","import struct\n","import csv\n","from tensorflow.core.example import example_pb2\n","import numpy as np\n","\n","import queue as Queue\n","import time\n","from random import shuffle\n","from threading import Thread\n","\n","\n","import tensorflow as tf\n","\n","import torch as T\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","import torch.nn.functional as F\n","\n","import os\n","\n","from rouge import Rouge\n","import argparse\n","\n","\n","import shutil\n","import collections\n","import tqdm\n","\n","import gc\n","import bert_score\n","import argparse\n","\n","import pandas as pd\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HBnke3Aw6YYN","colab_type":"text"},"source":["##Vocab"]},{"cell_type":"code","metadata":{"id":"NTpksdAg4JQb","colab_type":"code","colab":{}},"source":["\n","SENTENCE_START = '<s>'      # <s> and </s> are used in the data files to segment the abstracts into sentences. They don't receive vocab ids.\n","SENTENCE_END = '</s>'       # or <t>, </t>\n","\n","PAD_TOKEN = '[PAD]'         # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n","UNKNOWN_TOKEN = '[UNK]'     # This has a vocab id, which is used to represent out-of-vocabulary words\n","START_DECODING = '[START]'  # This has a vocab id, which is used at the start of every decoder input sequence\n","STOP_DECODING = '[STOP]'    # This has a vocab id, which is used at the end of untruncated target sequences\n","\n","# Note: none of <s>, </s>, [PAD], [UNK], [START], [STOP] should appear in the vocab file.\n","\n","class Vocab(object):\n","  \"\"\"Vocabulary class for mapping between words and ids (integers)\"\"\"\n","\n","  def __init__(self, vocab_file, max_size):\n","    \"\"\"Creates a vocab of up to max_size words, reading from the vocab_file. If max_size is 0, reads the entire vocab file.\n","\n","    param  vocab_file: path to the vocab file, which is assumed to contain \"<word> <frequency>\" on each line, sorted with most frequent word first. This code doesn't actually use the frequencies, though.\n","    param  max_size: integer. The maximum size of the resulting Vocabulary.\"\"\"\n","    self._word_to_id = {}                                                       #dictionary with word as key\n","    self._id_to_word = {}                                                       #           with id as key\n","    self._count = 0                                                             # keeps track of total number of words in the Vocab\n","\n","    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n","    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n","      self._word_to_id[w] = self._count                                         #fill in the dict\n","      self._id_to_word[self._count] = w     \n","      self._count += 1\n","\n","    # Read the vocab file and add words up to max_size\n","    with open(vocab_file, 'r') as vocab_f:\n","      for line in vocab_f:\n","        pieces = line.split()                                                   #list with two elements, the first is word the other one is its id\n","        if len(pieces) != 2:\n","          print('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n","          continue\n","        w = pieces[0]                                                           #word\n","        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:        #raise error if these words exist in vocab\n","          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n","        if w in self._word_to_id:                                             \n","          raise Exception('Duplicated word in vocabulary file: %s' % w)         #Duplication error\n","        self._word_to_id[w] = self._count                                       #assign id to word\n","        self._id_to_word[self._count] = w                                       #assign word to id\n","        self._count += 1                                                        #id increment\n","        if max_size != 0 and self._count >= max_size:                           #stop assigning when max_size has been reached\n","          print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n","          break\n","\n","    print(\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n","\n","  def word2id(self, word):\n","    \"\"\"Returns the id (integer) of a word (string). Returns [UNK] id if word is OOV.\"\"\"\n","    if word not in self._word_to_id:\n","      return self._word_to_id[UNKNOWN_TOKEN]\n","    return self._word_to_id[word]\n","\n","  def id2word(self, word_id):\n","    \"\"\"Returns the word (string) corresponding to an id (integer).\"\"\"\n","    if word_id not in self._id_to_word:\n","      raise ValueError('Id not found in vocab: %d' % word_id)\n","    return self._id_to_word[word_id]\n","\n","  def size(self):\n","    \"\"\"Returns the total size of the vocabulary\"\"\"\n","    return self._count\n","\n","\n","\n","\n","\n","def example_generator(data_path, single_pass):\n","  \"\"\"Generates tf.Examples from data files.\n","    Binary data format: <length><blob>. <length> represents the byte size\n","    of <blob>. <blob> is serialized tf.Example proto. The tf.Example contains\n","    the tokenized article text and summary.\n","    https://www.tensorflow.org/tutorials/load_data/tfrecord\n","\n","    param  data_path:\n","      Path to tf.Example data files. Can include wildcards, e.g. if you have several training data chunk files train_001.bin, train_002.bin, etc, then pass data_path=train_* to access them all.\n","    param  single_pass:\n","      Boolean. If True, go through the dataset exactly once, generating examples in the order they appear, then return. Otherwise, generate random examples indefinitely.\n","    Yields:\n","      Deserialized tf.Example.\n","  \"\"\"\n","  while True:\n","    filelist = glob.glob(data_path)                                             # get the list of datafiles\n","    assert filelist, ('Error: Empty filelist at %s' % data_path)                # check filelist isn't empty\n","    if single_pass:\n","      filelist = sorted(filelist)\n","    else:\n","      random.shuffle(filelist)                                                  #random shuffle the data if not single_pass\n","    for f in filelist:\n","      reader = open(f, 'rb')\n","      while True:\n","        len_bytes = reader.read(8)\n","        if not len_bytes: break # finished reading this file\n","        str_len = struct.unpack('q', len_bytes)[0]\n","        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n","        yield example_pb2.Example.FromString(example_str)\n","    if single_pass:\n","      print(\"example_generator completed reading all datafiles. No more data.\")\n","      break\n","\n","\n","def article2ids(article_words, vocab):\n","  \"\"\"Map the article words to their ids. Also return a list of OOVs in the article.\n","  Args:\n","    article_words: list of words (strings)\n","    vocab: Vocabulary object\n","  Returns:\n","    ids:\n","      A list of word ids (integers); OOVs are represented by their temporary article OOV number. If the vocabulary size is 50k and the article has 3 OOVs, then these temporary OOV numbers will be 50000, 50001, 50002.\n","    oovs:\n","      A list of the OOV words in the article (strings), in the order corresponding to their temporary article OOV numbers.\"\"\"\n","  ids = []\n","  oovs = []\n","  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n","  for w in article_words:\n","    i = vocab.word2id(w)\n","    if i == unk_id:                         # If w is OOV\n","      if w not in oovs:                     # Add to list of OOVs\n","        oovs.append(w)\n","      oov_num = oovs.index(w)               # This is 0 for the first article OOV, 1 for the second article OOV...\n","      ids.append(vocab.size() + oov_num)    # This is e.g. 50000 for the first article OOV, 50001 for the second...(append at the end)\n","    else:\n","      ids.append(i)\n","  return ids, oovs\n","\n","\n","def abstract2ids(abstract_words, vocab, article_oovs):\n","  \"\"\"Map the abstract words to their ids. In-article OOVs are mapped to their temporary OOV numbers.\n","  Args:\n","    abstract_words: list of words (strings)\n","    vocab: Vocabulary object\n","    article_oovs: list of in-article OOV words (strings), in the order corresponding to their temporary article OOV numbers\n","  Returns:\n","    ids: List of ids (integers). In-article OOV words are mapped to their temporary OOV numbers. Out-of-article OOV words are mapped to the UNK token id.\"\"\"\n","  ids = []\n","  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n","  for w in abstract_words:\n","    i = vocab.word2id(w)\n","    if i == unk_id:                         # If w is an OOV word\n","      if w in article_oovs:                 # If w is an in-article OOV\n","        vocab_idx = vocab.size() + article_oovs.index(w)    # Map to its temporary article OOV number\n","        ids.append(vocab_idx)\n","      else:                                 # If w is an out-of-article OOV\n","        ids.append(unk_id)                  # Map to the UNK token id\n","    else:\n","      ids.append(i)\n","  return ids\n","\n","\n","def outputids2words(id_list, vocab, article_oovs):\n","  \"\"\"Maps output ids to words, including mapping in-article OOVs from their temporary ids to the original OOV string (applicable in pointer-generator mode).\n","\n","    param  id_list: list of ids (integers)\n","            vocab: Vocabulary object\n","    param  article_oovs: list of OOV words (strings) in the order corresponding to their temporary article OOV ids (that have been assigned in pointer-generator mode), or None (in baseline mode)\n","    Returns:\n","            words: list of words (strings)\n","  \"\"\"\n","  words = []\n","  for i in id_list:\n","    try:\n","      w = vocab.id2word(i) # might be [UNK]\n","    except ValueError as e: # w is OOV\n","      assert article_oovs is not None, \"Error: model produced a word ID that isn't in the vocabulary. This should not happen in baseline (no pointer-generator) mode\"\n","      article_oov_idx = i - vocab.size()\n","      try:\n","        w = article_oovs[article_oov_idx]\n","      except ValueError as e: # i doesn't correspond to an article oov\n","        raise ValueError('Error: model produced word ID %i which corresponds to article OOV %i but this example only has %i article OOVs' % (i, article_oov_idx, len(article_oovs)))\n","    words.append(w) \n","  return words\n","\n","\n","def abstract2sents(abstract):\n","  \"\"\"Splits abstract text from datafile into list of sentences.\n","  Args:\n","    abstract: string containing <s> and </s> tags for starts and ends of sentences\n","  Returns:\n","    sents: List of sentence strings (no tags)\"\"\"\n","  cur = 0\n","  sents = []\n","  while True:\n","    try:\n","      start_p = abstract.index(SENTENCE_START, cur)\n","      end_p = abstract.index(SENTENCE_END, start_p + 1)\n","      cur = end_p + len(SENTENCE_END)\n","      sents.append(abstract[start_p+len(SENTENCE_START):end_p])\n","    except ValueError as e: # no more sentences\n","      return sents\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dNIWD6ZG6bFt","colab_type":"text"},"source":["## Batcher"]},{"cell_type":"code","metadata":{"id":"Z2fYANgE9sUJ","colab_type":"code","colab":{}},"source":["max_batch_queue = 1000 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A1G_QVgD4-1y","colab_type":"code","colab":{}},"source":["\n","import queue as Queue\n","import time\n","from random import shuffle\n","from threading import Thread\n","\n","import numpy as np\n","import tensorflow as tf\n","\n","\n","import random\n","random.seed(1234)\n","\n","class Example(object):\n","  '''\n","  read article, abstrction and vocab and process them into batching-ready format\n","  '''\n","\n","  def __init__(self, article, abstract_sentences, vocab):\n","    # Get ids of special tokens\n","    start_decoding = vocab.word2id(START_DECODING)      #data\n","    stop_decoding = vocab.word2id(STOP_DECODING)        #data\n","\n","    # Process the article\n","    article_words = article.split()\n","    if len(article_words) > max_enc_steps:              #truncate the input article to max_enc_step length\n","      article_words = article_words[ : max_enc_steps]   \n","    self.enc_len = len(article_words)                   # store the length after truncation but before padding\n","    self.enc_input = [vocab.word2id(w) for w in article_words] # list of word ids; OOVs are represented by the id for UNK token\n","\n","    # Process the abstract\n","    abstract = ' '.join(abstract_sentences)                 # string\n","    abstract_words = abstract.split()                       # list of strings, split by space\n","    abs_ids = [vocab.word2id(w) for w in abstract_words]    # list of word ids; OOVs are represented by the id for UNK token\n","\n","    # Get the decoder input sequence and target sequence\n","    self.dec_input, _ = self.get_dec_inp_targ_seqs(abs_ids, max_dec_steps, start_decoding, stop_decoding)\n","    self.dec_len = len(self.dec_input)\n","\n","    # If using pointer-generator mode, we need to store some extra info\n","    # Store a version of the enc_input where in-article OOVs are represented by their temporary OOV id; also store the in-article OOVs words themselves\n","    self.enc_input_extend_vocab, self.article_oovs = article2ids(article_words, vocab)     #data\n","\n","    # Get a verison of the reference summary where in-article OOVs are represented by their temporary article OOV id\n","    abs_ids_extend_vocab = abstract2ids(abstract_words, vocab, self.article_oovs)           #data\n","\n","    # Get decoder target sequence\n","    _, self.target = self.get_dec_inp_targ_seqs(abs_ids_extend_vocab, max_dec_steps, start_decoding, stop_decoding)\n","\n","    # Store the original strings\n","    self.original_article = article\n","    self.original_abstract = abstract\n","    self.original_abstract_sents = abstract_sentences\n","\n","\n","\n","  def get_dec_inp_targ_seqs(self, sequence, max_len, start_id, stop_id):\n","    inp = [start_id] + sequence[:]\n","    target = sequence[:]\n","    if len(inp) > max_len: # truncate\n","      inp = inp[:max_len]\n","      target = target[:max_len] # no end_token\n","    else: # no truncation\n","      target.append(stop_id) # end token\n","    assert len(inp) == len(target)\n","    return inp, target\n","\n","\n","  def pad_decoder_inp_targ(self, max_len, pad_id):              #padding\n","    while len(self.dec_input) < max_len:\n","      self.dec_input.append(pad_id)\n","    while len(self.target) < max_len:\n","      self.target.append(pad_id)\n","\n","\n","  def pad_encoder_input(self, max_len, pad_id):                 #padding\n","    while len(self.enc_input) < max_len:\n","      self.enc_input.append(pad_id)\n","    while len(self.enc_input_extend_vocab) < max_len:\n","      self.enc_input_extend_vocab.append(pad_id)\n","\n","\n","\n","#-------------------------------------------------------------------------------------------------\n","\n","\n","class Batch(object):\n","  def __init__(self, example_list, vocab, batch_size):\n","    self.batch_size = batch_size\n","    self.pad_id = vocab.word2id(PAD_TOKEN)      # id of the PAD token used to pad sequences\n","    self.init_encoder_seq(example_list)         # initialize the input to the encoder\n","    self.init_decoder_seq(example_list)         # initialize the input and targets for the decoder\n","    self.store_orig_strings(example_list)       # store the original strings\n","\n","\n","  def init_encoder_seq(self, example_list):\n","    # Determine the maximum length of the encoder input sequence in this batch\n","    max_enc_seq_len = max([ex.enc_len for ex in example_list])\n","\n","    # Pad the encoder input sequences up to the length of the longest sequence\n","    for ex in example_list:\n","      ex.pad_encoder_input(max_enc_seq_len, self.pad_id)\n","\n","    # Initialize the numpy arrays\n","    # Note: our enc_batch can have different length (second dimension) for each batch because we use dynamic_rnn for the encoder.\n","    self.enc_batch = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.int32)\n","    self.enc_lens = np.zeros((self.batch_size), dtype=np.int32)\n","    self.enc_padding_mask = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.float32)\n","\n","    # Fill in the numpy arrays\n","    for i, ex in enumerate(example_list):\n","      self.enc_batch[i, :] = ex.enc_input[:]\n","      self.enc_lens[i] = ex.enc_len\n","      for j in range(ex.enc_len):\n","        self.enc_padding_mask[i][j] = 1\n","\n","    # For pointer-generator mode, need to store some extra info\n","    # Determine the max number of in-article OOVs in this batch\n","    self.max_art_oovs = max([len(ex.article_oovs) for ex in example_list])\n","    # Store the in-article OOVs themselves\n","    self.art_oovs = [ex.article_oovs for ex in example_list]\n","    # Store the version of the enc_batch that uses the article OOV ids\n","    self.enc_batch_extend_vocab = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.int32)\n","    for i, ex in enumerate(example_list):\n","      self.enc_batch_extend_vocab[i, :] = ex.enc_input_extend_vocab[:]\n","\n","  def init_decoder_seq(self, example_list):\n","    # Pad the inputs and targets\n","    for ex in example_list:\n","      ex.pad_decoder_inp_targ(max_dec_steps, self.pad_id)           #config\n","\n","    # Initialize the numpy arrays.\n","    self.dec_batch = np.zeros((self.batch_size, max_dec_steps), dtype=np.int32)         #config\n","    self.target_batch = np.zeros((self.batch_size, max_dec_steps), dtype=np.int32)      #config\n","    # self.dec_padding_mask = np.zeros((self.batch_size, config.max_dec_steps), dtype=np.float32)\n","    self.dec_lens = np.zeros((self.batch_size), dtype=np.int32)\n","\n","    # Fill in the numpy arrays\n","    for i, ex in enumerate(example_list):\n","      self.dec_batch[i, :] = ex.dec_input[:]\n","      self.target_batch[i, :] = ex.target[:]\n","      self.dec_lens[i] = ex.dec_len\n","      # for j in range(ex.dec_len):\n","      #   self.dec_padding_mask[i][j] = 1\n","\n","  def store_orig_strings(self, example_list):\n","    self.original_articles = [ex.original_article for ex in example_list] # list of lists\n","    self.original_abstracts = [ex.original_abstract for ex in example_list] # list of lists\n","    self.original_abstracts_sents = [ex.original_abstract_sents for ex in example_list] # list of list of lists\n","\n","\n","#-------------------------------------------------------------------------------------------------\n","class Batcher(object):\n","  BATCH_QUEUE_MAX = max_batch_queue  #30 # max number of batches the batch_queue can hold\n","\n","  def __init__(self, data_path, vocab, mode, batch_size, single_pass):\n","    self._data_path = data_path\n","    self._vocab = vocab\n","    self._single_pass = single_pass\n","    self.mode = mode\n","    self.batch_size = batch_size\n","    # Initialize a queue of Batches waiting to be used, and a queue of Examples waiting to be batched\n","    self._batch_queue = Queue.Queue(self.BATCH_QUEUE_MAX)\n","    self._example_queue = Queue.Queue(self.BATCH_QUEUE_MAX * self.batch_size)\n","\n","    # Different settings depending on whether we're in single_pass mode or not\n","    if single_pass:\n","      self._num_example_q_threads = 1 # just one thread, so we read through the dataset just once\n","      self._num_batch_q_threads = 1  # just one thread to batch examples\n","      self._bucketing_cache_size = 1 # only load one batch's worth of examples before bucketing; this essentially means no bucketing\n","      self._finished_reading = False # this will tell us when we're finished reading the dataset\n","    else:\n","      self._num_example_q_threads = 1 #16 # num threads to fill example queue\n","      self._num_batch_q_threads = 1 #4  # num threads to fill batch queue\n","      self._bucketing_cache_size = 1 #100 # how many batches-worth of examples to load into cache before bucketing\n","\n","    # Start the threads that load the queues\n","    self._example_q_threads = []\n","    for _ in range(self._num_example_q_threads):\n","      self._example_q_threads.append(Thread(target=self.fill_example_queue))\n","      self._example_q_threads[-1].daemon = True\n","      self._example_q_threads[-1].start()\n","    self._batch_q_threads = []\n","    for _ in range(self._num_batch_q_threads):\n","      self._batch_q_threads.append(Thread(target=self.fill_batch_queue))\n","      self._batch_q_threads[-1].daemon = True\n","      self._batch_q_threads[-1].start()\n","\n","    # Start a thread that watches the other threads and restarts them if they're dead\n","    if not single_pass: # We don't want a watcher in single_pass mode because the threads shouldn't run forever\n","      self._watch_thread = Thread(target=self.watch_threads)\n","      self._watch_thread.daemon = True\n","      self._watch_thread.start()\n","\n","  def next_batch(self):\n","    # If the batch queue is empty, print a warning\n","    if self._batch_queue.qsize() == 0:\n","      # tf.logging.warning('Bucket input queue is empty when calling next_batch. Bucket queue size: %i, Input queue size: %i', self._batch_queue.qsize(), self._example_queue.qsize())\n","      if self._single_pass and self._finished_reading:\n","        tf.compat.v1.logging.info(\"Finished reading dataset in single_pass mode.\")\n","        return None\n","\n","    batch = self._batch_queue.get() # get the next Batch\n","    return batch\n","\n","  def fill_example_queue(self):\n","    input_gen = self.text_generator(example_generator(self._data_path, self._single_pass))\n","\n","    while True:\n","      try:\n","        (article, abstract) = next(input_gen) # read the next example from file. article and abstract are both strings.\n","      except StopIteration: # if there are no more examples:\n","        tf.compat.v1.logging.info(\"The example generator for this example queue filling thread has exhausted data.\")\n","        if self._single_pass:\n","          tf.compat.v1.logging.info(\"single_pass mode is on, so we've finished reading dataset. This thread is stopping.\")\n","          self._finished_reading = True\n","          break\n","        else:\n","          raise Exception(\"single_pass mode is off but the example generator is out of data; error.\")\n","\n","      # abstract_sentences = [sent.strip() for sent in data.abstract2sents(abstract)] # Use the <s> and </s> tags in abstract to get a list of sentences.\n","      abstract_sentences = [abstract.strip()]\n","      example = Example(article, abstract_sentences, self._vocab) # Process into an Example.\n","      self._example_queue.put(example) # place the Example in the example queue.\n","\n","  def fill_batch_queue(self):\n","    while True:\n","      if self.mode == 'decode':\n","        # beam search decode mode single example repeated in the batch\n","        ex = self._example_queue.get()\n","        b = [ex for _ in range(self.batch_size)]\n","        self._batch_queue.put(Batch(b, self._vocab, self.batch_size))\n","      else:\n","        # Get bucketing_cache_size-many batches of Examples into a list, then sort\n","        inputs = []\n","        for _ in range(self.batch_size * self._bucketing_cache_size):\n","          inputs.append(self._example_queue.get())\n","        inputs = sorted(inputs, key=lambda inp: inp.enc_len, reverse=True) # sort by length of encoder sequence\n","\n","        # Group the sorted Examples into batches, optionally shuffle the batches, and place in the batch queue.\n","        batches = []\n","        for i in range(0, len(inputs), self.batch_size):\n","          batches.append(inputs[i:i + self.batch_size])\n","        if not self._single_pass:\n","          shuffle(batches)\n","        for b in batches:  # each b is a list of Example objects\n","          self._batch_queue.put(Batch(b, self._vocab, self.batch_size))\n","\n","  def watch_threads(self):\n","    while True:\n","      tf.compat.v1.logging.info(\n","        'Bucket queue size: %i, Input queue size: %i',\n","        self._batch_queue.qsize(), self._example_queue.qsize())\n","\n","      time.sleep(60)\n","      for idx,t in enumerate(self._example_q_threads):\n","        if not t.is_alive(): # if the thread is dead\n","          tf.compat.v1.logging.error('Found example queue thread dead. Restarting.')\n","          new_t = Thread(target=self.fill_example_queue)\n","          self._example_q_threads[idx] = new_t\n","          new_t.daemon = True\n","          new_t.start()\n","      for idx,t in enumerate(self._batch_q_threads):\n","        if not t.is_alive(): # if the thread is dead\n","          tf.compat.v1.logging.error('Found batch queue thread dead. Restarting.')\n","          new_t = Thread(target=self.fill_batch_queue)\n","          self._batch_q_threads[idx] = new_t\n","          new_t.daemon = True\n","          new_t.start()\n","\n","\n","  def text_generator(self, example_generator):\n","    while True:\n","      e = next(example_generator) # e is a tf.Example\n","      try:\n","        article_text = e.features.feature['article'].bytes_list.value[0] # the article text was saved under the key 'article' in the data files\n","        abstract_text = e.features.feature['abstract'].bytes_list.value[0] # the abstract text was saved under the key 'abstract' in the data files\n","        article_text = article_text.decode()\n","        abstract_text = abstract_text.decode()\n","      except ValueError:\n","        tf.compat.v1.logging.error('Failed to get article or abstract from example')\n","        continue\n","      if len(article_text)==0: # See https://github.com/abisee/pointer-generator/issues/1\n","        #tf.logging.warning('Found an example with empty article text. Skipping it.')\n","        continue\n","      else:\n","        yield (article_text, abstract_text)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xVyknL418Tc1","colab_type":"text"},"source":["##Model with global attention mechanism"]},{"cell_type":"code","metadata":{"id":"Y-sYJS3I4-4Y","colab_type":"code","colab":{}},"source":["'''\n","import torch as T\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","import torch.nn.functional as F\n","'''\n","\n","def init_lstm_wt(lstm):                                 #initialisation \n","    for name, _ in lstm.named_parameters():\n","        if 'weight' in name:\n","            wt = getattr(lstm, name)\n","            #print(wt)\n","            wt.data = wt.data.uniform_(-rand_unif_init_mag, rand_unif_init_mag)\n","            #wt.uniform_(-rand_unif_init_mag, rand_unif_init_mag)     #commit\n","        elif 'bias' in name:\n","            # set forget bias to 1\n","            bias = getattr(lstm, name)\n","            n = bias.size(0)\n","            start, end = n // 4, n // 2\n","\n","            bias.data = bias.data.fill_(0.)\n","            #bias.fill_(0.)\n","            bias.data[start:end].fill_(1.)\n","\n","def init_linear_wt(linear):                             #initialisation\n","\n","    linear.weight.data = linear.weight.data.normal_(std=trunc_norm_init_std)\n","    #linear.weight.normal_(std=trunc_norm_init_std)\n","    if linear.bias is not None:\n","        linear.bias.data = linear.bias.data.normal_(std=trunc_norm_init_std)\n","        #linear.bias.normal_(std=trunc_norm_init_std)\n","\n","def init_wt_normal(wt):                                 #initialisation\n","    wt.data = wt.data.normal_(std=trunc_norm_init_std)\n","    #wt.normal_(std=trunc_norm_init_std)\n","\n","\n","class Encoder(nn.Module):\n","    \"\"\"\n","    bi-directional LSTM, reduced linear layer\n","    \"\"\"\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","\n","        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=1, batch_first=True, bidirectional=True)    #batch_true: If True, then the input and output tensors are provided as (batch, seq, feature)\n","        init_lstm_wt(self.lstm)\n","\n","        self.reduce_h = nn.Linear(hidden_dim * 2, hidden_dim)        #hidden state \n","        init_linear_wt(self.reduce_h)\n","        self.reduce_c = nn.Linear(hidden_dim * 2, hidden_dim)\n","        init_linear_wt(self.reduce_c)\n","\n","    def forward(self, x, seq_lens):\n","        \"\"\"\n","        param  x:  input sequence\n","        param  seq_len:  length of sequence\n","\n","        return:\n","        enc_out:  Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n","        (h_reduced, c_reduced):  Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n","        \"\"\"\n","        packed = pack_padded_sequence(x, seq_lens, batch_first=True)    #accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly\n","        enc_out, enc_hid = self.lstm(packed)                            # tensor containing the hidden state for t = seq_len. and tensor containing the cell state for t = seq_len.\n","        enc_out,_ = pad_packed_sequence(enc_out, batch_first=True)      \n","        enc_out = enc_out.contiguous()                              #bs, n_seq, 2*n_hid\n","        h, c = enc_hid                                              #shape of h: 2, bs, n_hid\n","        h = T.cat(list(h), dim=1)                                   #bs, 2*n_hid\n","        c = T.cat(list(c), dim=1)\n","        h_reduced = F.relu(self.reduce_h(h))                        #bs,n_hid\n","        c_reduced = F.relu(self.reduce_c(c))\n","        return enc_out, (h_reduced, c_reduced)      #enc_out: all encoder\n","\n","\n","class encoder_attention(nn.Module):\n","\n","    def __init__(self):\n","        super(encoder_attention, self).__init__()\n","        self.W_h = nn.Linear(hidden_dim * 2, hidden_dim * 2, bias=False)    #no bias just the weight matrix\n","        self.W_s = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n","        self.v = nn.Linear(hidden_dim * 2, 1, bias=False)\n","\n","\n","    def forward(self, st_hat, h, enc_padding_mask, sum_temporal_srcs):              #INTRA-TEMPORAL ATTENTION ON INPUT SEQUENCE in paper?\n","        ''' Perform attention over encoder hidden states\n","        :param st_hat: decoder hidden state at current time step\n","        :param h: encoder hidden states\n","        :param enc_padding_mask:\n","        :param sum_temporal_srcs: if using intra-temporal attention, contains summation of attention weights from previous decoder time steps\n","        return\n","        ct_e: encoder context vector\n","        at:   attention weight\n","        sum_temporal_srcs:  sum of attention weights from previous decoder time steps, will need to be input again for next iteration\n","        '''\n","\n","        # Standard attention technique (eq 1 in https://arxiv.org/pdf/1704.04368.pdf)\n","        et = self.W_h(h)                        # bs,n_seq,2*n_hid\n","        dec_fea = self.W_s(st_hat).unsqueeze(1) # bs,1,2*n_hid\n","        et = et + dec_fea\n","        et = T.tanh(et)                         # bs,n_seq,2*n_hid\n","        et = self.v(et).squeeze(2)              # bs,n_seq\n","\n","        # intra-temporal attention     (eq 3 in https://arxiv.org/pdf/1705.04304.pdf)\n","        if intra_encoder:\n","            exp_et = T.exp(et)\n","            if sum_temporal_srcs is None:\n","                et1 = exp_et\n","                sum_temporal_srcs  = get_cuda(T.FloatTensor(et.size()).fill_(1e-10)) + exp_et           #defined temporal score\n","            else:\n","                et1 = exp_et/sum_temporal_srcs  #bs, n_seq\n","                sum_temporal_srcs = sum_temporal_srcs + exp_et\n","        else:\n","            et1 = F.softmax(et, dim=1)\n","\n","        # assign 0 probability for padded elements\n","        at = et1 * enc_padding_mask\n","        normalization_factor = at.sum(1, keepdim=True)\n","        at = at / normalization_factor\n","\n","        at = at.unsqueeze(1)                    #bs,1,n_seq\n","        # Compute encoder context vector\n","        ct_e = T.bmm(at, h)                     #bs, 1, 2*n_hid, batch matrix-matrix product of matrices (temporal score and encoder hidden state)\n","        ct_e = ct_e.squeeze(1)\n","        at = at.squeeze(1)\n","\n","        return ct_e, at, sum_temporal_srcs\n","\n","class decoder_attention(nn.Module):\n","    def __init__(self):\n","        super(decoder_attention, self).__init__()\n","        if intra_decoder:\n","            self.W_prev = nn.Linear(hidden_dim, hidden_dim, bias=False)        #weight\n","            self.W_s = nn.Linear(hidden_dim, hidden_dim)\n","            self.v = nn.Linear(hidden_dim, 1, bias=False)\n","\n","    def forward(self, s_t, prev_s):\n","        '''Perform intra_decoder attention\n","        Args\n","        :param s_t: hidden state of decoder at current time step\n","        :param prev_s: If intra_decoder attention, contains list of previous decoder hidden states\n","        '''\n","        if intra_decoder is False:\n","            ct_d = get_cuda(T.zeros(s_t.size()))\n","        elif prev_s is None:\n","            ct_d = get_cuda(T.zeros(s_t.size()))\n","            prev_s = s_t.unsqueeze(1)               #bs, 1, n_hid\n","        else:\n","            # Standard attention technique (eq 1 in https://arxiv.org/pdf/1704.04368.pdf)      e = v tanh(Wh + Ws + b)\n","            et = self.W_prev(prev_s)                # bs,t-1,n_hid\n","            dec_fea = self.W_s(s_t).unsqueeze(1)    # bs,1,n_hid\n","            et = et + dec_fea\n","            et = T.tanh(et)                         # bs,t-1,n_hid\n","            et = self.v(et).squeeze(2)              # bs,t-1\n","            # intra-decoder attention     (eq 7 & 8 in https://arxiv.org/pdf/1705.04304.pdf)\n","            at = F.softmax(et, dim=1).unsqueeze(1)  #bs, 1, t-1,  alpha\n","            ct_d = T.bmm(at, prev_s).squeeze(1)     #bs, n_hid\n","            prev_s = T.cat([prev_s, s_t.unsqueeze(1)], dim=1)    #bs, t, n_hid, keep adding previous hidden state \n","\n","        return ct_d, prev_s                 #decoder context vector, previous decoder hidden states\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self):\n","        super(Decoder, self).__init__()\n","        self.enc_attention = encoder_attention()\n","        self.dec_attention = decoder_attention()\n","        self.x_context = nn.Linear(hidden_dim*2 + emb_dim, emb_dim)\n","\n","        self.lstm = nn.LSTMCell(emb_dim, hidden_dim)\n","        init_lstm_wt(self.lstm)\n","\n","        self.p_gen_linear = nn.Linear(hidden_dim * 5 + emb_dim, 1)\n","\n","        #p_vocab\n","        self.V = nn.Linear(hidden_dim*4, hidden_dim)\n","        self.V1 = nn.Linear(hidden_dim, vocab_size)\n","        init_linear_wt(self.V1)\n","\n","    def forward(self, x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s):\n","        x = self.x_context(T.cat([x_t, ct_e], dim=1))\n","        s_t = self.lstm(x, s_t)\n","\n","        dec_h, dec_c = s_t\n","        st_hat = T.cat([dec_h, dec_c], dim=1)\n","        ct_e, attn_dist, sum_temporal_srcs = self.enc_attention(st_hat, enc_out, enc_padding_mask, sum_temporal_srcs)\n","\n","        ct_d, prev_s = self.dec_attention(dec_h, prev_s)        #intra-decoder attention\n","\n","        p_gen = T.cat([ct_e, ct_d, st_hat, x], 1)\n","        p_gen = self.p_gen_linear(p_gen)            # bs,1\n","        p_gen = T.sigmoid(p_gen)                    # bs,1\n","\n","        out = T.cat([dec_h, ct_e, ct_d], dim=1)     # bs, 4*n_hid\n","        out = self.V(out)                           # bs,n_hid\n","        out = self.V1(out)                          # bs, n_vocab\n","        vocab_dist = F.softmax(out, dim=1)\n","        vocab_dist = p_gen * vocab_dist\n","        attn_dist_ = (1 - p_gen) * attn_dist\n","\n","        # pointer mechanism (as suggested in eq 9 https://arxiv.org/pdf/1704.04368.pdf)\n","        if extra_zeros is not None:\n","            vocab_dist = T.cat([vocab_dist, extra_zeros], dim=1)\n","        final_dist = vocab_dist.scatter_add(1, enc_batch_extend_vocab, attn_dist_)\n","\n","        return final_dist, s_t, ct_e, sum_temporal_srcs, prev_s\n","\n","\n","\n","class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        self.encoder = Encoder()\n","        self.decoder = Decoder()\n","        self.embeds = nn.Embedding(vocab_size, emb_dim)\n","        init_wt_normal(self.embeds.weight)\n","\n","        self.encoder = get_cuda(self.encoder)\n","        self.decoder = get_cuda(self.decoder)\n","        self.embeds = get_cuda(self.embeds)\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qN3QmLxZ8x2z","colab_type":"text"},"source":["## Utility"]},{"cell_type":"code","metadata":{"id":"KT4kwi374-7I","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch as T\n","\n","\n","def get_cuda(tensor):\n","    if T.cuda.is_available():\n","        tensor = tensor.cuda()\n","    return tensor\n","\n","def get_enc_data(batch):\n","    batch_size = len(batch.enc_lens)\n","    enc_batch = T.from_numpy(batch.enc_batch).long()\n","    enc_padding_mask = T.from_numpy(batch.enc_padding_mask).float()\n","\n","    enc_lens = batch.enc_lens\n","\n","    ct_e = T.zeros(batch_size, 2*   hidden_dim)   #config.hidden_dim\n","\n","    enc_batch = get_cuda(enc_batch)\n","    enc_padding_mask = get_cuda(enc_padding_mask)\n","\n","    ct_e = get_cuda(ct_e)\n","\n","    enc_batch_extend_vocab = None\n","    if batch.enc_batch_extend_vocab is not None:\n","        enc_batch_extend_vocab = T.from_numpy(batch.enc_batch_extend_vocab).long()\n","        enc_batch_extend_vocab = get_cuda(enc_batch_extend_vocab)\n","\n","    extra_zeros = None\n","    if batch.max_art_oovs > 0:\n","        extra_zeros = T.zeros(batch_size, batch.max_art_oovs)\n","        extra_zeros = get_cuda(extra_zeros)\n","\n","\n","    return enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, ct_e\n","\n","\n","def get_dec_data(batch):\n","    dec_batch = T.from_numpy(batch.dec_batch).long()\n","    dec_lens = batch.dec_lens\n","    max_dec_len = np.max(dec_lens)\n","    dec_lens = T.from_numpy(batch.dec_lens).float()\n","\n","    target_batch = T.from_numpy(batch.target_batch).long()\n","\n","    dec_batch = get_cuda(dec_batch)\n","    dec_lens = get_cuda(dec_lens)\n","    target_batch = get_cuda(target_batch)\n","\n","    return dec_batch, max_dec_len, dec_lens, target_batch"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iNJUK3Fc80I7","colab_type":"text"},"source":["## Beam search"]},{"cell_type":"code","metadata":{"id":"3iUdFWch87D9","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch as T\n","\n","\n","class Beam(object):\n","    def __init__(self, start_id, end_id, unk_id, hidden_state, context):\n","        h,c = hidden_state                                              #(n_hid,)\n","        self.tokens = T.LongTensor(beam_size,1).fill_(start_id)  #(beam, t) after t time steps\n","        self.scores = T.FloatTensor(beam_size,1).fill_(-30)      #beam,1; Initial score of beams = -30\n","        self.tokens, self.scores = get_cuda(self.tokens), get_cuda(self.scores)\n","        self.scores[0][0] = 0                                           #At time step t=0, all beams should extend from a single beam. So, I am giving high initial score to 1st beam\n","        self.hid_h = h.unsqueeze(0).repeat(beam_size, 1)         #beam, n_hid\n","        self.hid_c = c.unsqueeze(0).repeat(beam_size, 1)         #beam, n_hid\n","        self.context = context.unsqueeze(0).repeat(beam_size, 1) #beam, 2*n_hid\n","        self.sum_temporal_srcs = None\n","        self.prev_s = None\n","        self.done = False\n","        self.end_id = end_id\n","        self.unk_id = unk_id\n","\n","    def get_current_state(self):\n","        tokens = self.tokens[:,-1].clone()\n","        for i in range(len(tokens)):\n","            if tokens[i].item() >= vocab_size:\n","                tokens[i] = self.unk_id\n","        return tokens\n","\n","\n","    def advance(self, prob_dist, hidden_state, context, sum_temporal_srcs, prev_s):\n","        '''Perform beam search: Considering the probabilites of given n_beam x n_extended_vocab words, select first n_beam words that give high total scores\n","        :param prob_dist: (beam, n_extended_vocab)\n","        :param hidden_state: Tuple of (beam, n_hid) tensors\n","        :param context:   (beam, 2*n_hidden)\n","        :param sum_temporal_srcs:   (beam, n_seq)\n","        :param prev_s:  (beam, t, n_hid)\n","        '''\n","        n_extended_vocab = prob_dist.size(1)\n","        h, c = hidden_state\n","        log_probs = T.log(prob_dist+eps)                         #beam, n_extended_vocab\n","\n","        scores = log_probs + self.scores                                #beam, n_extended_vocab\n","        scores = scores.view(-1,1)                                      #beam*n_extended_vocab, 1\n","        best_scores, best_scores_id = T.topk(input=scores, k=beam_size, dim=0)   #will be sorted in descending order of scores\n","        self.scores = best_scores                                       #(beam,1); sorted\n","        beams_order = best_scores_id.squeeze(1)/n_extended_vocab        #(beam,); sorted\n","        best_words = best_scores_id%n_extended_vocab                    #(beam,1); sorted\n","        self.hid_h = h[beams_order]                                     #(beam, n_hid); sorted\n","        self.hid_c = c[beams_order]                                     #(beam, n_hid); sorted\n","        self.context = context[beams_order]\n","        if sum_temporal_srcs is not None:\n","            self.sum_temporal_srcs = sum_temporal_srcs[beams_order]     #(beam, n_seq); sorted\n","        if prev_s is not None:\n","            self.prev_s = prev_s[beams_order]                           #(beam, t, n_hid); sorted\n","        self.tokens = self.tokens[beams_order]                          #(beam, t); sorted\n","        self.tokens = T.cat([self.tokens, best_words], dim=1)           #(beam, t+1); sorted\n","\n","        #End condition is when top-of-beam is EOS.\n","        if best_words[0][0] == self.end_id:\n","            self.done = True\n","\n","    def get_best(self):\n","        best_token = self.tokens[0].cpu().numpy().tolist()              #Since beams are always in sorted (descending) order, 1st beam is the best beam\n","        try:\n","            end_idx = best_token.index(self.end_id)\n","        except ValueError:\n","            end_idx = len(best_token)\n","        best_token = best_token[1:end_idx]\n","        return best_token\n","\n","    def get_all(self):\n","        all_tokens = []\n","        for i in range(len(self.tokens)):\n","            all_tokens.append(self.tokens[i].cpu().numpy())\n","        return all_tokens\n","\n","\n","def beam_search(enc_hid, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, model, start_id, end_id, unk_id):\n","\n","    batch_size = len(enc_hid[0])\n","    beam_idx = T.LongTensor(list(range(batch_size)))\n","    beams = [Beam(start_id, end_id, unk_id, (enc_hid[0][i], enc_hid[1][i]), ct_e[i]) for i in range(batch_size)]   #For each example in batch, create Beam object\n","    n_rem = batch_size                                                  #Index of beams that are active, i.e: didn't generate [STOP] yet\n","    sum_temporal_srcs = None                                            #Number of examples in batch that didn't generate [STOP] yet\n","    prev_s = None\n","\n","    for t in range(max_dec_steps):\n","        x_t = T.stack(\n","            [beam.get_current_state() for beam in beams if beam.done == False]      #remaining(rem),beam\n","        ).contiguous().view(-1)                                                     #(rem*beam,)\n","        x_t = model.embeds(x_t)                                                 #rem*beam, n_emb\n","\n","        dec_h = T.stack(\n","            [beam.hid_h for beam in beams if beam.done == False]                    #rem*beam,n_hid\n","        ).contiguous().view(-1,hidden_dim)\n","        dec_c = T.stack(\n","            [beam.hid_c for beam in beams if beam.done == False]                    #rem,beam,n_hid\n","        ).contiguous().view(-1,hidden_dim)                                   #rem*beam,n_hid\n","\n","        ct_e = T.stack(\n","            [beam.context for beam in beams if beam.done == False]                  #rem,beam,n_hid\n","        ).contiguous().view(-1,2*hidden_dim)                                 #rem,beam,n_hid\n","\n","        if sum_temporal_srcs is not None:\n","            sum_temporal_srcs = T.stack(\n","                [beam.sum_temporal_srcs for beam in beams if beam.done == False]\n","            ).contiguous().view(-1, enc_out.size(1))                                #rem*beam, n_seq\n","\n","        if prev_s is not None:\n","            prev_s = T.stack(\n","                [beam.prev_s for beam in beams if beam.done == False]\n","            ).contiguous().view(-1, t, hidden_dim)                           #rem*beam, t-1, n_hid\n","\n","\n","        s_t = (dec_h, dec_c)\n","        enc_out_beam = enc_out[beam_idx].view(n_rem,-1).repeat(1, beam_size).view(-1, enc_out.size(1), enc_out.size(2))\n","        enc_pad_mask_beam = enc_padding_mask[beam_idx].repeat(1, beam_size).view(-1, enc_padding_mask.size(1))\n","\n","        extra_zeros_beam = None\n","        if extra_zeros is not None:\n","            extra_zeros_beam = extra_zeros[beam_idx].repeat(1, beam_size).view(-1, extra_zeros.size(1))\n","        enc_extend_vocab_beam = enc_batch_extend_vocab[beam_idx].repeat(1, beam_size).view(-1, enc_batch_extend_vocab.size(1))\n","\n","        final_dist, (dec_h, dec_c), ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out_beam, enc_pad_mask_beam, ct_e, extra_zeros_beam, enc_extend_vocab_beam, sum_temporal_srcs, prev_s)              #final_dist: rem*beam, n_extended_vocab\n","\n","        final_dist = final_dist.view(n_rem, beam_size, -1)                   #final_dist: rem, beam, n_extended_vocab\n","        dec_h = dec_h.view(n_rem, beam_size, -1)                             #rem, beam, n_hid\n","        dec_c = dec_c.view(n_rem, beam_size, -1)                             #rem, beam, n_hid\n","        ct_e = ct_e.view(n_rem, beam_size, -1)                             #rem, beam, 2*n_hid\n","\n","        if sum_temporal_srcs is not None:\n","            sum_temporal_srcs = sum_temporal_srcs.view(n_rem, beam_size, -1) #rem, beam, n_seq\n","\n","        if prev_s is not None:\n","            prev_s = prev_s.view(n_rem, beam_size, -1, hidden_dim)    #rem, beam, t\n","\n","        # For all the active beams, perform beam search\n","        active = []         #indices of active beams after beam search\n","\n","        for i in range(n_rem):\n","            b = beam_idx[i].item()\n","            beam = beams[b]\n","            if beam.done:\n","                continue\n","\n","            sum_temporal_srcs_i = prev_s_i = None\n","            if sum_temporal_srcs is not None:\n","                sum_temporal_srcs_i = sum_temporal_srcs[i]                              #beam, n_seq\n","            if prev_s is not None:\n","                prev_s_i = prev_s[i]                                                #beam, t, n_hid\n","            beam.advance(final_dist[i], (dec_h[i], dec_c[i]), ct_e[i], sum_temporal_srcs_i, prev_s_i)\n","            if beam.done == False:\n","                active.append(b)\n","\n","        if len(active) == 0:\n","            break\n","\n","        beam_idx = T.LongTensor(active)\n","        n_rem = len(beam_idx)\n","\n","    predicted_words = []\n","    for beam in beams:\n","        predicted_words.append(beam.get_best())\n","\n","    return predicted_words"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YD3rTR85871V","colab_type":"text"},"source":["## Evaluation with ROUGE"]},{"cell_type":"code","metadata":{"id":"NwbcqGQS9AQu","colab_type":"code","colab":{}},"source":["'''\n","import os\n","\n","import time\n","\n","import torch as T\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","from rouge import Rouge\n","import argparse\n","'''\n","\n","def get_cuda(tensor):\n","    if T.cuda.is_available():\n","        tensor = tensor.cuda()\n","    return tensor\n","\n","class Evaluate_r(object):\n","    def __init__(self, data_path, opt, batch_size):\n","        self.vocab = Vocab(vocab_path, vocab_size)\n","        self.batcher = Batcher(data_path, self.vocab, mode='eval',\n","                               batch_size=batch_size, single_pass=True)\n","        self.opt = opt\n","        time.sleep(5)\n","\n","    def setup_valid(self):\n","        self.model = Model()\n","        self.model = get_cuda(self.model)\n","        checkpoint = T.load(os.path.join(save_model_path, self.opt.load_model))\n","        self.model.load_state_dict(checkpoint[\"model_dict\"])\n","\n","\n","    def print_original_predicted(self, decoded_sents, ref_sents, article_sents, loadfile):\n","        filename = \"test_\"+loadfile.split(\".\")[0]+\".txt\"\n","    \n","        with open(os.path.join(save_example_path,filename), \"w\") as f:\n","            for i in range(len(decoded_sents)):\n","                f.write(\"article: \"+article_sents[i] + \"\\n\")\n","                f.write(\"ref: \" + ref_sents[i] + \"\\n\")\n","                f.write(\"dec: \" + decoded_sents[i] + \"\\n\\n\")\n","\n","    def evaluate_batch(self, print_sents = False):\n","\n","        self.setup_valid()\n","        batch = self.batcher.next_batch()\n","        start_id = self.vocab.word2id(START_DECODING)\n","        end_id = self.vocab.word2id(STOP_DECODING)\n","        unk_id = self.vocab.word2id(UNKNOWN_TOKEN)\n","        decoded_sents = []\n","        ref_sents = []\n","        article_sents = []\n","        rouge = Rouge()\n","        while batch is not None:\n","            enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, ct_e = get_enc_data(batch)\n","\n","            with T.autograd.no_grad():\n","                enc_batch = self.model.embeds(enc_batch)\n","                enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n","\n","            #-----------------------Summarization----------------------------------------------------\n","            with T.autograd.no_grad():\n","                pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, self.model, start_id, end_id, unk_id)\n","\n","            for i in range(len(pred_ids)):\n","                decoded_words = outputids2words(pred_ids[i], self.vocab, batch.art_oovs[i])\n","                if len(decoded_words) < 2:\n","                    decoded_words = \"xxx\"\n","                else:\n","                    decoded_words = \" \".join(decoded_words)\n","                decoded_sents.append(decoded_words)\n","                abstract = batch.original_abstracts[i]\n","                article = batch.original_articles[i]\n","                ref_sents.append(abstract)\n","                article_sents.append(article)\n","\n","            batch = self.batcher.next_batch()\n","\n","        load_file = self.opt.load_model\n","\n","        if print_sents:\n","            self.print_original_predicted(decoded_sents, ref_sents, article_sents, load_file)\n","\n","        scores = rouge.get_scores(decoded_sents, ref_sents, avg = True)\n","        if self.opt.task == \"test\":\n","            print(load_file, \"scores:\", scores)\n","        else:\n","            rouge_l = scores[\"rouge-l\"][\"f\"]\n","            print(load_file, \"rouge_l:\", \"%.4f\" % rouge_l)\n","            print('Scores:', scores)\n","        \n","        rouge_1 = scores['rouge-1']['f']        #rouge 1\n","        rouge_2 = scores['rouge-2']['f']        #rouge 2\n","        rouge_l = scores['rouge-l']['f']        #rouge L\n","\n","        return rouge_1, rouge_2, rouge_l\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xOQ9RERd9DdN","colab_type":"text"},"source":["## Evaluation with BERTScore"]},{"cell_type":"code","metadata":{"id":"F9e9Bu4i9Doz","colab_type":"code","colab":{}},"source":["import os\n","\n","import time\n","\n","import torch as T\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import gc\n","import bert_score\n","import argparse\n","\n","def get_cuda(tensor):\n","    if T.cuda.is_available():\n","        tensor = tensor.cuda()\n","    return tensor\n","\n","class Evaluate_b(object):\n","    def __init__(self, data_path, opt, batch_size):\n","        self.vocab = Vocab(vocab_path, vocab_size)\n","        self.batcher = Batcher(data_path, self.vocab, mode='eval',\n","                               batch_size=batch_size, single_pass=True)\n","        self.opt = opt\n","\n","        time.sleep(5)\n","\n","    def setup_valid(self):\n","        self.model = Model()\n","        self.model = get_cuda(self.model)\n","        checkpoint = T.load(os.path.join(save_model_path, self.opt.load_model))\n","        self.model.load_state_dict(checkpoint[\"model_dict\"])\n","\n","\n","    def print_original_predicted(self, decoded_sents, ref_sents, article_sents, loadfile):\n","        filename = \"test_\"+loadfile.split(\".\")[0]+\".txt\"\n","    \n","        with open(os.path.join(save_example_path,filename), \"w\") as f:\n","            for i in range(len(decoded_sents)):\n","                f.write(\"article: \"+article_sents[i] + \"\\n\")\n","                f.write(\"ref: \" + ref_sents[i] + \"\\n\")\n","                f.write(\"dec: \" + decoded_sents[i] + \"\\n\\n\")\n","\n","    def evaluate_batch(self, print_sents = False):\n","\n","        self.setup_valid()\n","        batch = self.batcher.next_batch()\n","        start_id = self.vocab.word2id(START_DECODING)\n","        end_id = self.vocab.word2id(STOP_DECODING)\n","        unk_id = self.vocab.word2id(UNKNOWN_TOKEN)\n","        decoded_sents = []\n","        ref_sents = []\n","        article_sents = []\n","        #rouge = Rouge()\n","        while batch is not None:\n","            enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, ct_e = get_enc_data(batch)\n","\n","            with T.autograd.no_grad():\n","                enc_batch = self.model.embeds(enc_batch)\n","                enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n","\n","            #-----------------------Summarization----------------------------------------------------\n","            with T.autograd.no_grad():\n","                pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, self.model, start_id, end_id, unk_id)\n","\n","            for i in range(len(pred_ids)):\n","                decoded_words = outputids2words(pred_ids[i], self.vocab, batch.art_oovs[i])\n","                if len(decoded_words) < 2:\n","                    decoded_words = \"xxx\"\n","                else:\n","                    decoded_words = \" \".join(decoded_words)\n","                decoded_sents.append(decoded_words)\n","                abstract = batch.original_abstracts[i]\n","                article = batch.original_articles[i]\n","                ref_sents.append(abstract)\n","                article_sents.append(article)\n","\n","            batch = self.batcher.next_batch()\n","\n","        load_file = self.opt.load_model\n","\n","        _,_,f = bert_score.score(decoded_sents, ref_sents, lang='en', verbose = False)\n","\n","        print('{}  score:{}'.format(load_file, f.mean()))\n","        gc.collect()\n","        \n","        if print_sents:\n","            self.print_original_predicted(decoded_sents, ref_sents, article_sents, load_file)\n","\n","        return f.mean()\n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xTPmEpvZ9j6P","colab_type":"text"},"source":["#Experiement"]},{"cell_type":"markdown","metadata":{"id":"LS3Nj4GX9r9x","colab_type":"text"},"source":["##Gigaword testing"]},{"cell_type":"markdown","metadata":{"id":"qyuqIwFoFQhh","colab_type":"text"},"source":["testing models folder:  "]},{"cell_type":"code","metadata":{"id":"JdJTRNE6FXKW","colab_type":"code","colab":{}},"source":["model_folder = 'drive/My Drive/Gigaword testing'        #directory containing all testing models for Gigaword dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ET142aYGYR8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"f376f9f7-4fa6-4265-977e-4bcfe5b309de","executionInfo":{"status":"ok","timestamp":1585851253748,"user_tz":-60,"elapsed":2033,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["!ls 'drive/My Drive/Gigaword testing' "],"execution_count":62,"outputs":[{"output_type":"stream","text":["'hybrid testing'  'RL(b) testing model'   test_00.bin\n","'ml testing'\t  'RL(r) testing model'   vocab\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"761Spz8zswwv","colab_type":"text"},"source":["###Hyperparameter"]},{"cell_type":"code","metadata":{"id":"qW4k3lfi9qYU","colab_type":"code","colab":{}},"source":["test_data_path = (model_folder + '/test_00.bin')\t   #'drive/My Drive/Gigaword/giga/chunked/test/test_00.bin'         #testing data file path\n","vocab_path =   (model_folder + '/vocab')\t\t#'drive/My Drive/Gigaword/giga/vocab'                           #vocab data file path\n","\n","# Hyperparameters\n","hidden_dim = 512\n","emb_dim = 256\n","batch_size = 200\n","max_enc_steps = 55\n","max_dec_steps = 15\n","beam_size = 4\n","min_dec_steps= 3\n","vocab_size = 50000\n","\n","lr = 0.001\n","rand_unif_init_mag = 0.02\n","trunc_norm_init_std = 1e-4\n","\n","eps = 1e-12\n","max_iterations = 16000\n","max_batch_queue = 1000          #1000\n","\n","\n","intra_encoder = True\n","intra_decoder = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JwWHFGAxBEma","colab_type":"code","outputId":"4c214a9a-6700-45da-8a6b-05877d2f055f","executionInfo":{"status":"ok","timestamp":1585754632108,"user_tz":-60,"elapsed":2788,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["!ls 'drive/My Drive/Gigaword'"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" giga\t\t\t\t'ml testing'\t        sumdata\n"," giga_pretrain.tar\t\t'RL(b) testing model'   summary.tar.gz\n","'local attention example d100'\t'RL(r) testing model'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c1UQijFnzobu","colab_type":"text"},"source":["###ML testing(ROUGE+BERTScore)"]},{"cell_type":"code","metadata":{"id":"_gpYSCnCzsJT","colab_type":"code","colab":{}},"source":["save_model_path = (model_folder + '/ml testing')#'drive/My Drive/Gigaword/ml testing'              #directory path for the model need to be tested\n","model_name_for_r_testing = 'ML testing.tar'\n","model_name_for_b_testing = 'ML testing.tar'                     #both names are the same for ML testing"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VuorN1lMAs7W","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H0ya6TVz0Ckv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":623},"outputId":"f5958efa-7780-45c6-e96d-bf1398ae9b57","executionInfo":{"status":"ok","timestamp":1585851477317,"user_tz":-60,"elapsed":79648,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n","    parser.add_argument(\"--start_from\", type=str, default=None)             \n","    parser.add_argument(\"--load_model\", type=str, default=model_name_for_r_testing)       # the name of the testing model\n","    #opt = parser.parse_args()\n","    opt, unknown = parser.parse_known_args()\n","\n","\n","    eval_r = Evaluate_r(test_data_path, opt, batch_size)        #or Evaluate_b(test_data_path, opt, batch_size) if evaluated with BERTScore\n","    r_ml_1, r_ml_2, r_ml_L = eval_r.evaluate_batch()\n","\n","    eval_b = Evaluate_b(test_data_path, opt, batch_size)\n","    ml_bert_testing = eval_b.evaluate_batch()"],"execution_count":71,"outputs":[{"output_type":"stream","text":["Warning: incorrectly formatted line in vocabulary file: # #\\/# 2632\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: ## #\\/# 263\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: -lrb-###-rrb- ###-#### 51\n","\n","\n","max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n","Finished constructing vocabulary of 50000 total words. Last word added: malignaggi\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:211: DeprecationWarning: generator 'Batcher.text_generator' raised StopIteration\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished reading dataset in single_pass mode.\n","ML testing.tar scores: {'rouge-1': {'f': 0.4211750260014512, 'p': 0.45982025514168373, 'r': 0.40514399771993204}, 'rouge-2': {'f': 0.22046333434940685, 'p': 0.24310209433423713, 'r': 0.21108568415711254}, 'rouge-l': {'f': 0.40631281347425713, 'p': 0.44417428999571834, 'r': 0.38991643404079745}}\n","Warning: incorrectly formatted line in vocabulary file: # #\\/# 2632\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: ## #\\/# 263\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: -lrb-###-rrb- ###-#### 51\n","\n","\n","max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n","Finished constructing vocabulary of 50000 total words. Last word added: malignaggi\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n","INFO:tensorflow:Finished reading dataset in single_pass mode.\n","ML testing.tar  score:0.9029324650764465\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IorThMXC0zrT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4677dc77-f758-442c-d68d-ba361d5913ba","executionInfo":{"status":"ok","timestamp":1585851477318,"user_tz":-60,"elapsed":73039,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["print('ROUGE_1: {:.4f} ; ROUGE_2: {:.4f}; ROUGE_L: {:.4f}; BERTScore: {:.4f}'.format(r_ml_1, r_ml_2, r_ml_L, ml_bert_testing))"],"execution_count":72,"outputs":[{"output_type":"stream","text":["ROUGE_1: 0.4212 ; ROUGE_2: 0.2205; ROUGE_L: 0.4063; BERTScore: 0.9029\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5W10lT0G1VkO","colab_type":"text"},"source":["###Hybrid testing (ROUGE+BERTScore)"]},{"cell_type":"code","metadata":{"id":"tx6a9sWK1YFe","colab_type":"code","colab":{}},"source":["save_model_path = (model_folder + '/hybrid testing')        #'drive/My Drive/Gigaword/hybrid testing'              #directory path for the model need to be tested\n","model_name_for_r_testing = 'hybrid for ROUGE testing.tar'\n","model_name_for_b_testing = 'hybrid for BERT testing.tar'                    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cIM4LVYg1aKr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":623},"outputId":"fbe98dc4-599e-4c89-924a-d47c06dfa35c","executionInfo":{"status":"ok","timestamp":1585851575618,"user_tz":-60,"elapsed":97204,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n","    parser.add_argument(\"--start_from\", type=str, default=None)             \n","    parser.add_argument(\"--load_model\", type=str, default=model_name_for_r_testing)       # the name of the testing model\n","    #opt = parser.parse_args()\n","    opt, unknown = parser.parse_known_args()\n","\n","    eval_r = Evaluate_r(test_data_path, opt, batch_size)        #or Evaluate_b(test_data_path, opt, batch_size) if evaluated with BERTScore\n","    r_hy_1, r_hy_2, r_hy_L = eval_r.evaluate_batch()\n","\n","    opt.load_model= model_name_for_b_testing    \n","    eval_b = Evaluate_b(test_data_path, opt, batch_size)\n","    hy_bert_testing = eval_b.evaluate_batch()"],"execution_count":74,"outputs":[{"output_type":"stream","text":["Warning: incorrectly formatted line in vocabulary file: # #\\/# 2632\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: ## #\\/# 263\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: -lrb-###-rrb- ###-#### 51\n","\n","\n","max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n","Finished constructing vocabulary of 50000 total words. Last word added: malignaggi\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:211: DeprecationWarning: generator 'Batcher.text_generator' raised StopIteration\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished reading dataset in single_pass mode.\n","hybrid for ROUGE testing.tar scores: {'rouge-1': {'f': 0.4232801800481467, 'p': 0.462242559028273, 'r': 0.4070486371398023}, 'rouge-2': {'f': 0.2223031150585183, 'p': 0.24536857190428593, 'r': 0.21281178841000253}, 'rouge-l': {'f': 0.4080882263595928, 'p': 0.44618194503908803, 'r': 0.3914904075696028}}\n","Warning: incorrectly formatted line in vocabulary file: # #\\/# 2632\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: ## #\\/# 263\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: -lrb-###-rrb- ###-#### 51\n","\n","\n","max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n","Finished constructing vocabulary of 50000 total words. Last word added: malignaggi\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n","INFO:tensorflow:Finished reading dataset in single_pass mode.\n","hybrid for BERT testing.tar  score:0.9031652808189392\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ThFYB8YX1aOj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6bbfa4b8-e638-4add-e50d-658aa18dcc40","executionInfo":{"status":"ok","timestamp":1585851575620,"user_tz":-60,"elapsed":97194,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["print('ROUGE_1: {:.4f} ; ROUGE_2: {:.4f}; ROUGE_L: {:.4f}; BERTScore: {:.4f}'.format(r_hy_1, r_hy_2, r_hy_L, hy_bert_testing))"],"execution_count":75,"outputs":[{"output_type":"stream","text":["ROUGE_1: 0.4233 ; ROUGE_2: 0.2223; ROUGE_L: 0.4081; BERTScore: 0.9032\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a_mYAn8etOly","colab_type":"text"},"source":["###RL(r) testing(ROUGE+BERTScore)"]},{"cell_type":"code","metadata":{"id":"64-uO-x1zzzE","colab_type":"code","colab":{}},"source":["save_model_path =  (model_folder + '/RL(r) testing model')             #'drive/My Drive/Gigaword/RL(r) testing model'              #directory path for the RL(r) model need to be tested\n","model_name_for_r_testing = 'RL(r) for ROUGE testing.tar'\n","model_name_for_b_testing = 'RL(r) for BERT testing.tar'                    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e9-ISFiM9UzC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"367a90d9-f340-4017-f68c-d08fbfe717fa","executionInfo":{"status":"ok","timestamp":1585848873694,"user_tz":-60,"elapsed":1967,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["!ls 'drive/My Drive/Gigaword/RL(r) testing model'"],"execution_count":98,"outputs":[{"output_type":"stream","text":["'giga RL(r) BERT testing.tar'  'RL(r) Giga model for ROUGE testing.tar'\n","'RL(r) for BERT testing.tar'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"44toI551CbtV","colab_type":"code","outputId":"39724bc5-df85-4541-d032-728ff5739085","executionInfo":{"status":"ok","timestamp":1585851700333,"user_tz":-60,"elapsed":124696,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}},"colab":{"base_uri":"https://localhost:8080/","height":623}},"source":["if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n","    parser.add_argument(\"--start_from\", type=str, default=None)             \n","    parser.add_argument(\"--load_model\", type=str, default=model_name_for_r_testing)       # the name of the testing model\n","    #opt = parser.parse_args()\n","    opt, unknown = parser.parse_known_args()\n","\n","\n","    eval_r = Evaluate_r(test_data_path, opt, batch_size)        #or Evaluate_b(test_data_path, opt, batch_size) if evaluated with BERTScore\n","    r_rlr_1, r_rlr_2, r_rlr_L = eval_r.evaluate_batch()\n","\n","    opt.load_model= model_name_for_b_testing    \n","    eval_b = Evaluate_b(test_data_path, opt, batch_size)\n","    rlr_bert_testing = eval_b.evaluate_batch()\n"],"execution_count":77,"outputs":[{"output_type":"stream","text":["Warning: incorrectly formatted line in vocabulary file: # #\\/# 2632\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: ## #\\/# 263\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: -lrb-###-rrb- ###-#### 51\n","\n","\n","max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n","Finished constructing vocabulary of 50000 total words. Last word added: malignaggi\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:211: DeprecationWarning: generator 'Batcher.text_generator' raised StopIteration\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished reading dataset in single_pass mode.\n","RL(r) for ROUGE testing.tar scores: {'rouge-1': {'f': 0.32906548014053744, 'p': 0.2719791472655776, 'r': 0.4463953833097463}, 'rouge-2': {'f': 0.15910624180178037, 'p': 0.13320390184045677, 'r': 0.2177210814978669}, 'rouge-l': {'f': 0.43262097102848707, 'p': 0.45848843418486085, 'r': 0.4304524925630909}}\n","Warning: incorrectly formatted line in vocabulary file: # #\\/# 2632\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: ## #\\/# 263\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: -lrb-###-rrb- ###-#### 51\n","\n","\n","max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n","Finished constructing vocabulary of 50000 total words. Last word added: malignaggi\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n","INFO:tensorflow:Finished reading dataset in single_pass mode.\n","RL(r) for BERT testing.tar  score:0.8767116665840149\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SAuQwlYByd5y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"02c85f00-9876-4619-fecb-5b80ed75fc73","executionInfo":{"status":"ok","timestamp":1585851701094,"user_tz":-60,"elapsed":739,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["print('ROUGE_1: {:.4f} ; ROUGE_2: {:.4f}; ROUGE_L: {:.4f}; BERTScore: {:.4f}'.format(r_rlr_1, r_rlr_2, r_rlr_L, rlr_bert_testing))"],"execution_count":78,"outputs":[{"output_type":"stream","text":["ROUGE_1: 0.3291 ; ROUGE_2: 0.1591; ROUGE_L: 0.4326; BERTScore: 0.8767\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J39k89sp6HN7","colab_type":"text"},"source":["###RL(b) testing(ROUGE+BERTScore)"]},{"cell_type":"code","metadata":{"id":"zq21aJF96LiX","colab_type":"code","colab":{}},"source":["save_model_path =  (model_folder + '/RL(b) testing model')         #'drive/My Drive/Gigaword/RL(b) testing model'              #directory path for the model need to be tested\n","model_name_for_r_testing = 'RL(b) for ROUGE testing.tar'\n","model_name_for_b_testing = 'RL(b) for BERT testing.tar'                  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTGO_HcN6Lm6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":623},"outputId":"20f2d220-30be-4138-b16b-18518d021d3d","executionInfo":{"status":"ok","timestamp":1585851800756,"user_tz":-60,"elapsed":100379,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n","    parser.add_argument(\"--start_from\", type=str, default=None)             \n","    parser.add_argument(\"--load_model\", type=str, default=model_name_for_r_testing)       # the name of the testing model\n","    #opt = parser.parse_args()\n","    opt, unknown = parser.parse_known_args()\n","\n","    eval_r = Evaluate_r(test_data_path, opt, batch_size)        #or Evaluate_b(test_data_path, opt, batch_size) if evaluated with BERTScore\n","    r_rlb_1, r_rlb_2, r_rlb_L = eval_r.evaluate_batch()\n","\n","    opt.load_model= model_name_for_b_testing    \n","    eval_b = Evaluate_b(test_data_path, opt, batch_size)\n","    rlb_bert_testing = eval_b.evaluate_batch()"],"execution_count":80,"outputs":[{"output_type":"stream","text":["Warning: incorrectly formatted line in vocabulary file: # #\\/# 2632\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: ## #\\/# 263\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: -lrb-###-rrb- ###-#### 51\n","\n","\n","max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n","Finished constructing vocabulary of 50000 total words. Last word added: malignaggi\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:211: DeprecationWarning: generator 'Batcher.text_generator' raised StopIteration\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished reading dataset in single_pass mode.\n","RL(b) for ROUGE testing.tar scores: {'rouge-1': {'f': 0.42036605420965195, 'p': 0.44055116906902614, 'r': 0.4218861143218181}, 'rouge-2': {'f': 0.2091989906511146, 'p': 0.22080562889491437, 'r': 0.2096222497740352}, 'rouge-l': {'f': 0.4033836281461028, 'p': 0.4248145188145189, 'r': 0.40202342951964265}}\n","Warning: incorrectly formatted line in vocabulary file: # #\\/# 2632\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: ## #\\/# 263\n","\n","\n","Warning: incorrectly formatted line in vocabulary file: -lrb-###-rrb- ###-#### 51\n","\n","\n","max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n","Finished constructing vocabulary of 50000 total words. Last word added: malignaggi\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n","INFO:tensorflow:Finished reading dataset in single_pass mode.\n","RL(b) for BERT testing.tar  score:0.9044777154922485\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9YXwY7oX6LpL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6c35b50e-2bde-4f4f-fcc4-3cf4363f146d","executionInfo":{"status":"ok","timestamp":1585851801285,"user_tz":-60,"elapsed":518,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["print('ROUGE_1: {:.4f} ; ROUGE_2: {:.4f}; ROUGE_L: {:.4f}; BERTScore: {:.4f}'.format(r_rlb_1, r_rlb_2, r_rlb_L, rlb_bert_testing))"],"execution_count":81,"outputs":[{"output_type":"stream","text":["ROUGE_1: 0.4204 ; ROUGE_2: 0.2092; ROUGE_L: 0.4034; BERTScore: 0.9045\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yrBiHase6Llc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6xYQMXnZI12g","colab_type":"text"},"source":["###Table"]},{"cell_type":"code","metadata":{"id":"XOrWkqwsCfiK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":167},"outputId":"27ebdcaf-7b50-444a-921e-b8a9b99754b8","executionInfo":{"status":"ok","timestamp":1585851802341,"user_tz":-60,"elapsed":585,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["data_frame = {'ROUGE 1':[r_ml_1, r_hy_1, r_rlr_1, r_rlb_1], 'ROUGE 2':[r_ml_2, r_hy_2, r_rlr_2, r_rlb_2], 'ROUGE L':[r_ml_L, r_hy_L, r_rlr_L, r_rlb_L], \n","              'BERTScore':[float(ml_bert_testing), float(hy_bert_testing), float(rlr_bert_testing), float(rlb_bert_testing)]}\n","df = pd.DataFrame(data_frame, index= ['ML', 'Hybrid', 'RL(r)', 'RL(b)']).round(4)\n","df"],"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ROUGE 1</th>\n","      <th>ROUGE 2</th>\n","      <th>ROUGE L</th>\n","      <th>BERTScore</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>ML</th>\n","      <td>0.4212</td>\n","      <td>0.2205</td>\n","      <td>0.4063</td>\n","      <td>0.9029</td>\n","    </tr>\n","    <tr>\n","      <th>Hybrid</th>\n","      <td>0.4233</td>\n","      <td>0.2223</td>\n","      <td>0.4081</td>\n","      <td>0.9032</td>\n","    </tr>\n","    <tr>\n","      <th>RL(r)</th>\n","      <td>0.3291</td>\n","      <td>0.1591</td>\n","      <td>0.4326</td>\n","      <td>0.8767</td>\n","    </tr>\n","    <tr>\n","      <th>RL(b)</th>\n","      <td>0.4204</td>\n","      <td>0.2092</td>\n","      <td>0.4034</td>\n","      <td>0.9045</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        ROUGE 1  ROUGE 2  ROUGE L  BERTScore\n","ML       0.4212   0.2205   0.4063     0.9029\n","Hybrid   0.4233   0.2223   0.4081     0.9032\n","RL(r)    0.3291   0.1591   0.4326     0.8767\n","RL(b)    0.4204   0.2092   0.4034     0.9045"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"markdown","metadata":{"id":"c_Vgx-cmFKqm","colab_type":"text"},"source":["##CNN/DM testing"]},{"cell_type":"code","metadata":{"id":"i4p0LaG5Kis1","colab_type":"code","colab":{}},"source":["model_folder = 'drive/My Drive/CNNDM testing'        #directory containing all testing models for Gigaword dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GErs9KY7Kk7i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"30664aaa-53b6-4032-9a1e-165e4b1594bb","executionInfo":{"status":"ok","timestamp":1585870229170,"user_tz":-60,"elapsed":2039,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["!ls 'drive/My Drive/CNNDM testing' "],"execution_count":17,"outputs":[{"output_type":"stream","text":["'hybrid testing'  'RL(b) testing'  'testing files'\n","'ml testing'\t  'RL(r) testing'   vocab\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZnvzuneQO33O","colab_type":"text"},"source":["###Hyperparameter"]},{"cell_type":"code","metadata":{"id":"6duTxJMIFMT4","colab_type":"code","colab":{}},"source":["test_data_path = (model_folder + '/testing files/test_0000.bin')\t #\"drive/My Drive/NLPProject/chunked/main_test/test_*\"            #this is where you save the testing data, _* refers to multiple testing files\n","vocab_path = \t(model_folder+ '/vocab')\t    #\"drive/My Drive/NLPProject/vocab\"                               #this is where you save the vocab file\n","\n","\n","# Hyperparameters\n","hidden_dim = 400\n","emb_dim = 200\n","batch_size = 50\n","max_enc_steps = 400\t\t\n","max_dec_steps = 100\t\t\n","beam_size = 5\n","min_dec_steps= 3\n","vocab_size = 30000 \n","\n","lr = 0.001\n","rand_unif_init_mag = 0.02\n","trunc_norm_init_std = 1e-4\n","\n","eps = 1e-12\n","max_iterations = 10000\n","max_batch_queue = 100\n","\n","intra_encoder = True\n","intra_decoder = True"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ka1Ba9CK2IP","colab_type":"text"},"source":["###ML testing(ROUGE+BERTScore)"]},{"cell_type":"code","metadata":{"id":"54qPdW2KK46M","colab_type":"code","colab":{}},"source":["save_model_path = (model_folder + '/ml testing')#'drive/My Drive/Gigaword/ml testing'              #directory path for the model need to be tested\n","model_name_for_r_testing = 'ml for ROUGE testing.tar'\n","model_name_for_b_testing = 'ml for BERT testing.tar'                     "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUpfrp92MbdB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"66cede71-8cf9-45bb-f739-0e99e1863a18","executionInfo":{"status":"ok","timestamp":1585869639021,"user_tz":-60,"elapsed":2074,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["!ls 'drive/My Drive/CNNDM testing/ml testing'  "],"execution_count":159,"outputs":[{"output_type":"stream","text":["'ml for BERT testing.tar'  'ml for ROUGE testing.tar'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HIctPbvKK48c","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":675},"outputId":"d4dfdde1-34bf-476f-dda0-6eaf8bb372a1","executionInfo":{"status":"error","timestamp":1585870247272,"user_tz":-60,"elapsed":6960,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n","    parser.add_argument(\"--start_from\", type=str, default=None)             \n","    parser.add_argument(\"--load_model\", type=str, default=model_name_for_r_testing)       # the name of the testing model\n","    #opt = parser.parse_args()\n","    opt, unknown = parser.parse_known_args()\n","\n","\n","    eval_r = Evaluate_r(test_data_path, opt, batch_size)        #or Evaluate_b(test_data_path, opt, batch_size) if evaluated with BERTScore\n","    r_ml_1, r_ml_2, r_ml_L = eval_r.evaluate_batch()\n","\n","\n","    opt.load_model= model_name_for_b_testing    \n","    eval_b = Evaluate_b(test_data_path, opt, batch_size)\n","    ml_bert_testing = eval_b.evaluate_batch()\n","\n","    print('ROUGE_1: {:.4f} ; ROUGE_2: {:.4f}; ROUGE_L: {:.4f}; BERTScore: {:.4f}'.format(r_ml_1, r_ml_2, r_ml_L, ml_bert_testing))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n","\n","\n","max_size of vocab was specified as 30000; we now have 30000 words. Stopping reading.\n","Finished constructing vocabulary of 30000 total words. Last word added: moles\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:211: DeprecationWarning: generator 'Batcher.text_generator' raised StopIteration\n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-893d61f11e56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0meval_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluate_r\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m#or Evaluate_b(test_data_path, opt, batch_size) if evaluated with BERTScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mr_ml_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_ml_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_ml_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-3c55f4909336>\u001b[0m in \u001b[0;36mevaluate_batch\u001b[0;34m(self, print_sents)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mstart_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTART_DECODING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-3c55f4909336>\u001b[0m in \u001b[0;36msetup_valid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    663\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torch_load_uninitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m                 \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    117\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: Current TensorFlow version is 2.2.0-rc2. To use TF 1.x instead,\nrestart your runtime (Ctrl+M .) and run \"%tensorflow_version 1.x\" before\nyou run \"import tensorflow\".\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"ShQxmWm6O-kN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5429ce34-8a1a-4339-e05a-5faa8f53a22e","executionInfo":{"status":"ok","timestamp":1585870276226,"user_tz":-60,"elapsed":532,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["T.cuda.is_available()"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"CYC13dRBL3DZ","colab_type":"text"},"source":["###Hybrid testing (ROUGE+BERTScore)"]},{"cell_type":"code","metadata":{"id":"EIkC9FAuL7Xd","colab_type":"code","colab":{}},"source":["save_model_path = (model_folder + '/hybrid testing')        #'drive/My Drive/Gigaword/hybrid testing'              #directory path for the model need to be tested\n","model_name_for_r_testing = 'hybrid for ROUGE testing.tar'\n","model_name_for_b_testing = 'hybrid for BERT testing.tar'                "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QgyG4j5gL7cZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":427},"outputId":"d76439a1-4f18-4ba5-ed37-9d63d2f13f96","executionInfo":{"status":"ok","timestamp":1585857299944,"user_tz":-60,"elapsed":550972,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n","    parser.add_argument(\"--start_from\", type=str, default=None)             \n","    parser.add_argument(\"--load_model\", type=str, default=model_name_for_r_testing)       # the name of the testing model\n","    #opt = parser.parse_args()\n","    opt, unknown = parser.parse_known_args()\n","\n","    eval_r = Evaluate_r(test_data_path, opt, batch_size)        #or Evaluate_b(test_data_path, opt, batch_size) if evaluated with BERTScore\n","    r_hy_1, r_hy_2, r_hy_L = eval_r.evaluate_batch()\n","\n","    opt.load_model= model_name_for_b_testing    \n","    eval_b = Evaluate_b(test_data_path, opt, batch_size)\n","    hy_bert_testing = eval_b.evaluate_batch()\n","    print('ROUGE_1: {:.4f} ; ROUGE_2: {:.4f}; ROUGE_L: {:.4f}; BERTScore: {:.4f}'.format(r_hy_1, r_hy_2, r_hy_L, hy_bert_testing))"],"execution_count":114,"outputs":[{"output_type":"stream","text":["Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n","\n","\n","max_size of vocab was specified as 30000; we now have 30000 words. Stopping reading.\n","Finished constructing vocabulary of 30000 total words. Last word added: moles\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:211: DeprecationWarning: generator 'Batcher.text_generator' raised StopIteration\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished reading dataset in single_pass mode.\n","hybrid for ROUGE testing.tar scores: {'rouge-1': {'f': 0.3205578683941674, 'p': 0.2902061358414377, 'r': 0.386230365415494}, 'rouge-2': {'f': 0.12501077585092013, 'p': 0.11408270655704755, 'r': 0.14944289404443603}, 'rouge-l': {'f': 0.3322952610845442, 'p': 0.3149026005276578, 'r': 0.37588170544612104}}\n","Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n","\n","\n","max_size of vocab was specified as 30000; we now have 30000 words. Stopping reading.\n","Finished constructing vocabulary of 30000 total words. Last word added: moles\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n","INFO:tensorflow:Finished reading dataset in single_pass mode.\n","hybrid for BERT testing.tar  score:0.8376993536949158\n","ROUGE_1: 0.3206 ; ROUGE_2: 0.1250; ROUGE_L: 0.3323; BERTScore: 0.8377\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"asWqiN9wMzwA","colab_type":"text"},"source":["###RL(r) testing(ROUGE+BERTScore)"]},{"cell_type":"code","metadata":{"id":"Z1E4u20EM2si","colab_type":"code","colab":{}},"source":["save_model_path =  (model_folder + '/RL(r) testing')             #'drive/My Drive/Gigaword/RL(r) testing model'              #directory path for the RL(r) model need to be tested\n","model_name_for_r_testing = 'RL(r) testing.tar'\n","model_name_for_b_testing = 'RL(r) testing.tar'                     #both names are the same for RL(r) testing"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k2P7V7DRNife","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":427},"outputId":"2098f9f7-67cc-4d73-dc61-2a66a22e2c40","executionInfo":{"status":"ok","timestamp":1585857938646,"user_tz":-60,"elapsed":638672,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n","    parser.add_argument(\"--start_from\", type=str, default=None)             \n","    parser.add_argument(\"--load_model\", type=str, default=model_name_for_r_testing)       # the name of the testing model\n","    #opt = parser.parse_args()\n","    opt, unknown = parser.parse_known_args()\n","\n","\n","    eval_r = Evaluate_r(test_data_path, opt, batch_size)        #or Evaluate_b(test_data_path, opt, batch_size) if evaluated with BERTScore\n","    r_rlr_1, r_rlr_2, r_rlr_L = eval_r.evaluate_batch()\n","\n","    opt.load_model= model_name_for_b_testing    \n","    eval_b = Evaluate_b(test_data_path, opt, batch_size)\n","    rlr_bert_testing = eval_b.evaluate_batch()\n","\n","    print('ROUGE_1: {:.4f} ; ROUGE_2: {:.4f}; ROUGE_L: {:.4f}; BERTScore: {:.4f}'.format(r_rlr_1, r_rlr_2, r_rlr_L, rlr_bert_testing))"],"execution_count":116,"outputs":[{"output_type":"stream","text":["Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n","\n","\n","max_size of vocab was specified as 30000; we now have 30000 words. Stopping reading.\n","Finished constructing vocabulary of 30000 total words. Last word added: moles\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:211: DeprecationWarning: generator 'Batcher.text_generator' raised StopIteration\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished reading dataset in single_pass mode.\n","RL(r) testing.tar scores: {'rouge-1': {'f': 0.2803087243540569, 'p': 0.23769121128051424, 'r': 0.36855667626067484}, 'rouge-2': {'f': 0.10360442582774361, 'p': 0.0877066415265804, 'r': 0.13673986648661476}, 'rouge-l': {'f': 0.3442404833958199, 'p': 0.35383596696042047, 'r': 0.3558461987570033}}\n","Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n","\n","\n","max_size of vocab was specified as 30000; we now have 30000 words. Stopping reading.\n","Finished constructing vocabulary of 30000 total words. Last word added: moles\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n","INFO:tensorflow:Finished reading dataset in single_pass mode.\n","RL(r) testing.tar  score:0.8204892873764038\n","ROUGE_1: 0.2803 ; ROUGE_2: 0.1036; ROUGE_L: 0.3442; BERTScore: 0.8205\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2NqwD9ldN1DH","colab_type":"text"},"source":["###RL(b) testing(ROUGE+BERTScore)"]},{"cell_type":"code","metadata":{"id":"Z6Syh1rXN4GA","colab_type":"code","colab":{}},"source":["save_model_path =  (model_folder + '/RL(b) testing')         #'drive/My Drive/Gigaword/RL(b) testing model'              #directory path for the model need to be tested\n","model_name_for_r_testing = 'RL(b) for ROUGE testing.tar'\n","model_name_for_b_testing = 'RL(b) for BERT testing.tar'                  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jzry6AHeN4lv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":427},"outputId":"dd09539c-537f-4be6-ae84-9ca751139537","executionInfo":{"status":"ok","timestamp":1585858384395,"user_tz":-60,"elapsed":445111,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n","    parser.add_argument(\"--start_from\", type=str, default=None)             \n","    parser.add_argument(\"--load_model\", type=str, default=model_name_for_r_testing)       # the name of the testing model\n","    #opt = parser.parse_args()\n","    opt, unknown = parser.parse_known_args()\n","\n","    eval_r = Evaluate_r(test_data_path, opt, batch_size)        #or Evaluate_b(test_data_path, opt, batch_size) if evaluated with BERTScore\n","    r_rlb_1, r_rlb_2, r_rlb_L = eval_r.evaluate_batch()\n","\n","    opt.load_model= model_name_for_b_testing    \n","    eval_b = Evaluate_b(test_data_path, opt, batch_size)\n","    rlb_bert_testing = eval_b.evaluate_batch()\n","    print('ROUGE_1: {:.4f} ; ROUGE_2: {:.4f}; ROUGE_L: {:.4f}; BERTScore: {:.4f}'.format(r_rlb_1, r_rlb_2, r_rlb_L, rlb_bert_testing))"],"execution_count":118,"outputs":[{"output_type":"stream","text":["Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n","\n","\n","max_size of vocab was specified as 30000; we now have 30000 words. Stopping reading.\n","Finished constructing vocabulary of 30000 total words. Last word added: moles\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:211: DeprecationWarning: generator 'Batcher.text_generator' raised StopIteration\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished reading dataset in single_pass mode.\n","RL(b) for ROUGE testing.tar scores: {'rouge-1': {'f': 0.30220935285726275, 'p': 0.32614719246397106, 'r': 0.30105528923872993}, 'rouge-2': {'f': 0.11736897732306088, 'p': 0.12741766778355326, 'r': 0.11691574551657218}, 'rouge-l': {'f': 0.32130715919253644, 'p': 0.35224349234967567, 'r': 0.31158168062560326}}\n","Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n","\n","\n","max_size of vocab was specified as 30000; we now have 30000 words. Stopping reading.\n","Finished constructing vocabulary of 30000 total words. Last word added: moles\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n","INFO:tensorflow:Finished reading dataset in single_pass mode.\n","RL(b) for BERT testing.tar  score:0.8394527435302734\n","ROUGE_1: 0.3022 ; ROUGE_2: 0.1174; ROUGE_L: 0.3213; BERTScore: 0.8395\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kNhsY7zpOs0H","colab_type":"text"},"source":["###Table"]},{"cell_type":"code","metadata":{"id":"Wb6mO2wkOuBm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":167},"outputId":"396bc067-6741-4ad5-ea35-71c5e57cef22","executionInfo":{"status":"ok","timestamp":1585858384396,"user_tz":-60,"elapsed":445100,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["data_frame = {'ROUGE 1':[r_ml_1, r_hy_1, r_rlr_1, r_rlb_1], 'ROUGE 2':[r_ml_2, r_hy_2, r_rlr_2, r_rlb_2], 'ROUGE L':[r_ml_L, r_hy_L, r_rlr_L, r_rlb_L], \n","              'BERTScore':[float(ml_bert_testing), float(hy_bert_testing), float(rlr_bert_testing), float(rlb_bert_testing)]}\n","df = pd.DataFrame(data_frame, index= ['ML', 'Hybrid', 'RL(r)', 'RL(b)']).round(4)\n","df"],"execution_count":119,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ROUGE 1</th>\n","      <th>ROUGE 2</th>\n","      <th>ROUGE L</th>\n","      <th>BERTScore</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>ML</th>\n","      <td>0.3273</td>\n","      <td>0.1311</td>\n","      <td>0.3372</td>\n","      <td>0.8372</td>\n","    </tr>\n","    <tr>\n","      <th>Hybrid</th>\n","      <td>0.3206</td>\n","      <td>0.1250</td>\n","      <td>0.3323</td>\n","      <td>0.8377</td>\n","    </tr>\n","    <tr>\n","      <th>RL(r)</th>\n","      <td>0.2803</td>\n","      <td>0.1036</td>\n","      <td>0.3442</td>\n","      <td>0.8205</td>\n","    </tr>\n","    <tr>\n","      <th>RL(b)</th>\n","      <td>0.3022</td>\n","      <td>0.1174</td>\n","      <td>0.3213</td>\n","      <td>0.8395</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        ROUGE 1  ROUGE 2  ROUGE L  BERTScore\n","ML       0.3273   0.1311   0.3372     0.8372\n","Hybrid   0.3206   0.1250   0.3323     0.8377\n","RL(r)    0.2803   0.1036   0.3442     0.8205\n","RL(b)    0.3022   0.1174   0.3213     0.8395"]},"metadata":{"tags":[]},"execution_count":119}]},{"cell_type":"code","metadata":{"id":"74gk1Kx82Ibz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dAZE4GRp2JDK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"1bdb6cde-9891-4ba4-a090-5d16f9e33ec6","executionInfo":{"status":"ok","timestamp":1585863924636,"user_tz":-60,"elapsed":2011,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["!ls 'drive/My Drive/NLP PROJ/cnn_ml_model'"],"execution_count":146,"outputs":[{"output_type":"stream","text":["0011000.tar  0013000.tar  0015000.tar  0017000.tar  0019000.tar\n","0012000.tar  0014000.tar  0016000.tar  0018000.tar  0020000.tar\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p6DKOlq12JF7","colab_type":"code","colab":{}},"source":["test_data_path = 'drive/My Drive/NLP PROJ/NLPProject/chunked/main_test/test_*'\t #\"drive/My Drive/NLPProject/chunked/main_test/test_*\"            #this is where you save the testing data, _* refers to multiple testing files\n","vocab_path = \t'drive/My Drive/NLP PROJ/NLPProject/vocab'\t    #\"drive/My Drive/NLPProject/vocab\"                               #this is where you save the vocab file\n","\n","save_model_path =  'drive/My Drive/NLP PROJ/cnn_ml_model'             #directory path for the model need to be tested\n","\n","\n","# Hyperparameters\n","hidden_dim = 400\n","emb_dim = 200\n","batch_size = 50\n","max_enc_steps = 400\t\t\n","max_dec_steps = 100\t\t\n","beam_size = 5\n","min_dec_steps= 3\n","vocab_size = 30000 \n","\n","lr = 0.001\n","rand_unif_init_mag = 0.02\n","trunc_norm_init_std = 1e-4\n","\n","eps = 1e-12\n","max_iterations = 10000\n","max_batch_queue = 100\n","\n","intra_encoder = True\n","intra_decoder = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"863MXyRW2JIZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":232},"outputId":"0df5090a-b0bf-4cfa-bbdc-28e89b721366","executionInfo":{"status":"ok","timestamp":1585866612195,"user_tz":-60,"elapsed":1837249,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n","    parser.add_argument(\"--start_from\", type=str, default=None)             \n","    parser.add_argument(\"--load_model\", type=str, default='0015000.tar')       # the name of the testing model\n","    #opt = parser.parse_args()\n","    opt, unknown = parser.parse_known_args()\n","\n","\n","    eval_r = Evaluate_r(test_data_path, opt, batch_size)        #or Evaluate_b(test_data_path, opt, batch_size) if evaluated with BERTScore\n","    r_ml_1, r_ml_2, r_ml_L = eval_r.evaluate_batch()\n"],"execution_count":151,"outputs":[{"output_type":"stream","text":["Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n","\n","\n","max_size of vocab was specified as 30000; we now have 30000 words. Stopping reading.\n","Finished constructing vocabulary of 30000 total words. Last word added: resurrected\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:211: DeprecationWarning: generator 'Batcher.text_generator' raised StopIteration\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished reading dataset in single_pass mode.\n","0015000.tar scores: {'rouge-1': {'f': 0.43653434823394277, 'p': 0.4021977966005998, 'r': 0.5068783269624053}, 'rouge-2': {'f': 0.20592913956989944, 'p': 0.19005306771209093, 'r': 0.23898476336358165}, 'rouge-l': {'f': 0.400196091248029, 'p': 0.3781428290719531, 'r': 0.4472944523418039}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jPJBbhh82JLC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IvQ14gpy2JNV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6T9aA3QD2JP9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L6_1oKNw2JSP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NC-ZpK7i2JUc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JnPYHcy92JWk","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBZgH7-W2JZE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"COscecAy2Jsz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YOCpdgQQ2Jvg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JA74bQw32Jxa","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AAkM3DtE2Jzs","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OOoOGIdfJBkW","colab_type":"text"},"source":["##WikiHow testing"]},{"cell_type":"code","metadata":{"id":"GCbgyZn_st2g","colab_type":"code","colab":{}},"source":["model_folder = 'drive/My Drive/Wikihow testing'        #directory containing all testing models for Gigaword dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f3G_CIbcsv_L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"c646cbf1-7a73-42ee-b349-d1666506afed","executionInfo":{"status":"ok","timestamp":1585861303559,"user_tz":-60,"elapsed":2097,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["!ls 'drive/My Drive/Wikihow testing' "],"execution_count":122,"outputs":[{"output_type":"stream","text":["'hybrid testing'  'ml testing'\t   'RL(r) testing'\n"," main_test\t  'RL(b) testing'   vocab\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nClXiYRgJFh8","colab_type":"text"},"source":["\n","\n","1.   **ML** model for ROUGE testing:  https://drive.google.com/file/d/115lnCmqJ6CdIqm-Oua0XR-IrEj1eDYfQ/view?usp=sharing\n","\n","2.   **ML** model for BERTScore testing:  https://drive.google.com/file/d/10-vbKjRANJwrsKiHMem1kdcHqwx0j-Gu/view?usp=sharing\n","\n","3.   **ML+RL** model for ROUGE testing:  https://drive.google.com/file/d/1DJ3G-aXH2BkU-60cAuCc-ErGKMUyyh8a/view?usp=sharing\n","\n","4.   **ML+RL** model for BERTScore testing:  https://drive.google.com/file/d/1W7zQM9n98LlKE8J0RBLsndZlMByQlhuJ/view?usp=sharing\n","\n","5.   **RL(r)** model for ROUGE testing:  https://drive.google.com/file/d/10eRbIkmAfQcGRxgUqwGFugvbFzRMCGAb/view?usp=sharing\n","\n","6.   **RL(r)** model for BERTScore testing:  https://drive.google.com/file/d/1-6ph9aZiLL9wgGLbsC0Cb46zZIKoDkvI/view?usp=sharing\n","\n","7.   **RL(b)** model for ROUGE testing:  https://drive.google.com/file/d/1PAY_aNuLs8Ts3whOJQpMQ5NolbG6TSrV/view?usp=sharing\n","\n","8.   **RL(b)** model for BERTScore testing:  https://drive.google.com/file/d/1zWilmP2oEw0GYjpZAkAFFfAWUmw7t8Hb/view?usp=sharing\n","\n","9.   **WikiHow** testing file:  https://drive.google.com/drive/folders/1oWupLbQUvmVJk4cC6hs0n4hATet0Uln2?usp=sharing\n","\n","10.  **WikiHow** vocab file:  https://drive.google.com/file/d/1t6Nh8GTylnkU6naUqbGx0oVxcBe-dJie/view?usp=sharing\n"]},{"cell_type":"code","metadata":{"id":"DlHW2t5kJEfz","colab_type":"code","colab":{}},"source":["test_data_path =  (model_folder + '/main_test/test_*')\t#\"wiki_test/test_*\"                      #testing files path\n","vocab_path =    (model_folder + '/vocab')\t\t#\"wiki_vocab/vocab_wiki\"\n","\n","\n","# Hyperparameters\n","hidden_dim = 512\n","emb_dim = 256\n","batch_size = 10\n","max_enc_steps = 500\n","max_dec_steps = 100\n","beam_size = 4\n","min_dec_steps= 3\n","vocab_size = 20000\n","\n","lr = 0.001\n","rand_unif_init_mag = 0.02\n","trunc_norm_init_std = 1e-4\n","\n","eps = 1e-12\n","max_iterations = 16000\n","max_batch_queue = 30 \n","\n","\n","intra_encoder = True\n","intra_decoder = True"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T6egdpTrs-0u","colab_type":"text"},"source":["###ML testing (ROUGE+BERTScore)"]},{"cell_type":"code","metadata":{"id":"RtiDvEXWtBWd","colab_type":"code","colab":{}},"source":["save_model_path = (model_folder + '/ml testing')#'drive/My Drive/Gigaword/ml testing'              #directory path for the model need to be tested\n","model_name_for_r_testing = 'ml for ROUGE testing.tar'\n","model_name_for_b_testing = 'ml for BERT testing.tar'                     "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HeTwi61qtBY_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":640},"outputId":"9f895f1e-8a0f-4db4-93f2-cd495df0f108","executionInfo":{"status":"error","timestamp":1585862645213,"user_tz":-60,"elapsed":1231796,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n","    parser.add_argument(\"--start_from\", type=str, default=None)             \n","    parser.add_argument(\"--load_model\", type=str, default=model_name_for_r_testing)       # the name of the testing model\n","    #opt = parser.parse_args()\n","    opt, unknown = parser.parse_known_args()\n","\n","\n","    eval_r = Evaluate_r(test_data_path, opt, batch_size)        #or Evaluate_b(test_data_path, opt, batch_size) if evaluated with BERTScore\n","    r_ml_1, r_ml_2, r_ml_L = eval_r.evaluate_batch()\n","\n","\n","    opt.load_model= model_name_for_b_testing    \n","    eval_b = Evaluate_b(test_data_path, opt, batch_size)\n","    ml_bert_testing = eval_b.evaluate_batch()\n","\n","    print('ROUGE_1: {:.4f} ; ROUGE_2: {:.4f}; ROUGE_L: {:.4f}; BERTScore: {:.4f}'.format(r_ml_1, r_ml_2, r_ml_L, ml_bert_testing))"],"execution_count":126,"outputs":[{"output_type":"stream","text":["max_size of vocab was specified as 20000; we now have 20000 words. Stopping reading.\n","Finished constructing vocabulary of 20000 total words. Last word added: declared\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:211: DeprecationWarning: generator 'Batcher.text_generator' raised StopIteration\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished reading dataset in single_pass mode.\n","ml for ROUGE testing.tar scores: {'rouge-1': {'f': 0.34325490125767216, 'p': 0.41510027642350417, 'r': 0.34732053034297433}, 'rouge-2': {'f': 0.1465648174004561, 'p': 0.17428484949760134, 'r': 0.15055950866200019}, 'rouge-l': {'f': 0.2558087953713469, 'p': 0.5565291566881132, 'r': 0.1860673566464986}}\n","max_size of vocab was specified as 20000; we now have 20000 words. Stopping reading.\n","Finished constructing vocabulary of 20000 total words. Last word added: declared\n","example_generator completed reading all datafiles. No more data.\n","INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n","INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-126-893d61f11e56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel_name_for_b_testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0meval_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluate_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mml_bert_testing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ROUGE_1: {:.4f} ; ROUGE_2: {:.4f}; ROUGE_L: {:.4f}; BERTScore: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_ml_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_ml_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_ml_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mml_bert_testing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-e12efcfc1370>\u001b[0m in \u001b[0;36mevaluate_batch\u001b[0;34m(self, print_sents)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m#-----------------------Summarization----------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mpred_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_zeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_batch_extend_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-d7c4edc373e8>\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(enc_hid, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, model, start_id, end_id, unk_id)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0menc_extend_vocab_beam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_batch_extend_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbeam_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_batch_extend_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mfinal_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdec_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_temporal_srcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out_beam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_pad_mask_beam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_zeros_beam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_extend_vocab_beam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_temporal_srcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_s\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m#final_dist: rem*beam, n_extended_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mfinal_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_rem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m#final_dist: rem, beam, n_extended_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-389a999b5a13>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mct_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_temporal_srcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_temporal_srcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mct_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_s\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m#intra-decoder attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mp_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mct_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-389a999b5a13>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, s_t, prev_s)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;31m# Standard attention technique (eq 1 in https://arxiv.org/pdf/1704.04368.pdf)      e = v tanh(Wh + Ws + b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0met\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_prev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_s\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# bs,t-1,n_hid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mdec_fea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# bs,1,n_hid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0met\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0met\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdec_fea\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"HYUVj58ttNm7","colab_type":"text"},"source":["###Hybrid testing(ROUGE+BERTScore)"]},{"cell_type":"code","metadata":{"id":"LOtnhFd3tQox","colab_type":"code","colab":{}},"source":["save_model_path = (model_folder + '/hybrid testing')        #'drive/My Drive/Gigaword/hybrid testing'              #directory path for the model need to be tested\n","model_name_for_r_testing = 'hybrid for ROUGE testing.tar'\n","model_name_for_b_testing = 'hybrid for BERT testing.tar'                "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mg_52rjgtQrO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":400},"outputId":"d3114340-86f9-4f36-bd4d-1226b3fd63ea","executionInfo":{"status":"error","timestamp":1585862649245,"user_tz":-60,"elapsed":3327,"user":{"displayName":"Yan Song","photoUrl":"","userId":"06548571015088915642"}}},"source":["if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n","    parser.add_argument(\"--start_from\", type=str, default=None)             \n","    parser.add_argument(\"--load_model\", type=str, default=model_name_for_r_testing)       # the name of the testing model\n","    #opt = parser.parse_args()\n","    opt, unknown = parser.parse_known_args()\n","\n","    eval_r = Evaluate_r(test_data_path, opt, batch_size)        #or Evaluate_b(test_data_path, opt, batch_size) if evaluated with BERTScore\n","    r_hy_1, r_hy_2, r_hy_L = eval_r.evaluate_batch()\n","\n","    opt.load_model= model_name_for_b_testing    \n","    eval_b = Evaluate_b(test_data_path, opt, batch_size)\n","    hy_bert_testing = eval_b.evaluate_batch()\n","    print('ROUGE_1: {:.4f} ; ROUGE_2: {:.4f}; ROUGE_L: {:.4f}; BERTScore: {:.4f}'.format(r_hy_1, r_hy_2, r_hy_L, hy_bert_testing))"],"execution_count":127,"outputs":[{"output_type":"stream","text":["max_size of vocab was specified as 20000; we now have 20000 words. Stopping reading.\n","Finished constructing vocabulary of 20000 total words. Last word added: declared\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-127-aee67f0f8114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munknown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0meval_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluate_r\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m#or Evaluate_b(test_data_path, opt, batch_size) if evaluated with BERTScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mr_hy_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_hy_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_hy_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-3c55f4909336>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, opt, batch_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m                                batch_size=batch_size, single_pass=True)\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"PDrMhQ7HteSV","colab_type":"text"},"source":["###RL(r) testing(ROUGE+BERTScore)"]},{"cell_type":"code","metadata":{"id":"aYeXmOd5thSn","colab_type":"code","colab":{}},"source":["save_model_path =  (model_folder + '/RL(r) testing')             #'drive/My Drive/Gigaword/RL(r) testing model'              #directory path for the RL(r) model need to be tested\n","model_name_for_r_testing = 'RL(r) testing.tar'\n","model_name_for_b_testing = 'RL(r) testing.tar'                     #both names are the same for RL(r) testing"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xeI5tHmtkWH","colab_type":"code","colab":{}},"source":["if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n","    parser.add_argument(\"--start_from\", type=str, default=None)             \n","    parser.add_argument(\"--load_model\", type=str, default=model_name_for_r_testing)       # the name of the testing model\n","    #opt = parser.parse_args()\n","    opt, unknown = parser.parse_known_args()\n","\n","\n","    eval_r = Evaluate_r(test_data_path, opt, batch_size)        #or Evaluate_b(test_data_path, opt, batch_size) if evaluated with BERTScore\n","    r_rlr_1, r_rlr_2, r_rlr_L = eval_r.evaluate_batch()\n","\n","    opt.load_model= model_name_for_b_testing    \n","    eval_b = Evaluate_b(test_data_path, opt, batch_size)\n","    rlr_bert_testing = eval_b.evaluate_batch()\n","\n","    print('ROUGE_1: {:.4f} ; ROUGE_2: {:.4f}; ROUGE_L: {:.4f}; BERTScore: {:.4f}'.format(r_rlr_1, r_rlr_2, r_rlr_L, rlr_bert_testing))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p_AgyA41tq7P","colab_type":"text"},"source":["###RL(b) testing(ROUGE+BERTScore)"]},{"cell_type":"code","metadata":{"id":"6st_Kntdtup5","colab_type":"code","colab":{}},"source":["save_model_path =  (model_folder + '/RL(b) testing')         #'drive/My Drive/Gigaword/RL(b) testing model'              #directory path for the model need to be tested\n","model_name_for_r_testing = 'RL(b) for ROUGE testing.tar'\n","model_name_for_b_testing = 'RL(b) for BERT testing.tar'                  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1LXN44ZUthVM","colab_type":"code","colab":{}},"source":["if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n","    parser.add_argument(\"--start_from\", type=str, default=None)             \n","    parser.add_argument(\"--load_model\", type=str, default=model_name_for_r_testing)       # the name of the testing model\n","    #opt = parser.parse_args()\n","    opt, unknown = parser.parse_known_args()\n","\n","    eval_r = Evaluate_r(test_data_path, opt, batch_size)        #or Evaluate_b(test_data_path, opt, batch_size) if evaluated with BERTScore\n","    r_rlb_1, r_rlb_2, r_rlb_L = eval_r.evaluate_batch()\n","\n","    opt.load_model= model_name_for_b_testing    \n","    eval_b = Evaluate_b(test_data_path, opt, batch_size)\n","    rlb_bert_testing = eval_b.evaluate_batch()\n","    print('ROUGE_1: {:.4f} ; ROUGE_2: {:.4f}; ROUGE_L: {:.4f}; BERTScore: {:.4f}'.format(r_rlb_1, r_rlb_2, r_rlb_L, rlb_bert_testing))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OCgnqgzatzhl","colab_type":"text"},"source":["###Table"]},{"cell_type":"code","metadata":{"id":"41NopaOptypr","colab_type":"code","colab":{}},"source":["data_frame = {'ROUGE 1':[r_ml_1, r_hy_1, r_rlr_1, r_rlb_1], 'ROUGE 2':[r_ml_2, r_hy_2, r_rlr_2, r_rlb_2], 'ROUGE L':[r_ml_L, r_hy_L, r_rlr_L, r_rlb_L], \n","              'BERTScore':[float(ml_bert_testing), float(hy_bert_testing), float(rlr_bert_testing), float(rlb_bert_testing)]}\n","df = pd.DataFrame(data_frame, index= ['ML', 'Hybrid', 'RL(r)', 'RL(b)']).round(4)\n","df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BNu5mqc7tysq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h8p6qawpJEol","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aA5G08iiJEqz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}